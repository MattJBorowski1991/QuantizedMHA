==PROF== Connected to process 25964 (/teamspace/studios/this_studio/QuantizedMHA/bin/profile_unfused)
==PROF== Profiling "extract_mat" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 240: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 241: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 242: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 243: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 244: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 245: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 246: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 247: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 248: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 249: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 250: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 251: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 252: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 253: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 254: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 255: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 256: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 257: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 258: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 259: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 260: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 261: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 262: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 263: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 264: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 265: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 266: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 267: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 268: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 269: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 270: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 271: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 272: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 273: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 274: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 275: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 276: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 277: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 278: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 279: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 280: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 281: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 282: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 283: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 284: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 285: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 286: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 287: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 288: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 289: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 290: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 291: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 292: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 293: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 294: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 295: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 296: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 297: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 298: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 299: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 300: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 301: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 302: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 303: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 304: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 305: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 306: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 307: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 308: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 309: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 310: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 311: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 312: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 313: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 314: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 315: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 316: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 317: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 318: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 319: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 320: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 321: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 322: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 323: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 324: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 325: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 326: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 327: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 328: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 329: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 330: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 331: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 332: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 333: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 334: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 335: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 336: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 337: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 338: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 339: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 340: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 341: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 342: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 343: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 344: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 345: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 346: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 347: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 348: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 349: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 350: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 351: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 352: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 353: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 354: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 355: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 356: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 357: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 358: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 359: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 360: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 361: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 362: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 363: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 364: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 365: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 366: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 367: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 368: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 369: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 370: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 371: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 372: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 373: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 374: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 375: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 376: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 377: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 378: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 379: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 380: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 381: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 382: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 383: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 384: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 385: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 386: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 387: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 388: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 389: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 390: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 391: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 392: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 393: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 394: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 395: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 396: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 397: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 398: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 399: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 400: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 401: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 402: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 403: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 404: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 405: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 406: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 407: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 408: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 409: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 410: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 411: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 412: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 413: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 414: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 415: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 416: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 417: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 418: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 419: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 420: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 421: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 422: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 423: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 424: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 425: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 426: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 427: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 428: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 429: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 430: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 431: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 432: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 433: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 434: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 435: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 436: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 437: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 438: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 439: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 440: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 441: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 442: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 443: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 444: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 445: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 446: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 447: 0%....50%....100% - 8 passes
Initializing host data (constant values for correctness check)...
Running correctness check 
Loaded reference output from .cache/ref_N4096_d1024.bin
Correctness check PASSED.
Loaded input matrices from .cache/input_random_N4096_d1024.bin
Running 0 warmup iterations...
Running 1 profiling iterations...
Profiling complete.
==PROF== Disconnected from process 25964
[25964] profile_unfused@127.0.0.1
  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       785.34
    Elapsed Cycles                cycle         5496
    Memory Throughput                 %        33.16
    DRAM Throughput                   %        33.16
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        17.81
    L2 Cache Throughput               %        18.54
    SM Active Cycles              cycle      3172.86
    Compute (SM) Throughput           %        10.59
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.65
    Achieved Active Warps Per SM           warp        36.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14093.33
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      3172.86
    Total L1 Elapsed Cycles          cycle       309428
    Average L2 Active Cycles         cycle      2576.42
    Total L2 Elapsed Cycles          cycle       135240
    Average SM Active Cycles         cycle      3172.86
    Total SM Elapsed Cycles          cycle       309428
    Average SMSP Active Cycles       cycle      3096.80
    Total SMSP Elapsed Cycles        cycle      1237712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.863%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.82% above the average, while the minimum instance value is 10.97% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       803.58
    Elapsed Cycles                cycle         5862
    Memory Throughput                 %        31.71
    DRAM Throughput                   %        31.71
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.41
    L2 Cache Throughput               %        17.91
    SM Active Cycles              cycle      3244.74
    Compute (SM) Throughput           %        10.44
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.15
    Achieved Active Warps Per SM           warp        35.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3244.74
    Total L1 Elapsed Cycles          cycle       313848
    Average L2 Active Cycles         cycle      2622.67
    Total L2 Elapsed Cycles          cycle       140088
    Average SM Active Cycles         cycle      3244.74
    Total SM Elapsed Cycles          cycle       313848
    Average SMSP Active Cycles       cycle      3187.61
    Total SMSP Elapsed Cycles        cycle      1255392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       823.50
    Elapsed Cycles                cycle         5899
    Memory Throughput                 %        32.09
    DRAM Throughput                   %        32.09
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        16.97
    L2 Cache Throughput               %        17.91
    SM Active Cycles              cycle      3328.50
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14021.33
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3328.50
    Total L1 Elapsed Cycles          cycle       306474
    Average L2 Active Cycles         cycle      2625.08
    Total L2 Elapsed Cycles          cycle       140160
    Average SM Active Cycles         cycle      3328.50
    Total SM Elapsed Cycles          cycle       306474
    Average SMSP Active Cycles       cycle      3165.57
    Total SMSP Elapsed Cycles        cycle      1225896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.678%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.48% above the average, while the minimum instance value is 11.36% below   
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.16
    Elapsed Cycles                cycle      1297143
    Memory Throughput                 %        82.32
    DRAM Throughput                   %        11.83
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.48
    SM Active Cycles              cycle   1292598.59
    Compute (SM) Throughput           %        82.32
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1204064
    Total DRAM Elapsed Cycles        cycle     61059072
    Average L1 Active Cycles         cycle   1292598.59
    Total L1 Elapsed Cycles          cycle     75155106
    Average L2 Active Cycles         cycle   1172450.96
    Total L2 Elapsed Cycles          cycle     32267592
    Average SM Active Cycles         cycle   1292598.59
    Total SM Elapsed Cycles          cycle     75155106
    Average SMSP Active Cycles       cycle   1292416.86
    Total SMSP Elapsed Cycles        cycle    300620424
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.17
    Elapsed Cycles                cycle       390793
    Memory Throughput                 %        87.32
    DRAM Throughput                   %        87.32
    Duration                         us       482.27
    L1/TEX Cache Throughput           %        49.08
    L2 Cache Throughput               %        58.01
    SM Active Cycles              cycle    379035.86
    Compute (SM) Throughput           %        30.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.72
    Achieved Active Warps Per SM           warp        45.95
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2628896
    Total DRAM Elapsed Cycles        cycle     18063360
    Average L1 Active Cycles         cycle    379035.86
    Total L1 Elapsed Cycles          cycle     22276544
    Average L2 Active Cycles         cycle    391259.12
    Total L2 Elapsed Cycles          cycle      9547680
    Average SM Active Cycles         cycle    379035.86
    Total SM Elapsed Cycles          cycle     22276544
    Average SMSP Active Cycles       cycle    377141.54
    Total SMSP Elapsed Cycles        cycle     89106176
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.66
    Elapsed Cycles                cycle      1204880
    Memory Throughput                 %        84.62
    DRAM Throughput                   %        23.18
    Duration                         ms         1.48
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        14.00
    SM Active Cycles              cycle   1158139.48
    Compute (SM) Throughput           %        84.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147821.33
    Total DRAM Elapsed Cycles        cycle     55602176
    Average L1 Active Cycles         cycle   1158139.48
    Total L1 Elapsed Cycles          cycle     69424720
    Average L2 Active Cycles         cycle   1205993.54
    Total L2 Elapsed Cycles          cycle     29382288
    Average SM Active Cycles         cycle   1158139.48
    Total SM Elapsed Cycles          cycle     69424720
    Average SMSP Active Cycles       cycle   1157035.97
    Total SMSP Elapsed Cycles        cycle    277698880
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.31
    Elapsed Cycles                cycle         5504
    Memory Throughput                 %        28.37
    DRAM Throughput                   %        28.37
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        20.08
    L2 Cache Throughput               %        15.81
    SM Active Cycles              cycle      2813.40
    Compute (SM) Throughput           %        11.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.44
    Achieved Active Warps Per SM           warp        35.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12104
    Total DRAM Elapsed Cycles        cycle       256000
    Average L1 Active Cycles         cycle      2813.40
    Total L1 Elapsed Cycles          cycle       292284
    Average L2 Active Cycles         cycle      2399.96
    Total L2 Elapsed Cycles          cycle       134904
    Average SM Active Cycles         cycle      2813.40
    Total SM Elapsed Cycles          cycle       292284
    Average SMSP Active Cycles       cycle      2770.41
    Total SMSP Elapsed Cycles        cycle      1169136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.766%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.49% above the average, while the minimum instance value is 28.06% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.85
    Elapsed Cycles                cycle         5850
    Memory Throughput                 %        30.99
    DRAM Throughput                   %        30.99
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.24
    L2 Cache Throughput               %        17.46
    SM Active Cycles              cycle      3276.76
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.97
    Achieved Active Warps Per SM           warp        34.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14066.67
    Total DRAM Elapsed Cycles        cycle       272384
    Average L1 Active Cycles         cycle      3276.76
    Total L1 Elapsed Cycles          cycle       307676
    Average L2 Active Cycles         cycle      2576.04
    Total L2 Elapsed Cycles          cycle       143928
    Average SM Active Cycles         cycle      3276.76
    Total SM Elapsed Cycles          cycle       307676
    Average SMSP Active Cycles       cycle      3069.42
    Total SMSP Elapsed Cycles        cycle      1230704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.611%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.70% above the average, while the minimum instance value is 10.99% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.35
    Elapsed Cycles                cycle         5759
    Memory Throughput                 %        31.48
    DRAM Throughput                   %        31.48
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        18.08
    L2 Cache Throughput               %        17.70
    SM Active Cycles              cycle      3124.45
    Compute (SM) Throughput           %        10.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.47
    Achieved Active Warps Per SM           warp        36.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14128
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3124.45
    Total L1 Elapsed Cycles          cycle       298118
    Average L2 Active Cycles         cycle      2628.67
    Total L2 Elapsed Cycles          cycle       141816
    Average SM Active Cycles         cycle      3124.45
    Total SM Elapsed Cycles          cycle       298118
    Average SMSP Active Cycles       cycle      3133.68
    Total SMSP Elapsed Cycles        cycle      1192472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.988%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.82% above the average, while the minimum instance value is 15.63% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.79
    Elapsed Cycles                cycle         5667
    Memory Throughput                 %        31.74
    DRAM Throughput                   %        31.74
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.36
    L2 Cache Throughput               %        18.03
    SM Active Cycles              cycle      3254.38
    Compute (SM) Throughput           %        10.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.44
    Achieved Active Warps Per SM           warp        34.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13976
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3254.38
    Total L1 Elapsed Cycles          cycle       302720
    Average L2 Active Cycles         cycle      2539.75
    Total L2 Elapsed Cycles          cycle       139176
    Average SM Active Cycles         cycle      3254.38
    Total SM Elapsed Cycles          cycle       302720
    Average SMSP Active Cycles       cycle      3102.07
    Total SMSP Elapsed Cycles        cycle      1210880
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.31
    Elapsed Cycles                cycle      1311313
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.93
    Duration                         ms         1.62
    L1/TEX Cache Throughput           %        82.54
    L2 Cache Throughput               %        14.59
    SM Active Cycles              cycle   1292266.79
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203000
    Total DRAM Elapsed Cycles        cycle     60521472
    Average L1 Active Cycles         cycle   1292266.79
    Total L1 Elapsed Cycles          cycle     75123188
    Average L2 Active Cycles         cycle   1165772.75
    Total L2 Elapsed Cycles          cycle     31982184
    Average SM Active Cycles         cycle   1292266.79
    Total SM Elapsed Cycles          cycle     75123188
    Average SMSP Active Cycles       cycle   1292443.52
    Total SMSP Elapsed Cycles        cycle    300492752
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.29
    Elapsed Cycles                cycle       391313
    Memory Throughput                 %        87.32
    DRAM Throughput                   %        87.32
    Duration                         us       482.78
    L1/TEX Cache Throughput           %        48.76
    L2 Cache Throughput               %        58.00
    SM Active Cycles              cycle    379192.79
    Compute (SM) Throughput           %        30.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.03
    Achieved Active Warps Per SM           warp        46.10
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2631850.67
    Total DRAM Elapsed Cycles        cycle     18084864
    Average L1 Active Cycles         cycle    379192.79
    Total L1 Elapsed Cycles          cycle     22420174
    Average L2 Active Cycles         cycle    394658.75
    Total L2 Elapsed Cycles          cycle      9557808
    Average SM Active Cycles         cycle    379192.79
    Total SM Elapsed Cycles          cycle     22420174
    Average SMSP Active Cycles       cycle    380329.90
    Total SMSP Elapsed Cycles        cycle     89680696
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.16
    Elapsed Cycles                cycle      1220054
    Memory Throughput                 %        84.45
    DRAM Throughput                   %        23.63
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.41
    L2 Cache Throughput               %        14.11
    SM Active Cycles              cycle   1158732.02
    Compute (SM) Throughput           %        84.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.99
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148386.67
    Total DRAM Elapsed Cycles        cycle     54545408
    Average L1 Active Cycles         cycle   1158732.02
    Total L1 Elapsed Cycles          cycle     69562820
    Average L2 Active Cycles         cycle   1182895.12
    Total L2 Elapsed Cycles          cycle     29088816
    Average SM Active Cycles         cycle   1158732.02
    Total SM Elapsed Cycles          cycle     69562820
    Average SMSP Active Cycles       cycle   1157019.55
    Total SMSP Elapsed Cycles        cycle    278251280
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.08
    Elapsed Cycles                cycle         5532
    Memory Throughput                 %        28.35
    DRAM Throughput                   %        28.35
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        19.39
    L2 Cache Throughput               %        15.51
    SM Active Cycles              cycle      2913.24
    Compute (SM) Throughput           %        11.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.34
    Achieved Active Warps Per SM           warp        34.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12338.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      2913.24
    Total L1 Elapsed Cycles          cycle       293958
    Average L2 Active Cycles         cycle      2478.33
    Total L2 Elapsed Cycles          cycle       137712
    Average SM Active Cycles         cycle      2913.24
    Total SM Elapsed Cycles          cycle       293958
    Average SMSP Active Cycles       cycle      2861.82
    Total SMSP Elapsed Cycles        cycle      1175832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.114%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.16% above the average, while the minimum instance value is 5.22% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.57
    Elapsed Cycles                cycle         5727
    Memory Throughput                 %        31.00
    DRAM Throughput                   %        31.00
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.51
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3227.43
    Compute (SM) Throughput           %        10.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.24
    Achieved Active Warps Per SM           warp        35.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13968
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3227.43
    Total L1 Elapsed Cycles          cycle       309074
    Average L2 Active Cycles         cycle      2681.42
    Total L2 Elapsed Cycles          cycle       142512
    Average SM Active Cycles         cycle      3227.43
    Total SM Elapsed Cycles          cycle       309074
    Average SMSP Active Cycles       cycle      3183.96
    Total SMSP Elapsed Cycles        cycle      1236296
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.54%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 17.64% above the average, while the minimum instance value is 13.76% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.96%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.41% above the average, while the minimum instance value is 7.18% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       823.31
    Elapsed Cycles                cycle         5817
    Memory Throughput                 %        32.60
    DRAM Throughput                   %        32.60
    Duration                         us         6.98
    L1/TEX Cache Throughput           %        17.18
    L2 Cache Throughput               %        18.09
    SM Active Cycles              cycle      3289.34
    Compute (SM) Throughput           %        10.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.21
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14021.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3289.34
    Total L1 Elapsed Cycles          cycle       312234
    Average L2 Active Cycles         cycle      2647.04
    Total L2 Elapsed Cycles          cycle       138288
    Average SM Active Cycles         cycle      3289.34
    Total SM Elapsed Cycles          cycle       312234
    Average SMSP Active Cycles       cycle      3211.48
    Total SMSP Elapsed Cycles        cycle      1248936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.182%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.69% above the average, while the minimum instance value is 13.28% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       838.79
    Elapsed Cycles                cycle         5921
    Memory Throughput                 %        32.62
    DRAM Throughput                   %        32.62
    Duration                         us         6.98
    L1/TEX Cache Throughput           %        17.42
    L2 Cache Throughput               %        17.75
    SM Active Cycles              cycle      3242.88
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.71
    Achieved Active Warps Per SM           warp        36.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3242.88
    Total L1 Elapsed Cycles          cycle       306406
    Average L2 Active Cycles         cycle      2553.04
    Total L2 Elapsed Cycles          cycle       140880
    Average SM Active Cycles         cycle      3242.88
    Total SM Elapsed Cycles          cycle       306406
    Average SMSP Active Cycles       cycle      3093.43
    Total SMSP Elapsed Cycles        cycle      1225624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.057%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.24% above the average, while the minimum instance value is 7.98% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.057%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.24% above the average, while the minimum instance value is 7.98% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.16
    Elapsed Cycles                cycle      1313203
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        12.33
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292436.22
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203981.33
    Total DRAM Elapsed Cycles        cycle     58579968
    Average L1 Active Cycles         cycle   1292436.22
    Total L1 Elapsed Cycles          cycle     75131290
    Average L2 Active Cycles         cycle   1146528.88
    Total L2 Elapsed Cycles          cycle     31291848
    Average SM Active Cycles         cycle   1292436.22
    Total SM Elapsed Cycles          cycle     75131290
    Average SMSP Active Cycles       cycle   1292163.26
    Total SMSP Elapsed Cycles        cycle    300525160
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.48
    Elapsed Cycles                cycle       385107
    Memory Throughput                 %        87.31
    DRAM Throughput                   %        87.31
    Duration                         us       483.07
    L1/TEX Cache Throughput           %        49.12
    L2 Cache Throughput               %        57.87
    SM Active Cycles              cycle    379398.22
    Compute (SM) Throughput           %        30.86
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.44
    Achieved Active Warps Per SM           warp        45.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2633416
    Total DRAM Elapsed Cycles        cycle     18096128
    Average L1 Active Cycles         cycle    379398.22
    Total L1 Elapsed Cycles          cycle     22248518
    Average L2 Active Cycles         cycle    395618.38
    Total L2 Elapsed Cycles          cycle      9563088
    Average SM Active Cycles         cycle    379398.22
    Total SM Elapsed Cycles          cycle     22248518
    Average SMSP Active Cycles       cycle    378412.96
    Total SMSP Elapsed Cycles        cycle     88994072
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.19
    Elapsed Cycles                cycle      1188507
    Memory Throughput                 %        84.90
    DRAM Throughput                   %        23.05
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158145.10
    Compute (SM) Throughput           %        84.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148994.67
    Total DRAM Elapsed Cycles        cycle     55931904
    Average L1 Active Cycles         cycle   1158145.10
    Total L1 Elapsed Cycles          cycle     69191848
    Average L2 Active Cycles         cycle   1215675.04
    Total L2 Elapsed Cycles          cycle     29557128
    Average SM Active Cycles         cycle   1158145.10
    Total SM Elapsed Cycles          cycle     69191848
    Average SMSP Active Cycles       cycle   1159203.34
    Total SMSP Elapsed Cycles        cycle    276767392
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       793.00
    Elapsed Cycles                cycle         5554
    Memory Throughput                 %        28.58
    DRAM Throughput                   %        28.58
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.30
    L2 Cache Throughput               %        15.83
    SM Active Cycles              cycle      2927.03
    Compute (SM) Throughput           %        11.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.84
    Achieved Active Warps Per SM           warp        34.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12144
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2927.03
    Total L1 Elapsed Cycles          cycle       296850
    Average L2 Active Cycles         cycle      2469.58
    Total L2 Elapsed Cycles          cycle       134640
    Average SM Active Cycles         cycle      2927.03
    Total SM Elapsed Cycles          cycle       296850
    Average SMSP Active Cycles       cycle      2864.45
    Total SMSP Elapsed Cycles        cycle      1187400
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       783.90
    Elapsed Cycles                cycle         5842
    Memory Throughput                 %        31.02
    DRAM Throughput                   %        31.02
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.40
    L2 Cache Throughput               %        17.42
    SM Active Cycles              cycle      3247.12
    Compute (SM) Throughput           %        10.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.40
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13976
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3247.12
    Total L1 Elapsed Cycles          cycle       308416
    Average L2 Active Cycles         cycle      2627.12
    Total L2 Elapsed Cycles          cycle       143472
    Average SM Active Cycles         cycle      3247.12
    Total SM Elapsed Cycles          cycle       308416
    Average SMSP Active Cycles       cycle      3083.37
    Total SMSP Elapsed Cycles        cycle      1233664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.755%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.42% above the average, while the minimum instance value is 13.40% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.932%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 13.68% above the average, while the minimum instance value is 10.91% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.755%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.42% above the average, while the minimum instance value is 13.40% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.21
    Elapsed Cycles                cycle         5817
    Memory Throughput                 %        31.26
    DRAM Throughput                   %        31.26
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.71
    L2 Cache Throughput               %        17.52
    SM Active Cycles              cycle      3190.67
    Compute (SM) Throughput           %        10.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.13
    Achieved Active Warps Per SM           warp        35.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14082.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3190.67
    Total L1 Elapsed Cycles          cycle       305258
    Average L2 Active Cycles         cycle      2635.17
    Total L2 Elapsed Cycles          cycle       142824
    Average SM Active Cycles         cycle      3190.67
    Total SM Elapsed Cycles          cycle       305258
    Average SMSP Active Cycles       cycle      3143.42
    Total SMSP Elapsed Cycles        cycle      1221032
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       786.50
    Elapsed Cycles                cycle         5714
    Memory Throughput                 %        31.78
    DRAM Throughput                   %        31.78
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.61
    L2 Cache Throughput               %        17.86
    SM Active Cycles              cycle      3207.57
    Compute (SM) Throughput           %        10.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.98
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13992
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3207.57
    Total L1 Elapsed Cycles          cycle       303942
    Average L2 Active Cycles         cycle      2607.75
    Total L2 Elapsed Cycles          cycle       140160
    Average SM Active Cycles         cycle      3207.57
    Total SM Elapsed Cycles          cycle       303942
    Average SMSP Active Cycles       cycle      3113.48
    Total SMSP Elapsed Cycles        cycle      1215768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.029%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.85% above the average, while the minimum instance value is 10.18% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.451%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.18% above the average, while the minimum instance value is 14.15% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.029%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.85% above the average, while the minimum instance value is 10.18% below   
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.68
    Elapsed Cycles                cycle      1312001
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.60
    SM Active Cycles              cycle   1292510.53
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203856
    Total DRAM Elapsed Cycles        cycle     60507136
    Average L1 Active Cycles         cycle   1292510.53
    Total L1 Elapsed Cycles          cycle     75133466
    Average L2 Active Cycles         cycle   1165285.33
    Total L2 Elapsed Cycles          cycle     31974648
    Average SM Active Cycles         cycle   1292510.53
    Total SM Elapsed Cycles          cycle     75133466
    Average SMSP Active Cycles       cycle   1292281.36
    Total SMSP Elapsed Cycles        cycle    300533864
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.10
    Elapsed Cycles                cycle       388497
    Memory Throughput                 %        87.37
    DRAM Throughput                   %        87.37
    Duration                         us       479.42
    L1/TEX Cache Throughput           %        48.58
    L2 Cache Throughput               %        58.31
    SM Active Cycles              cycle    380374.84
    Compute (SM) Throughput           %        30.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.79
    Achieved Active Warps Per SM           warp        45.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2614984
    Total DRAM Elapsed Cycles        cycle     17958912
    Average L1 Active Cycles         cycle    380374.84
    Total L1 Elapsed Cycles          cycle     22488900
    Average L2 Active Cycles         cycle    396086.92
    Total L2 Elapsed Cycles          cycle      9490848
    Average SM Active Cycles         cycle    380374.84
    Total SM Elapsed Cycles          cycle     22488900
    Average SMSP Active Cycles       cycle    381832.23
    Total SMSP Elapsed Cycles        cycle     89955600
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.93
    Elapsed Cycles                cycle      1210250
    Memory Throughput                 %        84.65
    DRAM Throughput                   %        23.10
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.54
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1157048.95
    Compute (SM) Throughput           %        84.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148264
    Total DRAM Elapsed Cycles        cycle     55809024
    Average L1 Active Cycles         cycle   1157048.95
    Total L1 Elapsed Cycles          cycle     69400810
    Average L2 Active Cycles         cycle   1205484.33
    Total L2 Elapsed Cycles          cycle     29491104
    Average SM Active Cycles         cycle   1157048.95
    Total SM Elapsed Cycles          cycle     69400810
    Average SMSP Active Cycles       cycle   1158550.50
    Total SMSP Elapsed Cycles        cycle    277603240
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       783.66
    Elapsed Cycles                cycle         5512
    Memory Throughput                 %        28.85
    DRAM Throughput                   %        28.85
    Duration                         us         6.98
    L1/TEX Cache Throughput           %        19.57
    L2 Cache Throughput               %        15.73
    SM Active Cycles              cycle      2886.17
    Compute (SM) Throughput           %        11.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.42
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12309.33
    Total DRAM Elapsed Cycles        cycle       256000
    Average L1 Active Cycles         cycle      2886.17
    Total L1 Elapsed Cycles          cycle       296878
    Average L2 Active Cycles         cycle      2491.08
    Total L2 Elapsed Cycles          cycle       135408
    Average SM Active Cycles         cycle      2886.17
    Total SM Elapsed Cycles          cycle       296878
    Average SMSP Active Cycles       cycle      2852.71
    Total SMSP Elapsed Cycles        cycle      1187512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.162%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.15% above the average, while the minimum instance value is 19.10% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.203%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.13% above the average, while the minimum instance value is 25.12% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.162%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.15% above the average, while the minimum instance value is 19.10% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.78%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.36% above the average, while the minimum instance value is 5.34% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       784.82
    Elapsed Cycles                cycle         5726
    Memory Throughput                 %        31.71
    DRAM Throughput                   %        31.71
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        18.14
    L2 Cache Throughput               %        17.82
    SM Active Cycles              cycle      3114.21
    Compute (SM) Throughput           %        10.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.66
    Achieved Active Warps Per SM           warp        36.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3114.21
    Total L1 Elapsed Cycles          cycle       310702
    Average L2 Active Cycles         cycle      2698.04
    Total L2 Elapsed Cycles          cycle       140640
    Average SM Active Cycles         cycle      3114.21
    Total SM Elapsed Cycles          cycle       310702
    Average SMSP Active Cycles       cycle      3184.47
    Total SMSP Elapsed Cycles        cycle      1242808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.48
    Elapsed Cycles                cycle         5733
    Memory Throughput                 %        31.50
    DRAM Throughput                   %        31.50
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.76
    L2 Cache Throughput               %        17.74
    SM Active Cycles              cycle      3181.95
    Compute (SM) Throughput           %        10.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.46
    Achieved Active Warps Per SM           warp        35.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3181.95
    Total L1 Elapsed Cycles          cycle       305126
    Average L2 Active Cycles         cycle      2671.96
    Total L2 Elapsed Cycles          cycle       141096
    Average SM Active Cycles         cycle      3181.95
    Total SM Elapsed Cycles          cycle       305126
    Average SMSP Active Cycles       cycle      3161.94
    Total SMSP Elapsed Cycles        cycle      1220504
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.65%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.40% above the average, while the minimum instance value is 12.24% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.44
    Elapsed Cycles                cycle         5752
    Memory Throughput                 %        31.20
    DRAM Throughput                   %        31.20
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.63
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3205.41
    Compute (SM) Throughput           %        10.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.85
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3205.41
    Total L1 Elapsed Cycles          cycle       305796
    Average L2 Active Cycles         cycle      2545.58
    Total L2 Elapsed Cycles          cycle       141744
    Average SM Active Cycles         cycle      3205.41
    Total SM Elapsed Cycles          cycle       305796
    Average SMSP Active Cycles       cycle      3053.89
    Total SMSP Elapsed Cycles        cycle      1223184
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.53
    Elapsed Cycles                cycle      1311837
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.62
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.60
    SM Active Cycles              cycle   1292455.67
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203752
    Total DRAM Elapsed Cycles        cycle     60510208
    Average L1 Active Cycles         cycle   1292455.67
    Total L1 Elapsed Cycles          cycle     75131346
    Average L2 Active Cycles         cycle   1165201.50
    Total L2 Elapsed Cycles          cycle     31977240
    Average SM Active Cycles         cycle   1292455.67
    Total SM Elapsed Cycles          cycle     75131346
    Average SMSP Active Cycles       cycle   1292464.19
    Total SMSP Elapsed Cycles        cycle    300525384
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.33
    Elapsed Cycles                cycle       392341
    Memory Throughput                 %        87.23
    DRAM Throughput                   %        87.23
    Duration                         us       483.97
    L1/TEX Cache Throughput           %        48.75
    L2 Cache Throughput               %        57.78
    SM Active Cycles              cycle    384097.86
    Compute (SM) Throughput           %        30.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.98
    Achieved Active Warps Per SM           warp        45.59
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2635845.33
    Total DRAM Elapsed Cycles        cycle     18130944
    Average L1 Active Cycles         cycle    384097.86
    Total L1 Elapsed Cycles          cycle     22422534
    Average L2 Active Cycles         cycle    394875.62
    Total L2 Elapsed Cycles          cycle      9581496
    Average SM Active Cycles         cycle    384097.86
    Total SM Elapsed Cycles          cycle     22422534
    Average SMSP Active Cycles       cycle    380991.72
    Total SMSP Elapsed Cycles        cycle     89690136
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.89
    Elapsed Cycles                cycle      1207258
    Memory Throughput                 %        84.81
    DRAM Throughput                   %        23.15
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.42
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1158636.31
    Compute (SM) Throughput           %        84.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.98
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148450.67
    Total DRAM Elapsed Cycles        cycle     55685120
    Average L1 Active Cycles         cycle   1158636.31
    Total L1 Elapsed Cycles          cycle     69263058
    Average L2 Active Cycles         cycle   1203544.83
    Total L2 Elapsed Cycles          cycle     29426472
    Average SM Active Cycles         cycle   1158636.31
    Total SM Elapsed Cycles          cycle     69263058
    Average SMSP Active Cycles       cycle   1157986.33
    Total SMSP Elapsed Cycles        cycle    277052232
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.50
    Elapsed Cycles                cycle         5489
    Memory Throughput                 %        28.52
    DRAM Throughput                   %        28.52
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        19.44
    L2 Cache Throughput               %        15.79
    SM Active Cycles              cycle      2906.88
    Compute (SM) Throughput           %        11.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.82
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12120
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2906.88
    Total L1 Elapsed Cycles          cycle       293738
    Average L2 Active Cycles         cycle      2393.29
    Total L2 Elapsed Cycles          cycle       135096
    Average SM Active Cycles         cycle      2906.88
    Total SM Elapsed Cycles          cycle       293738
    Average SMSP Active Cycles       cycle      2785.01
    Total SMSP Elapsed Cycles        cycle      1174952
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.347%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.72% above the average, while the minimum instance value is 21.33% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       786.50
    Elapsed Cycles                cycle         5716
    Memory Throughput                 %        31.70
    DRAM Throughput                   %        31.70
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.46
    L2 Cache Throughput               %        17.84
    SM Active Cycles              cycle      3236.47
    Compute (SM) Throughput           %        10.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.57
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13960
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3236.47
    Total L1 Elapsed Cycles          cycle       309706
    Average L2 Active Cycles         cycle      2675.29
    Total L2 Elapsed Cycles          cycle       140280
    Average SM Active Cycles         cycle      3236.47
    Total SM Elapsed Cycles          cycle       309706
    Average SMSP Active Cycles       cycle      3253.10
    Total SMSP Elapsed Cycles        cycle      1238824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.10
    Elapsed Cycles                cycle         5727
    Memory Throughput                 %        32.24
    DRAM Throughput                   %        32.24
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.33
    L2 Cache Throughput               %        18.15
    SM Active Cycles              cycle      3260.38
    Compute (SM) Throughput           %        10.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.26
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3260.38
    Total L1 Elapsed Cycles          cycle       301368
    Average L2 Active Cycles         cycle      2664.25
    Total L2 Elapsed Cycles          cycle       137904
    Average SM Active Cycles         cycle      3260.38
    Total SM Elapsed Cycles          cycle       301368
    Average SMSP Active Cycles       cycle      3159.01
    Total SMSP Elapsed Cycles        cycle      1205472
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.152%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.21% above the average, while the minimum instance value is 15.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.079%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.35% above the average, while the minimum instance value is 10.48% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.152%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.21% above the average, while the minimum instance value is 15.19% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       800.40
    Elapsed Cycles                cycle         5713
    Memory Throughput                 %        32.60
    DRAM Throughput                   %        32.60
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        17.94
    L2 Cache Throughput               %        18.20
    SM Active Cycles              cycle      3149.05
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.38
    Achieved Active Warps Per SM           warp        36.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14021.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3149.05
    Total L1 Elapsed Cycles          cycle       305972
    Average L2 Active Cycles         cycle      2534.33
    Total L2 Elapsed Cycles          cycle       137520
    Average SM Active Cycles         cycle      3149.05
    Total SM Elapsed Cycles          cycle       305972
    Average SMSP Active Cycles       cycle      3069.98
    Total SMSP Elapsed Cycles        cycle      1223888
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       833.26
    Elapsed Cycles                cycle      1313242
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.41
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292678.81
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.54
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1206149.33
    Total DRAM Elapsed Cycles        cycle     58293248
    Average L1 Active Cycles         cycle   1292678.81
    Total L1 Elapsed Cycles          cycle     75115572
    Average L2 Active Cycles         cycle   1147641.50
    Total L2 Elapsed Cycles          cycle     31305576
    Average SM Active Cycles         cycle   1292678.81
    Total SM Elapsed Cycles          cycle     75115572
    Average SMSP Active Cycles       cycle   1292553.48
    Total SMSP Elapsed Cycles        cycle    300462288
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.11
    Elapsed Cycles                cycle       389743
    Memory Throughput                 %        87.36
    DRAM Throughput                   %        87.36
    Duration                         us       480.77
    L1/TEX Cache Throughput           %        48.48
    L2 Cache Throughput               %        58.19
    SM Active Cycles              cycle    382960.86
    Compute (SM) Throughput           %        30.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.72
    Achieved Active Warps Per SM           warp        45.46
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2621853.33
    Total DRAM Elapsed Cycles        cycle     18008064
    Average L1 Active Cycles         cycle    382960.86
    Total L1 Elapsed Cycles          cycle     22581116
    Average L2 Active Cycles         cycle    397916.54
    Total L2 Elapsed Cycles          cycle      9517848
    Average SM Active Cycles         cycle    382960.86
    Total SM Elapsed Cycles          cycle     22581116
    Average SMSP Active Cycles       cycle    385261.29
    Total SMSP Elapsed Cycles        cycle     90324464
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.08
    Elapsed Cycles                cycle      1213761
    Memory Throughput                 %        84.87
    DRAM Throughput                   %        23.06
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158180.33
    Compute (SM) Throughput           %        84.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.99
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2150557.33
    Total DRAM Elapsed Cycles        cycle     55944192
    Average L1 Active Cycles         cycle   1158180.33
    Total L1 Elapsed Cycles          cycle     69213928
    Average L2 Active Cycles         cycle   1205220.83
    Total L2 Elapsed Cycles          cycle     29563056
    Average SM Active Cycles         cycle   1158180.33
    Total SM Elapsed Cycles          cycle     69213928
    Average SMSP Active Cycles       cycle   1158386.28
    Total SMSP Elapsed Cycles        cycle    276855712
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.82
    Elapsed Cycles                cycle         5487
    Memory Throughput                 %        29.03
    DRAM Throughput                   %        29.03
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        19.63
    L2 Cache Throughput               %        15.80
    SM Active Cycles              cycle      2877.38
    Compute (SM) Throughput           %        11.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.39
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12338.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2877.38
    Total L1 Elapsed Cycles          cycle       287532
    Average L2 Active Cycles         cycle         2453
    Total L2 Elapsed Cycles          cycle       135072
    Average SM Active Cycles         cycle      2877.38
    Total SM Elapsed Cycles          cycle       287532
    Average SMSP Active Cycles       cycle      2799.70
    Total SMSP Elapsed Cycles        cycle      1150128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.049%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.48% above the average, while the minimum instance value is 22.38% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.9%                                                                                            
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.54% above the average, while the minimum instance value is 4.69% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       783.56
    Elapsed Cycles                cycle         5689
    Memory Throughput                 %        31.86
    DRAM Throughput                   %        31.86
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.57
    L2 Cache Throughput               %        17.90
    SM Active Cycles              cycle      3214.83
    Compute (SM) Throughput           %        10.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.48
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3214.83
    Total L1 Elapsed Cycles          cycle       299968
    Average L2 Active Cycles         cycle      2634.83
    Total L2 Elapsed Cycles          cycle       139704
    Average SM Active Cycles         cycle      3214.83
    Total SM Elapsed Cycles          cycle       299968
    Average SMSP Active Cycles       cycle      3161.29
    Total SMSP Elapsed Cycles        cycle      1199872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.212%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.53% above the average, while the minimum instance value is 12.47% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.47
    Elapsed Cycles                cycle         5700
    Memory Throughput                 %        31.68
    DRAM Throughput                   %        31.68
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.62
    L2 Cache Throughput               %        17.90
    SM Active Cycles              cycle      3206.22
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.63
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14058.67
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3206.22
    Total L1 Elapsed Cycles          cycle       310014
    Average L2 Active Cycles         cycle      2639.25
    Total L2 Elapsed Cycles          cycle       139776
    Average SM Active Cycles         cycle      3206.22
    Total SM Elapsed Cycles          cycle       310014
    Average SMSP Active Cycles       cycle      3132.63
    Total SMSP Elapsed Cycles        cycle      1240056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.238%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.73% above the average, while the minimum instance value is 8.15% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.238%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.73% above the average, while the minimum instance value is 8.15% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         5614
    Memory Throughput                 %        32.22
    DRAM Throughput                   %        32.22
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.50
    L2 Cache Throughput               %        18.15
    SM Active Cycles              cycle      3227.69
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.94
    Achieved Active Warps Per SM           warp        34.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14021.33
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3227.69
    Total L1 Elapsed Cycles          cycle       307806
    Average L2 Active Cycles         cycle      2624.67
    Total L2 Elapsed Cycles          cycle       137760
    Average SM Active Cycles         cycle      3227.69
    Total SM Elapsed Cycles          cycle       307806
    Average SMSP Active Cycles       cycle      3109.87
    Total SMSP Elapsed Cycles        cycle      1231224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.608%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.57% above the average, while the minimum instance value is 10.80% below   
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.62
    Elapsed Cycles                cycle      1311849
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.59
    SM Active Cycles              cycle   1292611.07
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203544
    Total DRAM Elapsed Cycles        cycle     60504064
    Average L1 Active Cycles         cycle   1292611.07
    Total L1 Elapsed Cycles          cycle     75114028
    Average L2 Active Cycles         cycle   1165805.50
    Total L2 Elapsed Cycles          cycle     31973040
    Average SM Active Cycles         cycle   1292611.07
    Total SM Elapsed Cycles          cycle     75114028
    Average SMSP Active Cycles       cycle   1292411.34
    Total SMSP Elapsed Cycles        cycle    300456112
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.40
    Elapsed Cycles                cycle       390136
    Memory Throughput                 %        87.12
    DRAM Throughput                   %        87.12
    Duration                         us       481.02
    L1/TEX Cache Throughput           %        48.72
    L2 Cache Throughput               %        58.08
    SM Active Cycles              cycle    382190.60
    Compute (SM) Throughput           %        30.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.02
    Achieved Active Warps Per SM           warp        45.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2616661.33
    Total DRAM Elapsed Cycles        cycle     18020352
    Average L1 Active Cycles         cycle    382190.60
    Total L1 Elapsed Cycles          cycle     22439084
    Average L2 Active Cycles         cycle    394485.79
    Total L2 Elapsed Cycles          cycle      9523104
    Average SM Active Cycles         cycle    382190.60
    Total SM Elapsed Cycles          cycle     22439084
    Average SMSP Active Cycles       cycle    380381.70
    Total SMSP Elapsed Cycles        cycle     89756336
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.00
    Elapsed Cycles                cycle      1210657
    Memory Throughput                 %        84.55
    DRAM Throughput                   %        23.10
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.39
    L2 Cache Throughput               %        13.94
    SM Active Cycles              cycle   1158951.91
    Compute (SM) Throughput           %        84.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148752
    Total DRAM Elapsed Cycles        cycle     55818240
    Average L1 Active Cycles         cycle   1158951.91
    Total L1 Elapsed Cycles          cycle     69481976
    Average L2 Active Cycles         cycle   1204732.75
    Total L2 Elapsed Cycles          cycle     29497344
    Average SM Active Cycles         cycle   1158951.91
    Total SM Elapsed Cycles          cycle     69481976
    Average SMSP Active Cycles       cycle   1156831.79
    Total SMSP Elapsed Cycles        cycle    277927904
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.30
    Elapsed Cycles                cycle         5408
    Memory Throughput                 %        28.86
    DRAM Throughput                   %        28.86
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.80
    L2 Cache Throughput               %        16.08
    SM Active Cycles              cycle         2853
    Compute (SM) Throughput           %        11.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.02
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12117.33
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle         2853
    Total L1 Elapsed Cycles          cycle       292668
    Average L2 Active Cycles         cycle      2384.83
    Total L2 Elapsed Cycles          cycle       132624
    Average SM Active Cycles         cycle         2853
    Total SM Elapsed Cycles          cycle       292668
    Average SMSP Active Cycles       cycle      2830.05
    Total SMSP Elapsed Cycles        cycle      1170672
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       786.49
    Elapsed Cycles                cycle         5799
    Memory Throughput                 %        31.39
    DRAM Throughput                   %        31.39
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        18.23
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3099.22
    Compute (SM) Throughput           %        10.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.05
    Achieved Active Warps Per SM           warp        36.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3099.22
    Total L1 Elapsed Cycles          cycle       300014
    Average L2 Active Cycles         cycle         2565
    Total L2 Elapsed Cycles          cycle       141984
    Average SM Active Cycles         cycle      3099.22
    Total SM Elapsed Cycles          cycle       300014
    Average SMSP Active Cycles       cycle      3033.12
    Total SMSP Elapsed Cycles        cycle      1200056
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       796.23
    Elapsed Cycles                cycle         5861
    Memory Throughput                 %        31.59
    DRAM Throughput                   %        31.59
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.40
    L2 Cache Throughput               %        17.74
    SM Active Cycles              cycle      3246.76
    Compute (SM) Throughput           %        10.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.38
    Achieved Active Warps Per SM           warp        36.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3246.76
    Total L1 Elapsed Cycles          cycle       318082
    Average L2 Active Cycles         cycle      2628.96
    Total L2 Elapsed Cycles          cycle       141072
    Average SM Active Cycles         cycle      3246.76
    Total SM Elapsed Cycles          cycle       318082
    Average SMSP Active Cycles       cycle      3152.16
    Total SMSP Elapsed Cycles        cycle      1272328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.499%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.31% above the average, while the minimum instance value is 11.65% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       832.89
    Elapsed Cycles                cycle         5750
    Memory Throughput                 %        33.34
    DRAM Throughput                   %        33.34
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        17.57
    L2 Cache Throughput               %        18.29
    SM Active Cycles              cycle      3215.76
    Compute (SM) Throughput           %        10.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.53
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13997.33
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      3215.76
    Total L1 Elapsed Cycles          cycle       304302
    Average L2 Active Cycles         cycle      2563.46
    Total L2 Elapsed Cycles          cycle       136680
    Average SM Active Cycles         cycle      3215.76
    Total SM Elapsed Cycles          cycle       304302
    Average SMSP Active Cycles       cycle      3084.36
    Total SMSP Elapsed Cycles        cycle      1217208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.031%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.56% above the average, while the minimum instance value is 9.84% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.25
    Elapsed Cycles                cycle      1313017
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.27
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292516.38
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203413.33
    Total DRAM Elapsed Cycles        cycle     58845184
    Average L1 Active Cycles         cycle   1292516.38
    Total L1 Elapsed Cycles          cycle     75120290
    Average L2 Active Cycles         cycle   1149376.08
    Total L2 Elapsed Cycles          cycle     31314000
    Average SM Active Cycles         cycle   1292516.38
    Total SM Elapsed Cycles          cycle     75120290
    Average SMSP Active Cycles       cycle   1292536.81
    Total SMSP Elapsed Cycles        cycle    300481160
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       820.78
    Elapsed Cycles                cycle       398954
    Memory Throughput                 %        87.54
    DRAM Throughput                   %        87.54
    Duration                         us       479.49
    L1/TEX Cache Throughput           %        47.92
    L2 Cache Throughput               %        57.99
    SM Active Cycles              cycle       391394
    Compute (SM) Throughput           %        30.09
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.64
    Achieved Active Warps Per SM           warp        45.43
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2620568
    Total DRAM Elapsed Cycles        cycle     17960960
    Average L1 Active Cycles         cycle       391394
    Total L1 Elapsed Cycles          cycle     22811846
    Average L2 Active Cycles         cycle    396510.42
    Total L2 Elapsed Cycles          cycle      9531240
    Average SM Active Cycles         cycle       391394
    Total SM Elapsed Cycles          cycle     22811846
    Average SMSP Active Cycles       cycle    388913.66
    Total SMSP Elapsed Cycles        cycle     91247384
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       823.71
    Elapsed Cycles                cycle      1216059
    Memory Throughput                 %        85.52
    DRAM Throughput                   %        23.60
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.35
    L2 Cache Throughput               %        14.16
    SM Active Cycles              cycle   1159580.69
    Compute (SM) Throughput           %        85.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148104
    Total DRAM Elapsed Cycles        cycle     54614016
    Average L1 Active Cycles         cycle   1159580.69
    Total L1 Elapsed Cycles          cycle     68692196
    Average L2 Active Cycles         cycle      1212959
    Total L2 Elapsed Cycles          cycle     29011896
    Average SM Active Cycles         cycle   1159580.69
    Total SM Elapsed Cycles          cycle     68692196
    Average SMSP Active Cycles       cycle   1158643.09
    Total SMSP Elapsed Cycles        cycle    274768784
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       778.88
    Elapsed Cycles                cycle         5463
    Memory Throughput                 %        28.68
    DRAM Throughput                   %        28.68
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        19.35
    L2 Cache Throughput               %        15.69
    SM Active Cycles              cycle      2920.09
    Compute (SM) Throughput           %        11.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.89
    Achieved Active Warps Per SM           warp        34.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12333.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      2920.09
    Total L1 Elapsed Cycles          cycle       292204
    Average L2 Active Cycles         cycle      2479.83
    Total L2 Elapsed Cycles          cycle       136056
    Average SM Active Cycles         cycle      2920.09
    Total SM Elapsed Cycles          cycle       292204
    Average SMSP Active Cycles       cycle      2853.74
    Total SMSP Elapsed Cycles        cycle      1168816
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.632%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.16% above the average, while the minimum instance value is 4.27% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.34
    Elapsed Cycles                cycle         5861
    Memory Throughput                 %        30.91
    DRAM Throughput                   %        30.91
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        18.05
    L2 Cache Throughput               %        17.26
    SM Active Cycles              cycle      3130.74
    Compute (SM) Throughput           %        10.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.90
    Achieved Active Warps Per SM           warp        36.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14085.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3130.74
    Total L1 Elapsed Cycles          cycle       312088
    Average L2 Active Cycles         cycle      2588.12
    Total L2 Elapsed Cycles          cycle       144768
    Average SM Active Cycles         cycle      3130.74
    Total SM Elapsed Cycles          cycle       312088
    Average SMSP Active Cycles       cycle      3088.77
    Total SMSP Elapsed Cycles        cycle      1248352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.24
    Elapsed Cycles                cycle         5823
    Memory Throughput                 %        30.86
    DRAM Throughput                   %        30.86
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.86
    L2 Cache Throughput               %        17.45
    SM Active Cycles              cycle      3163.40
    Compute (SM) Throughput           %        10.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.83
    Achieved Active Warps Per SM           warp        36.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3163.40
    Total L1 Elapsed Cycles          cycle       315430
    Average L2 Active Cycles         cycle      2638.71
    Total L2 Elapsed Cycles          cycle       143184
    Average SM Active Cycles         cycle      3163.40
    Total SM Elapsed Cycles          cycle       315430
    Average SMSP Active Cycles       cycle      3206.62
    Total SMSP Elapsed Cycles        cycle      1261720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.061%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.28% above the average, while the minimum instance value is 9.06% below   
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.065%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.45% above the average, while the minimum instance value is 6.43% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       795.96
    Elapsed Cycles                cycle         5828
    Memory Throughput                 %        31.59
    DRAM Throughput                   %        31.59
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.72
    L2 Cache Throughput               %        17.75
    SM Active Cycles              cycle      3188.81
    Compute (SM) Throughput           %        10.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.48
    Achieved Active Warps Per SM           warp        36.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14072
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3188.81
    Total L1 Elapsed Cycles          cycle       305394
    Average L2 Active Cycles         cycle      2580.67
    Total L2 Elapsed Cycles          cycle       140856
    Average SM Active Cycles         cycle      3188.81
    Total SM Elapsed Cycles          cycle       305394
    Average SMSP Active Cycles       cycle      3090.47
    Total SMSP Elapsed Cycles        cycle      1221576
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.69
    Elapsed Cycles                cycle      1313176
    Memory Throughput                 %        82.37
    DRAM Throughput                   %        12.33
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292485.26
    Compute (SM) Throughput           %        82.37
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203202.67
    Total DRAM Elapsed Cycles        cycle     58530816
    Average L1 Active Cycles         cycle   1292485.26
    Total L1 Elapsed Cycles          cycle     75110390
    Average L2 Active Cycles         cycle   1173675.92
    Total L2 Elapsed Cycles          cycle     31295928
    Average SM Active Cycles         cycle   1292485.26
    Total SM Elapsed Cycles          cycle     75110390
    Average SMSP Active Cycles       cycle   1292495.19
    Total SMSP Elapsed Cycles        cycle    300441560
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.69
    Elapsed Cycles                cycle       385597
    Memory Throughput                 %        87.22
    DRAM Throughput                   %        87.22
    Duration                         us       483.42
    L1/TEX Cache Throughput           %        49.20
    L2 Cache Throughput               %        57.84
    SM Active Cycles              cycle    380060.57
    Compute (SM) Throughput           %        30.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.03
    Achieved Active Warps Per SM           warp        45.62
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2632613.33
    Total DRAM Elapsed Cycles        cycle     18109440
    Average L1 Active Cycles         cycle    380060.57
    Total L1 Elapsed Cycles          cycle     22230688
    Average L2 Active Cycles         cycle    395776.04
    Total L2 Elapsed Cycles          cycle      9570144
    Average SM Active Cycles         cycle    380060.57
    Total SM Elapsed Cycles          cycle     22230688
    Average SMSP Active Cycles       cycle    378523.13
    Total SMSP Elapsed Cycles        cycle     88922752
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       821.36
    Elapsed Cycles                cycle      1216832
    Memory Throughput                 %        84.44
    DRAM Throughput                   %        23.53
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.41
    L2 Cache Throughput               %        14.15
    SM Active Cycles              cycle   1158744.62
    Compute (SM) Throughput           %        84.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.99
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148477.33
    Total DRAM Elapsed Cycles        cycle     54786048
    Average L1 Active Cycles         cycle   1158744.62
    Total L1 Elapsed Cycles          cycle     69567346
    Average L2 Active Cycles         cycle   1184702.08
    Total L2 Elapsed Cycles          cycle     29039328
    Average SM Active Cycles         cycle   1158744.62
    Total SM Elapsed Cycles          cycle     69567346
    Average SMSP Active Cycles       cycle   1157973.02
    Total SMSP Elapsed Cycles        cycle    278269384
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       798.56
    Elapsed Cycles                cycle         5513
    Memory Throughput                 %        28.82
    DRAM Throughput                   %        28.82
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.65
    L2 Cache Throughput               %        16.08
    SM Active Cycles              cycle      2874.55
    Compute (SM) Throughput           %        11.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.56
    Achieved Active Warps Per SM           warp        35.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12098.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2874.55
    Total L1 Elapsed Cycles          cycle       289792
    Average L2 Active Cycles         cycle      2475.04
    Total L2 Elapsed Cycles          cycle       132672
    Average SM Active Cycles         cycle      2874.55
    Total SM Elapsed Cycles          cycle       289792
    Average SMSP Active Cycles       cycle      2811.72
    Total SMSP Elapsed Cycles        cycle      1159168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.247%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.12% above the average, while the minimum instance value is 14.80% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.625%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.00% above the average, while the minimum instance value is 23.00% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.247%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.12% above the average, while the minimum instance value is 14.80% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.808%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.97% above the average, while the minimum instance value is 6.30% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.79
    Elapsed Cycles                cycle         5728
    Memory Throughput                 %        31.30
    DRAM Throughput                   %        31.30
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.48
    L2 Cache Throughput               %        17.55
    SM Active Cycles              cycle      3231.48
    Compute (SM) Throughput           %        10.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.78
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14101.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3231.48
    Total L1 Elapsed Cycles          cycle       305324
    Average L2 Active Cycles         cycle      2666.71
    Total L2 Elapsed Cycles          cycle       142512
    Average SM Active Cycles         cycle      3231.48
    Total SM Elapsed Cycles          cycle       305324
    Average SMSP Active Cycles       cycle      3160.25
    Total SMSP Elapsed Cycles        cycle      1221296
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.058%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.43% above the average, while the minimum instance value is 8.58% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       808.20
    Elapsed Cycles                cycle         5869
    Memory Throughput                 %        31.84
    DRAM Throughput                   %        31.84
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.48
    L2 Cache Throughput               %        17.84
    SM Active Cycles              cycle      3231.24
    Compute (SM) Throughput           %        10.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.82
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3231.24
    Total L1 Elapsed Cycles          cycle       308946
    Average L2 Active Cycles         cycle      2649.92
    Total L2 Elapsed Cycles          cycle       140112
    Average SM Active Cycles         cycle      3231.24
    Total SM Elapsed Cycles          cycle       308946
    Average SMSP Active Cycles       cycle      3147.26
    Total SMSP Elapsed Cycles        cycle      1235784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.024%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.28% above the average, while the minimum instance value is 8.70% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.659%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.96% above the average, while the minimum instance value is 18.25% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.024%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.28% above the average, while the minimum instance value is 8.70% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       794.69
    Elapsed Cycles                cycle         5641
    Memory Throughput                 %        32.57
    DRAM Throughput                   %        32.57
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        17.08
    L2 Cache Throughput               %        18.33
    SM Active Cycles              cycle      3307.60
    Compute (SM) Throughput           %        10.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.60
    Achieved Active Warps Per SM           warp        33.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14008
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3307.60
    Total L1 Elapsed Cycles          cycle       310502
    Average L2 Active Cycles         cycle      2642.08
    Total L2 Elapsed Cycles          cycle       136320
    Average SM Active Cycles         cycle      3307.60
    Total SM Elapsed Cycles          cycle       310502
    Average SMSP Active Cycles       cycle      3149.53
    Total SMSP Elapsed Cycles        cycle      1242008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.136%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.04% above the average, while the minimum instance value is 6.36% below the       
          average.                                                                                                      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.24
    Elapsed Cycles                cycle      1297500
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.83
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.48
    SM Active Cycles              cycle   1292432.59
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203410.67
    Total DRAM Elapsed Cycles        cycle     61049856
    Average L1 Active Cycles         cycle   1292432.59
    Total L1 Elapsed Cycles          cycle     75128418
    Average L2 Active Cycles         cycle   1173264.58
    Total L2 Elapsed Cycles          cycle     32260944
    Average SM Active Cycles         cycle   1292432.59
    Total SM Elapsed Cycles          cycle     75128418
    Average SMSP Active Cycles       cycle   1292351.24
    Total SMSP Elapsed Cycles        cycle    300513672
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.34
    Elapsed Cycles                cycle       389736
    Memory Throughput                 %        87.32
    DRAM Throughput                   %        87.32
    Duration                         us       480.64
    L1/TEX Cache Throughput           %        48.67
    L2 Cache Throughput               %        58.11
    SM Active Cycles              cycle    381859.86
    Compute (SM) Throughput           %        30.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.94
    Achieved Active Warps Per SM           warp        45.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2620216
    Total DRAM Elapsed Cycles        cycle     18004992
    Average L1 Active Cycles         cycle    381859.86
    Total L1 Elapsed Cycles          cycle     22453040
    Average L2 Active Cycles         cycle    397355.04
    Total L2 Elapsed Cycles          cycle      9515064
    Average SM Active Cycles         cycle    381859.86
    Total SM Elapsed Cycles          cycle     22453040
    Average SMSP Active Cycles       cycle    383570.25
    Total SMSP Elapsed Cycles        cycle     89812160
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.11
    Elapsed Cycles                cycle      1208108
    Memory Throughput                 %        84.56
    DRAM Throughput                   %        23.16
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.50
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1157484.28
    Compute (SM) Throughput           %        84.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.03
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2149224
    Total DRAM Elapsed Cycles        cycle     55685120
    Average L1 Active Cycles         cycle   1157484.28
    Total L1 Elapsed Cycles          cycle     69470624
    Average L2 Active Cycles         cycle   1205195.50
    Total L2 Elapsed Cycles          cycle     29426256
    Average SM Active Cycles         cycle   1157484.28
    Total SM Elapsed Cycles          cycle     69470624
    Average SMSP Active Cycles       cycle   1158368.72
    Total SMSP Elapsed Cycles        cycle    277882496
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.60
    Elapsed Cycles                cycle         5509
    Memory Throughput                 %        28.77
    DRAM Throughput                   %        28.77
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        18.89
    L2 Cache Throughput               %        15.78
    SM Active Cycles              cycle      2990.53
    Compute (SM) Throughput           %        11.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.58
    Achieved Active Warps Per SM           warp        33.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12322.67
    Total DRAM Elapsed Cycles        cycle       257024
    Average L1 Active Cycles         cycle      2990.53
    Total L1 Elapsed Cycles          cycle       288602
    Average L2 Active Cycles         cycle      2461.50
    Total L2 Elapsed Cycles          cycle       135024
    Average SM Active Cycles         cycle      2990.53
    Total SM Elapsed Cycles          cycle       288602
    Average SMSP Active Cycles       cycle      2834.70
    Total SMSP Elapsed Cycles        cycle      1154408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.621%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.87% above the average, while the minimum instance value is 25.28% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.41%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.65% above the average, while the minimum instance value is 4.77% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz          785
    Elapsed Cycles                cycle         5700
    Memory Throughput                 %        31.91
    DRAM Throughput                   %        31.91
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.70
    L2 Cache Throughput               %        17.86
    SM Active Cycles              cycle      3191.26
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14050.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3191.26
    Total L1 Elapsed Cycles          cycle       305970
    Average L2 Active Cycles         cycle      2657.04
    Total L2 Elapsed Cycles          cycle       139920
    Average SM Active Cycles         cycle      3191.26
    Total SM Elapsed Cycles          cycle       305970
    Average SMSP Active Cycles       cycle      3188.45
    Total SMSP Elapsed Cycles        cycle      1223880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.301%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.77% above the average, while the minimum instance value is 15.35% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.91
    Elapsed Cycles                cycle         5779
    Memory Throughput                 %        31.26
    DRAM Throughput                   %        31.26
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        18.21
    L2 Cache Throughput               %        17.57
    SM Active Cycles              cycle      3103.16
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.12
    Achieved Active Warps Per SM           warp        36.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3103.16
    Total L1 Elapsed Cycles          cycle       306262
    Average L2 Active Cycles         cycle      2629.83
    Total L2 Elapsed Cycles          cycle       142128
    Average SM Active Cycles         cycle      3103.16
    Total SM Elapsed Cycles          cycle       306262
    Average SMSP Active Cycles       cycle      3131.34
    Total SMSP Elapsed Cycles        cycle      1225048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.623%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.17% above the average, while the minimum instance value is 19.01% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       786.51
    Elapsed Cycles                cycle         5795
    Memory Throughput                 %        31.41
    DRAM Throughput                   %        31.41
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.71
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3190.14
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.28
    Achieved Active Warps Per SM           warp        36.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3190.14
    Total L1 Elapsed Cycles          cycle       307638
    Average L2 Active Cycles         cycle      2661.17
    Total L2 Elapsed Cycles          cycle       141816
    Average SM Active Cycles         cycle      3190.14
    Total SM Elapsed Cycles          cycle       307638
    Average SMSP Active Cycles       cycle      3174.13
    Total SMSP Elapsed Cycles        cycle      1230552
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.76
    Elapsed Cycles                cycle      1311742
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292429.09
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1205349.33
    Total DRAM Elapsed Cycles        cycle     60506112
    Average L1 Active Cycles         cycle   1292429.09
    Total L1 Elapsed Cycles          cycle     75139396
    Average L2 Active Cycles         cycle   1166472.96
    Total L2 Elapsed Cycles          cycle     31975104
    Average SM Active Cycles         cycle   1292429.09
    Total SM Elapsed Cycles          cycle     75139396
    Average SMSP Active Cycles       cycle   1292420.22
    Total SMSP Elapsed Cycles        cycle    300557584
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.92
    Elapsed Cycles                cycle       394488
    Memory Throughput                 %        87.15
    DRAM Throughput                   %        87.15
    Duration                         us       486.18
    L1/TEX Cache Throughput           %        48.87
    L2 Cache Throughput               %        57.52
    SM Active Cycles              cycle    384620.02
    Compute (SM) Throughput           %        30.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.30
    Achieved Active Warps Per SM           warp        45.75
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2645197.33
    Total DRAM Elapsed Cycles        cycle     18210816
    Average L1 Active Cycles         cycle    384620.02
    Total L1 Elapsed Cycles          cycle     22363378
    Average L2 Active Cycles         cycle    395822.71
    Total L2 Elapsed Cycles          cycle      9625080
    Average SM Active Cycles         cycle    384620.02
    Total SM Elapsed Cycles          cycle     22363378
    Average SMSP Active Cycles       cycle    381636.50
    Total SMSP Elapsed Cycles        cycle     89453512
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.21
    Elapsed Cycles                cycle      1208686
    Memory Throughput                 %        84.54
    DRAM Throughput                   %        23.13
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.47
    L2 Cache Throughput               %        13.97
    SM Active Cycles              cycle   1157888.36
    Compute (SM) Throughput           %        84.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148466.67
    Total DRAM Elapsed Cycles        cycle     55720960
    Average L1 Active Cycles         cycle   1157888.36
    Total L1 Elapsed Cycles          cycle     69485958
    Average L2 Active Cycles         cycle   1204467.33
    Total L2 Elapsed Cycles          cycle     29445840
    Average SM Active Cycles         cycle   1157888.36
    Total SM Elapsed Cycles          cycle     69485958
    Average SMSP Active Cycles       cycle   1158018.62
    Total SMSP Elapsed Cycles        cycle    277943832
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.16
    Elapsed Cycles                cycle         5397
    Memory Throughput                 %        28.75
    DRAM Throughput                   %        28.75
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.19
    L2 Cache Throughput               %        16.08
    SM Active Cycles              cycle      2943.66
    Compute (SM) Throughput           %        11.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.83
    Achieved Active Warps Per SM           warp        34.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12072
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2943.66
    Total L1 Elapsed Cycles          cycle       290544
    Average L2 Active Cycles         cycle      2487.96
    Total L2 Elapsed Cycles          cycle       132744
    Average SM Active Cycles         cycle      2943.66
    Total SM Elapsed Cycles          cycle       290544
    Average SMSP Active Cycles       cycle      2848.44
    Total SMSP Elapsed Cycles        cycle      1162176
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.897%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.03% above the average, while the minimum instance value is 20.74% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.476%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.63% above the average, while the minimum instance value is 20.83% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.897%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.03% above the average, while the minimum instance value is 20.74% below the      
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.92
    Elapsed Cycles                cycle         5698
    Memory Throughput                 %        31.54
    DRAM Throughput                   %        31.54
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.48
    L2 Cache Throughput               %        17.88
    SM Active Cycles              cycle      3231.79
    Compute (SM) Throughput           %        10.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14050.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3231.79
    Total L1 Elapsed Cycles          cycle       309246
    Average L2 Active Cycles         cycle      2700.29
    Total L2 Elapsed Cycles          cycle       139968
    Average SM Active Cycles         cycle      3231.79
    Total SM Elapsed Cycles          cycle       309246
    Average SMSP Active Cycles       cycle      3125.14
    Total SMSP Elapsed Cycles        cycle      1236984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.565%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.49% above the average, while the minimum instance value is 12.80% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.82%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 16.89% above the average, while the minimum instance value is 8.08% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.58
    Elapsed Cycles                cycle         5872
    Memory Throughput                 %        30.84
    DRAM Throughput                   %        30.84
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        17.41
    SM Active Cycles              cycle      3278.83
    Compute (SM) Throughput           %        10.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.59
    Achieved Active Warps Per SM           warp        34.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14053.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3278.83
    Total L1 Elapsed Cycles          cycle       310918
    Average L2 Active Cycles         cycle      2665.25
    Total L2 Elapsed Cycles          cycle       143880
    Average SM Active Cycles         cycle      3278.83
    Total SM Elapsed Cycles          cycle       310918
    Average SMSP Active Cycles       cycle      3178.25
    Total SMSP Elapsed Cycles        cycle      1243672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.78%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.45% above the average, while the minimum instance value is 10.36% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.192%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 12.13% above the average, while the minimum instance value is 13.63% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.78%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.45% above the average, while the minimum instance value is 10.36% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       820.27
    Elapsed Cycles                cycle         5902
    Memory Throughput                 %        32.20
    DRAM Throughput                   %        32.20
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.28
    L2 Cache Throughput               %        17.83
    SM Active Cycles              cycle      3269.07
    Compute (SM) Throughput           %        10.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.13
    Achieved Active Warps Per SM           warp        35.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14013.33
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3269.07
    Total L1 Elapsed Cycles          cycle       308350
    Average L2 Active Cycles         cycle      2607.96
    Total L2 Elapsed Cycles          cycle       140256
    Average SM Active Cycles         cycle      3269.07
    Total SM Elapsed Cycles          cycle       308350
    Average SMSP Active Cycles       cycle      3197.94
    Total SMSP Elapsed Cycles        cycle      1233400
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.81
    Elapsed Cycles                cycle      1313534
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.28
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle      1292568
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203645.33
    Total DRAM Elapsed Cycles        cycle     58815488
    Average L1 Active Cycles         cycle      1292568
    Total L1 Elapsed Cycles          cycle     75120532
    Average L2 Active Cycles         cycle      1147177
    Total L2 Elapsed Cycles          cycle     31324488
    Average SM Active Cycles         cycle      1292568
    Total SM Elapsed Cycles          cycle     75120532
    Average SMSP Active Cycles       cycle   1292735.03
    Total SMSP Elapsed Cycles        cycle    300482128
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.08
    Elapsed Cycles                cycle       399520
    Memory Throughput                 %        87.52
    DRAM Throughput                   %        87.52
    Duration                         us       476.48
    L1/TEX Cache Throughput           %        48.93
    L2 Cache Throughput               %        57.88
    SM Active Cycles              cycle    391494.64
    Compute (SM) Throughput           %        30.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.73
    Achieved Active Warps Per SM           warp        45.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2603546.67
    Total DRAM Elapsed Cycles        cycle     17848320
    Average L1 Active Cycles         cycle    391494.64
    Total L1 Elapsed Cycles          cycle     22332328
    Average L2 Active Cycles         cycle    396901.38
    Total L2 Elapsed Cycles          cycle      9551376
    Average SM Active Cycles         cycle    391494.64
    Total SM Elapsed Cycles          cycle     22332328
    Average SMSP Active Cycles       cycle    389722.03
    Total SMSP Elapsed Cycles        cycle     89329312
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.31
    Elapsed Cycles                cycle      1189640
    Memory Throughput                 %        85.21
    DRAM Throughput                   %        23.04
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.42
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158591.86
    Compute (SM) Throughput           %        85.21
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.97
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148048
    Total DRAM Elapsed Cycles        cycle     55949312
    Average L1 Active Cycles         cycle   1158591.86
    Total L1 Elapsed Cycles          cycle     68939398
    Average L2 Active Cycles         cycle   1213992.04
    Total L2 Elapsed Cycles          cycle     29566032
    Average SM Active Cycles         cycle   1158591.86
    Total SM Elapsed Cycles          cycle     68939398
    Average SMSP Active Cycles       cycle   1158127.53
    Total SMSP Elapsed Cycles        cycle    275757592
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       819.90
    Elapsed Cycles                cycle         5634
    Memory Throughput                 %        29.55
    DRAM Throughput                   %        29.55
    Duration                         us         6.78
    L1/TEX Cache Throughput           %        19.05
    L2 Cache Throughput               %        15.94
    SM Active Cycles              cycle      2965.21
    Compute (SM) Throughput           %        11.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.02
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12253.33
    Total DRAM Elapsed Cycles        cycle       248832
    Average L1 Active Cycles         cycle      2965.21
    Total L1 Elapsed Cycles          cycle       293172
    Average L2 Active Cycles         cycle      2486.17
    Total L2 Elapsed Cycles          cycle       133920
    Average SM Active Cycles         cycle      2965.21
    Total SM Elapsed Cycles          cycle       293172
    Average SMSP Active Cycles       cycle      2925.43
    Total SMSP Elapsed Cycles        cycle      1172688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.354%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.13% above the average, while the minimum instance value is 18.49% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.716%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.88% above the average, while the minimum instance value is 21.89% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.354%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.13% above the average, while the minimum instance value is 18.49% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.132%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 16.01% above the average, while the minimum instance value is 4.91% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       836.22
    Elapsed Cycles                cycle         6071
    Memory Throughput                 %        31.89
    DRAM Throughput                   %        31.89
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.12
    L2 Cache Throughput               %        17.33
    SM Active Cycles              cycle      3300.60
    Compute (SM) Throughput           %        10.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.37
    Achieved Active Warps Per SM           warp        36.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3300.60
    Total L1 Elapsed Cycles          cycle       317840
    Average L2 Active Cycles         cycle      2569.83
    Total L2 Elapsed Cycles          cycle       144264
    Average SM Active Cycles         cycle      3300.60
    Total SM Elapsed Cycles          cycle       317840
    Average SMSP Active Cycles       cycle      3141.91
    Total SMSP Elapsed Cycles        cycle      1271360
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.12
    Elapsed Cycles                cycle         5621
    Memory Throughput                 %        31.96
    DRAM Throughput                   %        31.96
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.42
    L2 Cache Throughput               %        17.90
    SM Active Cycles              cycle      3243.41
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.30
    Achieved Active Warps Per SM           warp        34.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14074.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3243.41
    Total L1 Elapsed Cycles          cycle       319330
    Average L2 Active Cycles         cycle      2657.62
    Total L2 Elapsed Cycles          cycle       139896
    Average SM Active Cycles         cycle      3243.41
    Total SM Elapsed Cycles          cycle       319330
    Average SMSP Active Cycles       cycle      3163.27
    Total SMSP Elapsed Cycles        cycle      1277320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.843%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.91% above the average, while the minimum instance value is 11.42% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.082%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.15% above the average, while the minimum instance value is 6.87% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.08
    Elapsed Cycles                cycle         5656
    Memory Throughput                 %        31.56
    DRAM Throughput                   %        31.56
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.67
    L2 Cache Throughput               %        17.76
    SM Active Cycles              cycle      3196.97
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.16
    Achieved Active Warps Per SM           warp        35.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14058.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3196.97
    Total L1 Elapsed Cycles          cycle       306354
    Average L2 Active Cycles         cycle      2560.92
    Total L2 Elapsed Cycles          cycle       140856
    Average SM Active Cycles         cycle      3196.97
    Total SM Elapsed Cycles          cycle       306354
    Average SMSP Active Cycles       cycle      3063.94
    Total SMSP Elapsed Cycles        cycle      1225416
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.84
    Elapsed Cycles                cycle      1311622
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.59
    SM Active Cycles              cycle   1292401.59
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.58
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203530.67
    Total DRAM Elapsed Cycles        cycle     60493824
    Average L1 Active Cycles         cycle   1292401.59
    Total L1 Elapsed Cycles          cycle     75120282
    Average L2 Active Cycles         cycle   1165015.75
    Total L2 Elapsed Cycles          cycle     31967952
    Average SM Active Cycles         cycle   1292401.59
    Total SM Elapsed Cycles          cycle     75120282
    Average SMSP Active Cycles       cycle   1292364.90
    Total SMSP Elapsed Cycles        cycle    300481128
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.56
    Elapsed Cycles                cycle       389463
    Memory Throughput                 %        87.28
    DRAM Throughput                   %        87.28
    Duration                         us       480.13
    L1/TEX Cache Throughput           %        48.67
    L2 Cache Throughput               %        58.20
    SM Active Cycles              cycle    383424.52
    Compute (SM) Throughput           %        30.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.73
    Achieved Active Warps Per SM           warp        45.47
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2616408
    Total DRAM Elapsed Cycles        cycle     17985536
    Average L1 Active Cycles         cycle    383424.52
    Total L1 Elapsed Cycles          cycle     22473128
    Average L2 Active Cycles         cycle    395527.62
    Total L2 Elapsed Cycles          cycle      9505464
    Average SM Active Cycles         cycle    383424.52
    Total SM Elapsed Cycles          cycle     22473128
    Average SMSP Active Cycles       cycle    381615.91
    Total SMSP Elapsed Cycles        cycle     89892512
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.40
    Elapsed Cycles                cycle      1206943
    Memory Throughput                 %        84.51
    DRAM Throughput                   %        23.17
    Duration                         ms         1.48
    L1/TEX Cache Throughput           %        87.53
    L2 Cache Throughput               %        14.00
    SM Active Cycles              cycle   1157072.76
    Compute (SM) Throughput           %        84.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148160
    Total DRAM Elapsed Cycles        cycle     55618560
    Average L1 Active Cycles         cycle   1157072.76
    Total L1 Elapsed Cycles          cycle     69509094
    Average L2 Active Cycles         cycle   1185016.42
    Total L2 Elapsed Cycles          cycle     29391864
    Average SM Active Cycles         cycle   1157072.76
    Total SM Elapsed Cycles          cycle     69509094
    Average SMSP Active Cycles       cycle   1158509.69
    Total SMSP Elapsed Cycles        cycle    278036376
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       798.95
    Elapsed Cycles                cycle         5542
    Memory Throughput                 %        28.45
    DRAM Throughput                   %        28.45
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.82
    L2 Cache Throughput               %        15.90
    SM Active Cycles              cycle      2850.91
    Compute (SM) Throughput           %        11.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.44
    Achieved Active Warps Per SM           warp        36.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12088
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2850.91
    Total L1 Elapsed Cycles          cycle       292466
    Average L2 Active Cycles         cycle      2453.79
    Total L2 Elapsed Cycles          cycle       133944
    Average SM Active Cycles         cycle      2850.91
    Total SM Elapsed Cycles          cycle       292466
    Average SMSP Active Cycles       cycle      2846.50
    Total SMSP Elapsed Cycles        cycle      1169864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.681%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.06% above the average, while the minimum instance value is 21.90% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       799.42
    Elapsed Cycles                cycle         5884
    Memory Throughput                 %        31.59
    DRAM Throughput                   %        31.59
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.48
    L2 Cache Throughput               %        17.73
    SM Active Cycles              cycle      3232.36
    Compute (SM) Throughput           %        10.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.49
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14069.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3232.36
    Total L1 Elapsed Cycles          cycle       305348
    Average L2 Active Cycles         cycle      2638.04
    Total L2 Elapsed Cycles          cycle       141144
    Average SM Active Cycles         cycle      3232.36
    Total SM Elapsed Cycles          cycle       305348
    Average SMSP Active Cycles       cycle      3223.52
    Total SMSP Elapsed Cycles        cycle      1221392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.004%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.81% above the average, while the minimum instance value is 11.77% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       802.52
    Elapsed Cycles                cycle         5802
    Memory Throughput                 %        31.84
    DRAM Throughput                   %        31.84
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.21
    L2 Cache Throughput               %        17.97
    SM Active Cycles              cycle      3283.43
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.64
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3283.43
    Total L1 Elapsed Cycles          cycle       307706
    Average L2 Active Cycles         cycle      2622.29
    Total L2 Elapsed Cycles          cycle       139080
    Average SM Active Cycles         cycle      3283.43
    Total SM Elapsed Cycles          cycle       307706
    Average SMSP Active Cycles       cycle      3136.35
    Total SMSP Elapsed Cycles        cycle      1230824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.246%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 10.09% above the average, while the minimum instance value is 10.76% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.246%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 10.09% above the average, while the minimum instance value is 10.76% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       799.72
    Elapsed Cycles                cycle         5808
    Memory Throughput                 %        31.80
    DRAM Throughput                   %        31.80
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        16.93
    L2 Cache Throughput               %        17.96
    SM Active Cycles              cycle      3337.57
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.47
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14000
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3337.57
    Total L1 Elapsed Cycles          cycle       306150
    Average L2 Active Cycles         cycle      2615.38
    Total L2 Elapsed Cycles          cycle       139200
    Average SM Active Cycles         cycle      3337.57
    Total SM Elapsed Cycles          cycle       306150
    Average SMSP Active Cycles       cycle      3177.64
    Total SMSP Elapsed Cycles        cycle      1224600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.638%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.37% above the average, while the minimum instance value is 12.42% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.13
    Elapsed Cycles                cycle      1313855
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.33
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292436.03
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203594.67
    Total DRAM Elapsed Cycles        cycle     58589184
    Average L1 Active Cycles         cycle   1292436.03
    Total L1 Elapsed Cycles          cycle     75120216
    Average L2 Active Cycles         cycle   1172256.08
    Total L2 Elapsed Cycles          cycle     31312728
    Average SM Active Cycles         cycle   1292436.03
    Total SM Elapsed Cycles          cycle     75120216
    Average SMSP Active Cycles       cycle   1292380.47
    Total SMSP Elapsed Cycles        cycle    300480864
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.73
    Elapsed Cycles                cycle       384452
    Memory Throughput                 %        87.47
    DRAM Throughput                   %        87.47
    Duration                         us       481.89
    L1/TEX Cache Throughput           %        48.68
    L2 Cache Throughput               %        58.00
    SM Active Cycles              cycle    378577.84
    Compute (SM) Throughput           %        30.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.19
    Achieved Active Warps Per SM           warp        45.69
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2631485.33
    Total DRAM Elapsed Cycles        cycle     18051072
    Average L1 Active Cycles         cycle    378577.84
    Total L1 Elapsed Cycles          cycle     22452996
    Average L2 Active Cycles         cycle    398840.79
    Total L2 Elapsed Cycles          cycle      9539568
    Average SM Active Cycles         cycle    378577.84
    Total SM Elapsed Cycles          cycle     22452996
    Average SMSP Active Cycles       cycle    380814.47
    Total SMSP Elapsed Cycles        cycle     89811984
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.24
    Elapsed Cycles                cycle      1212119
    Memory Throughput                 %        84.81
    DRAM Throughput                   %        23.07
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.43
    L2 Cache Throughput               %        13.94
    SM Active Cycles              cycle   1158514.10
    Compute (SM) Throughput           %        84.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2147352
    Total DRAM Elapsed Cycles        cycle     55843840
    Average L1 Active Cycles         cycle   1158514.10
    Total L1 Elapsed Cycles          cycle     69264056
    Average L2 Active Cycles         cycle   1204433.88
    Total L2 Elapsed Cycles          cycle     29510208
    Average SM Active Cycles         cycle   1158514.10
    Total SM Elapsed Cycles          cycle     69264056
    Average SMSP Active Cycles       cycle   1157046.84
    Total SMSP Elapsed Cycles        cycle    277056224
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.23
    Elapsed Cycles                cycle         5481
    Memory Throughput                 %        28.87
    DRAM Throughput                   %        28.87
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.11
    L2 Cache Throughput               %        15.82
    SM Active Cycles              cycle      2955.84
    Compute (SM) Throughput           %        11.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.78
    Achieved Active Warps Per SM           warp        33.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12266.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2955.84
    Total L1 Elapsed Cycles          cycle       289676
    Average L2 Active Cycles         cycle      2474.79
    Total L2 Elapsed Cycles          cycle       134616
    Average SM Active Cycles         cycle      2955.84
    Total SM Elapsed Cycles          cycle       289676
    Average SMSP Active Cycles       cycle      2953.44
    Total SMSP Elapsed Cycles        cycle      1158704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.587%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.44% above the average, while the minimum instance value is 16.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.587%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.44% above the average, while the minimum instance value is 16.13% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.534%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.81% above the average, while the minimum instance value is 4.72% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       799.47
    Elapsed Cycles                cycle         5859
    Memory Throughput                 %        31.49
    DRAM Throughput                   %        31.49
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.78
    L2 Cache Throughput               %        17.67
    SM Active Cycles              cycle      3177.48
    Compute (SM) Throughput           %        10.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.51
    Achieved Active Warps Per SM           warp        36.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3177.48
    Total L1 Elapsed Cycles          cycle       315190
    Average L2 Active Cycles         cycle      2668.04
    Total L2 Elapsed Cycles          cycle       141432
    Average SM Active Cycles         cycle      3177.48
    Total SM Elapsed Cycles          cycle       315190
    Average SMSP Active Cycles       cycle      3231.08
    Total SMSP Elapsed Cycles        cycle      1260760
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.72
    Elapsed Cycles                cycle         5666
    Memory Throughput                 %        31.47
    DRAM Throughput                   %        31.47
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        18.15
    L2 Cache Throughput               %        17.71
    SM Active Cycles              cycle      3112.12
    Compute (SM) Throughput           %        10.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.42
    Achieved Active Warps Per SM           warp        35.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3112.12
    Total L1 Elapsed Cycles          cycle       305498
    Average L2 Active Cycles         cycle      2673.50
    Total L2 Elapsed Cycles          cycle       141144
    Average SM Active Cycles         cycle      3112.12
    Total SM Elapsed Cycles          cycle       305498
    Average SMSP Active Cycles       cycle      3159.15
    Total SMSP Elapsed Cycles        cycle      1221992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.573%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.96% above the average, while the minimum instance value is 10.51% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       781.66
    Elapsed Cycles                cycle         5685
    Memory Throughput                 %        31.65
    DRAM Throughput                   %        31.65
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.51
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3225.79
    Compute (SM) Throughput           %        10.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.35
    Achieved Active Warps Per SM           warp        35.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14096
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3225.79
    Total L1 Elapsed Cycles          cycle       304568
    Average L2 Active Cycles         cycle      2600.46
    Total L2 Elapsed Cycles          cycle       141528
    Average SM Active Cycles         cycle      3225.79
    Total SM Elapsed Cycles          cycle       304568
    Average SMSP Active Cycles       cycle      3118.05
    Total SMSP Elapsed Cycles        cycle      1218272
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.22
    Elapsed Cycles                cycle      1313276
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        12.33
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.51
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292714.29
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.53
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203712
    Total DRAM Elapsed Cycles        cycle     58567680
    Average L1 Active Cycles         cycle   1292714.29
    Total L1 Elapsed Cycles          cycle     75126922
    Average L2 Active Cycles         cycle   1147737.42
    Total L2 Elapsed Cycles          cycle     31285656
    Average SM Active Cycles         cycle   1292714.29
    Total SM Elapsed Cycles          cycle     75126922
    Average SMSP Active Cycles       cycle   1292264.86
    Total SMSP Elapsed Cycles        cycle    300507688
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.08
    Elapsed Cycles                cycle       401314
    Memory Throughput                 %        87.64
    DRAM Throughput                   %        87.64
    Duration                         us       478.59
    L1/TEX Cache Throughput           %        47.07
    L2 Cache Throughput               %        57.70
    SM Active Cycles              cycle    388889.26
    Compute (SM) Throughput           %        29.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.57
    Achieved Active Warps Per SM           warp        45.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2618858.67
    Total DRAM Elapsed Cycles        cycle     17928192
    Average L1 Active Cycles         cycle    388889.26
    Total L1 Elapsed Cycles          cycle     23220950
    Average L2 Active Cycles         cycle       398530
    Total L2 Elapsed Cycles          cycle      9590328
    Average SM Active Cycles         cycle    388889.26
    Total SM Elapsed Cycles          cycle     23220950
    Average SMSP Active Cycles       cycle    392819.89
    Total SMSP Elapsed Cycles        cycle     92883800
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       821.85
    Elapsed Cycles                cycle      1214556
    Memory Throughput                 %        84.25
    DRAM Throughput                   %        23.58
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.55
    L2 Cache Throughput               %        14.18
    SM Active Cycles              cycle   1156811.57
    Compute (SM) Throughput           %        84.25
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.12
    Achieved Active Warps Per SM           warp        37.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2149085.33
    Total DRAM Elapsed Cycles        cycle     54677504
    Average L1 Active Cycles         cycle   1156811.57
    Total L1 Elapsed Cycles          cycle     69730602
    Average L2 Active Cycles         cycle   1181997.96
    Total L2 Elapsed Cycles          cycle     28975416
    Average SM Active Cycles         cycle   1156811.57
    Total SM Elapsed Cycles          cycle     69730602
    Average SMSP Active Cycles       cycle   1157690.86
    Total SMSP Elapsed Cycles        cycle    278922408
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       802.02
    Elapsed Cycles                cycle         5539
    Memory Throughput                 %        28.79
    DRAM Throughput                   %        28.79
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.30
    L2 Cache Throughput               %        16.03
    SM Active Cycles              cycle      2926.97
    Compute (SM) Throughput           %        10.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.32
    Achieved Active Warps Per SM           warp        34.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12085.33
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2926.97
    Total L1 Elapsed Cycles          cycle       300840
    Average L2 Active Cycles         cycle      2522.58
    Total L2 Elapsed Cycles          cycle       132960
    Average SM Active Cycles         cycle      2926.97
    Total SM Elapsed Cycles          cycle       300840
    Average SMSP Active Cycles       cycle      2875.29
    Total SMSP Elapsed Cycles        cycle      1203360
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.965%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.76% above the average, while the minimum instance value is 25.50% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       837.10
    Elapsed Cycles                cycle         5994
    Memory Throughput                 %        32.25
    DRAM Throughput                   %        32.25
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.52
    L2 Cache Throughput               %        17.54
    SM Active Cycles              cycle      3225.34
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.20
    Achieved Active Warps Per SM           warp        37.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3225.34
    Total L1 Elapsed Cycles          cycle       306458
    Average L2 Active Cycles         cycle      2653.12
    Total L2 Elapsed Cycles          cycle       142440
    Average SM Active Cycles         cycle      3225.34
    Total SM Elapsed Cycles          cycle       306458
    Average SMSP Active Cycles       cycle      3109.56
    Total SMSP Elapsed Cycles        cycle      1225832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.498%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.34% above the average, while the minimum instance value is 13.24% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       778.90
    Elapsed Cycles                cycle         5712
    Memory Throughput                 %        31.14
    DRAM Throughput                   %        31.14
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.74
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3184.97
    Compute (SM) Throughput           %        10.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.23
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3184.97
    Total L1 Elapsed Cycles          cycle       309752
    Average L2 Active Cycles         cycle      2612.12
    Total L2 Elapsed Cycles          cycle       142320
    Average SM Active Cycles         cycle      3184.97
    Total SM Elapsed Cycles          cycle       309752
    Average SMSP Active Cycles       cycle      3103.65
    Total SMSP Elapsed Cycles        cycle      1239008
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       800.58
    Elapsed Cycles                cycle         5839
    Memory Throughput                 %        31.78
    DRAM Throughput                   %        31.78
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.33
    L2 Cache Throughput               %        17.87
    SM Active Cycles              cycle      3260.86
    Compute (SM) Throughput           %        10.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.33
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14101.33
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3260.86
    Total L1 Elapsed Cycles          cycle       315422
    Average L2 Active Cycles         cycle      2657.38
    Total L2 Elapsed Cycles          cycle       139896
    Average SM Active Cycles         cycle      3260.86
    Total SM Elapsed Cycles          cycle       315422
    Average SMSP Active Cycles       cycle      3192.71
    Total SMSP Elapsed Cycles        cycle      1261688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.512%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.19% above the average, while the minimum instance value is 9.84% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.512%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.19% above the average, while the minimum instance value is 9.84% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.51
    Elapsed Cycles                cycle      1313371
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        12.36
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292474.43
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1206584
    Total DRAM Elapsed Cycles        cycle     58563584
    Average L1 Active Cycles         cycle   1292474.43
    Total L1 Elapsed Cycles          cycle     75124558
    Average L2 Active Cycles         cycle   1147410.42
    Total L2 Elapsed Cycles          cycle     31309368
    Average SM Active Cycles         cycle   1292474.43
    Total SM Elapsed Cycles          cycle     75124558
    Average SMSP Active Cycles       cycle   1292560.65
    Total SMSP Elapsed Cycles        cycle    300498232
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       819.24
    Elapsed Cycles                cycle       398996
    Memory Throughput                 %        87.58
    DRAM Throughput                   %        87.58
    Duration                         us       480.99
    L1/TEX Cache Throughput           %        47.75
    L2 Cache Throughput               %        57.87
    SM Active Cycles              cycle    391574.07
    Compute (SM) Throughput           %        30.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.68
    Achieved Active Warps Per SM           warp        45.45
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2629789.33
    Total DRAM Elapsed Cycles        cycle     18017280
    Average L1 Active Cycles         cycle    391574.07
    Total L1 Elapsed Cycles          cycle     22883584
    Average L2 Active Cycles         cycle    396488.42
    Total L2 Elapsed Cycles          cycle      9552552
    Average SM Active Cycles         cycle    391574.07
    Total SM Elapsed Cycles          cycle     22883584
    Average SMSP Active Cycles       cycle    388732.15
    Total SMSP Elapsed Cycles        cycle     91534336
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.26
    Elapsed Cycles                cycle      1211012
    Memory Throughput                 %        84.19
    DRAM Throughput                   %        23.77
    Duration                         ms         1.45
    L1/TEX Cache Throughput           %        87.53
    L2 Cache Throughput               %        14.22
    SM Active Cycles              cycle   1157198.41
    Compute (SM) Throughput           %        84.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.09
    Achieved Active Warps Per SM           warp        37.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148170.67
    Total DRAM Elapsed Cycles        cycle     54223872
    Average L1 Active Cycles         cycle   1157198.41
    Total L1 Elapsed Cycles          cycle     69777248
    Average L2 Active Cycles         cycle   1181136.21
    Total L2 Elapsed Cycles          cycle     28884384
    Average SM Active Cycles         cycle   1157198.41
    Total SM Elapsed Cycles          cycle     69777248
    Average SMSP Active Cycles       cycle   1158715.52
    Total SMSP Elapsed Cycles        cycle    279108992
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       800.93
    Elapsed Cycles                cycle         5557
    Memory Throughput                 %        29.22
    DRAM Throughput                   %        29.22
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        18.79
    L2 Cache Throughput               %        15.97
    SM Active Cycles              cycle      3007.02
    Compute (SM) Throughput           %        11.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.47
    Achieved Active Warps Per SM           warp        34.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12266.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      3007.02
    Total L1 Elapsed Cycles          cycle       294348
    Average L2 Active Cycles         cycle      2427.58
    Total L2 Elapsed Cycles          cycle       133488
    Average SM Active Cycles         cycle      3007.02
    Total SM Elapsed Cycles          cycle       294348
    Average SMSP Active Cycles       cycle      2835.74
    Total SMSP Elapsed Cycles        cycle      1177392
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.479%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.80% above the average, while the minimum instance value is 22.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.153%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.10% above the average, while the minimum instance value is 4.51% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.68
    Elapsed Cycles                cycle         5817
    Memory Throughput                 %        30.75
    DRAM Throughput                   %        30.75
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        18.11
    L2 Cache Throughput               %        17.25
    SM Active Cycles              cycle      3119.93
    Compute (SM) Throughput           %        10.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.16
    Achieved Active Warps Per SM           warp        37.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14013.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3119.93
    Total L1 Elapsed Cycles          cycle       312512
    Average L2 Active Cycles         cycle      2567.04
    Total L2 Elapsed Cycles          cycle       144816
    Average SM Active Cycles         cycle      3119.93
    Total SM Elapsed Cycles          cycle       312512
    Average SMSP Active Cycles       cycle      3062.65
    Total SMSP Elapsed Cycles        cycle      1250048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.107%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.99% above the average, while the minimum instance value is 9.26% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.44
    Elapsed Cycles                cycle         5788
    Memory Throughput                 %        30.80
    DRAM Throughput                   %        30.80
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.87
    L2 Cache Throughput               %        17.37
    SM Active Cycles              cycle      3161.17
    Compute (SM) Throughput           %        10.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.05
    Achieved Active Warps Per SM           warp        36.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3161.17
    Total L1 Elapsed Cycles          cycle       311322
    Average L2 Active Cycles         cycle      2663.29
    Total L2 Elapsed Cycles          cycle       144096
    Average SM Active Cycles         cycle      3161.17
    Total SM Elapsed Cycles          cycle       311322
    Average SMSP Active Cycles       cycle      3175.30
    Total SMSP Elapsed Cycles        cycle      1245288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.288%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.98% above the average, while the minimum instance value is 10.51% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.133%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.68% above the average, while the minimum instance value is 12.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.288%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.98% above the average, while the minimum instance value is 10.51% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.66
    Elapsed Cycles                cycle         5798
    Memory Throughput                 %        31.13
    DRAM Throughput                   %        31.13
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.69
    L2 Cache Throughput               %        17.57
    SM Active Cycles              cycle      3194.50
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.84
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3194.50
    Total L1 Elapsed Cycles          cycle       307814
    Average L2 Active Cycles         cycle      2675.42
    Total L2 Elapsed Cycles          cycle       142296
    Average SM Active Cycles         cycle      3194.50
    Total SM Elapsed Cycles          cycle       307814
    Average SMSP Active Cycles       cycle      3169.94
    Total SMSP Elapsed Cycles        cycle      1231256
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.07
    Elapsed Cycles                cycle      1312440
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.54
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292295.36
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.59
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204018.67
    Total DRAM Elapsed Cycles        cycle     60478464
    Average L1 Active Cycles         cycle   1292295.36
    Total L1 Elapsed Cycles          cycle     75134874
    Average L2 Active Cycles         cycle   1165895.33
    Total L2 Elapsed Cycles          cycle     31958664
    Average SM Active Cycles         cycle   1292295.36
    Total SM Elapsed Cycles          cycle     75134874
    Average SMSP Active Cycles       cycle   1292245.07
    Total SMSP Elapsed Cycles        cycle    300539496
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.99
    Elapsed Cycles                cycle       396031
    Memory Throughput                 %        86.92
    DRAM Throughput                   %        86.92
    Duration                         us       487.94
    L1/TEX Cache Throughput           %        47.72
    L2 Cache Throughput               %        57.28
    SM Active Cycles              cycle    387958.33
    Compute (SM) Throughput           %        29.98
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.03
    Achieved Active Warps Per SM           warp        45.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2647629.33
    Total DRAM Elapsed Cycles        cycle     18276352
    Average L1 Active Cycles         cycle    387958.33
    Total L1 Elapsed Cycles          cycle     22899832
    Average L2 Active Cycles         cycle    395502.33
    Total L2 Elapsed Cycles          cycle      9659472
    Average SM Active Cycles         cycle    387958.33
    Total SM Elapsed Cycles          cycle     22899832
    Average SMSP Active Cycles       cycle    388289.55
    Total SMSP Elapsed Cycles        cycle     91599328
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.41
    Elapsed Cycles                cycle      1213706
    Memory Throughput                 %        84.25
    DRAM Throughput                   %        23.71
    Duration                         ms         1.45
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        14.20
    SM Active Cycles              cycle   1158206.31
    Compute (SM) Throughput           %        84.25
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148330.67
    Total DRAM Elapsed Cycles        cycle     54374400
    Average L1 Active Cycles         cycle   1158206.31
    Total L1 Elapsed Cycles          cycle     69723300
    Average L2 Active Cycles         cycle   1184506.54
    Total L2 Elapsed Cycles          cycle     28928712
    Average SM Active Cycles         cycle   1158206.31
    Total SM Elapsed Cycles          cycle     69723300
    Average SMSP Active Cycles       cycle   1158120.29
    Total SMSP Elapsed Cycles        cycle    278893200
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       779.09
    Elapsed Cycles                cycle         5493
    Memory Throughput                 %        27.87
    DRAM Throughput                   %        27.87
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        19.81
    L2 Cache Throughput               %        15.57
    SM Active Cycles              cycle      2852.14
    Compute (SM) Throughput           %        11.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.20
    Achieved Active Warps Per SM           warp        36.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12082.67
    Total DRAM Elapsed Cycles        cycle       260096
    Average L1 Active Cycles         cycle      2852.14
    Total L1 Elapsed Cycles          cycle       294472
    Average L2 Active Cycles         cycle      2487.54
    Total L2 Elapsed Cycles          cycle       136848
    Average SM Active Cycles         cycle      2852.14
    Total SM Elapsed Cycles          cycle       294472
    Average SMSP Active Cycles       cycle      2824.01
    Total SMSP Elapsed Cycles        cycle      1177888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.085%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.05% above the average, while the minimum instance value is 15.61% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.085%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.05% above the average, while the minimum instance value is 15.61% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.25
    Elapsed Cycles                cycle         5612
    Memory Throughput                 %        31.93
    DRAM Throughput                   %        31.93
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.46
    L2 Cache Throughput               %        17.89
    SM Active Cycles              cycle      3236.64
    Compute (SM) Throughput           %        10.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.93
    Achieved Active Warps Per SM           warp        34.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3236.64
    Total L1 Elapsed Cycles          cycle       303020
    Average L2 Active Cycles         cycle      2648.88
    Total L2 Elapsed Cycles          cycle       139800
    Average SM Active Cycles         cycle      3236.64
    Total SM Elapsed Cycles          cycle       303020
    Average SMSP Active Cycles       cycle      3166.44
    Total SMSP Elapsed Cycles        cycle      1212080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.319%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.43% above the average, while the minimum instance value is 15.68% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       788.46
    Elapsed Cycles                cycle         5781
    Memory Throughput                 %        31.14
    DRAM Throughput                   %        31.14
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.24
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3276.57
    Compute (SM) Throughput           %        10.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.87
    Achieved Active Warps Per SM           warp        34.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3276.57
    Total L1 Elapsed Cycles          cycle       310558
    Average L2 Active Cycles         cycle      2584.46
    Total L2 Elapsed Cycles          cycle       141792
    Average SM Active Cycles         cycle      3276.57
    Total SM Elapsed Cycles          cycle       310558
    Average SMSP Active Cycles       cycle      3178.98
    Total SMSP Elapsed Cycles        cycle      1242232
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.124%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.37% above the average, while the minimum instance value is 11.77% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.089%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.94% above the average, while the minimum instance value is 18.18% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.124%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.37% above the average, while the minimum instance value is 11.77% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       789.03
    Elapsed Cycles                cycle         5744
    Memory Throughput                 %        31.56
    DRAM Throughput                   %        31.56
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.69
    L2 Cache Throughput               %        17.82
    SM Active Cycles              cycle      3193.34
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.53
    Achieved Active Warps Per SM           warp        35.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3193.34
    Total L1 Elapsed Cycles          cycle       309942
    Average L2 Active Cycles         cycle      2564.25
    Total L2 Elapsed Cycles          cycle       140376
    Average SM Active Cycles         cycle      3193.34
    Total SM Elapsed Cycles          cycle       309942
    Average SMSP Active Cycles       cycle      3044.14
    Total SMSP Elapsed Cycles        cycle      1239768
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.11
    Elapsed Cycles                cycle      1312204
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292447.07
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203218.67
    Total DRAM Elapsed Cycles        cycle     60469248
    Average L1 Active Cycles         cycle   1292447.07
    Total L1 Elapsed Cycles          cycle     75131796
    Average L2 Active Cycles         cycle      1165559
    Total L2 Elapsed Cycles          cycle     31955208
    Average SM Active Cycles         cycle   1292447.07
    Total SM Elapsed Cycles          cycle     75131796
    Average SMSP Active Cycles       cycle   1292439.09
    Total SMSP Elapsed Cycles        cycle    300527184
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.02
    Elapsed Cycles                cycle       393048
    Memory Throughput                 %        87.22
    DRAM Throughput                   %        87.22
    Duration                         us       484.29
    L1/TEX Cache Throughput           %        48.87
    L2 Cache Throughput               %        57.75
    SM Active Cycles              cycle    384602.22
    Compute (SM) Throughput           %        30.68
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.79
    Achieved Active Warps Per SM           warp        45.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2636896
    Total DRAM Elapsed Cycles        cycle     18140160
    Average L1 Active Cycles         cycle    384602.22
    Total L1 Elapsed Cycles          cycle     22377942
    Average L2 Active Cycles         cycle    399638.79
    Total L2 Elapsed Cycles          cycle      9587736
    Average SM Active Cycles         cycle    384602.22
    Total SM Elapsed Cycles          cycle     22377942
    Average SMSP Active Cycles       cycle    385198.47
    Total SMSP Elapsed Cycles        cycle     89511768
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.31
    Elapsed Cycles                cycle      1210719
    Memory Throughput                 %        84.19
    DRAM Throughput                   %        23.22
    Duration                         ms         1.48
    L1/TEX Cache Throughput           %        87.50
    L2 Cache Throughput               %        13.97
    SM Active Cycles              cycle   1157499.28
    Compute (SM) Throughput           %        84.19
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2150688
    Total DRAM Elapsed Cycles        cycle     55578624
    Average L1 Active Cycles         cycle   1157499.28
    Total L1 Elapsed Cycles          cycle     69775686
    Average L2 Active Cycles         cycle   1186308.33
    Total L2 Elapsed Cycles          cycle     29428056
    Average SM Active Cycles         cycle   1157499.28
    Total SM Elapsed Cycles          cycle     69775686
    Average SMSP Active Cycles       cycle   1158604.76
    Total SMSP Elapsed Cycles        cycle    279102744
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       797.17
    Elapsed Cycles                cycle         5481
    Memory Throughput                 %        29.60
    DRAM Throughput                   %        29.60
    Duration                         us         6.78
    L1/TEX Cache Throughput           %        19.16
    L2 Cache Throughput               %        16.21
    SM Active Cycles              cycle      2948.22
    Compute (SM) Throughput           %        11.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.35
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12274.67
    Total DRAM Elapsed Cycles        cycle       248832
    Average L1 Active Cycles         cycle      2948.22
    Total L1 Elapsed Cycles          cycle       297894
    Average L2 Active Cycles         cycle      2428.50
    Total L2 Elapsed Cycles          cycle       131736
    Average SM Active Cycles         cycle      2948.22
    Total SM Elapsed Cycles          cycle       297894
    Average SMSP Active Cycles       cycle      2858.38
    Total SMSP Elapsed Cycles        cycle      1191576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.121%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.92% above the average, while the minimum instance value is 18.66% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.518%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.92% above the average, while the minimum instance value is 24.82% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.121%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.92% above the average, while the minimum instance value is 18.66% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.53%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.76% above the average, while the minimum instance value is 4.92% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.87
    Elapsed Cycles                cycle         5835
    Memory Throughput                 %        30.67
    DRAM Throughput                   %        30.67
    Duration                         us         7.46
    L1/TEX Cache Throughput           %        18.03
    L2 Cache Throughput               %        17.21
    SM Active Cycles              cycle      3134.12
    Compute (SM) Throughput           %        10.59
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.83
    Achieved Active Warps Per SM           warp        36.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14080
    Total DRAM Elapsed Cycles        cycle       275456
    Average L1 Active Cycles         cycle      3134.12
    Total L1 Elapsed Cycles          cycle       309336
    Average L2 Active Cycles         cycle      2653.12
    Total L2 Elapsed Cycles          cycle       145296
    Average SM Active Cycles         cycle      3134.12
    Total SM Elapsed Cycles          cycle       309336
    Average SMSP Active Cycles       cycle      3139.11
    Total SMSP Elapsed Cycles        cycle      1237344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.191%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.22% above the average, while the minimum instance value is 15.01% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.51
    Elapsed Cycles                cycle         5716
    Memory Throughput                 %        31.33
    DRAM Throughput                   %        31.33
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.63
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3205.33
    Compute (SM) Throughput           %        10.41
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.75
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14064
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3205.33
    Total L1 Elapsed Cycles          cycle       314734
    Average L2 Active Cycles         cycle      2587.79
    Total L2 Elapsed Cycles          cycle       142344
    Average SM Active Cycles         cycle      3205.33
    Total SM Elapsed Cycles          cycle       314734
    Average SMSP Active Cycles       cycle      3185.69
    Total SMSP Elapsed Cycles        cycle      1258936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.123%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 10.37% above the average, while the minimum instance value is 8.96% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.384%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.58% above the average, while the minimum instance value is 17.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.123%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 10.37% above the average, while the minimum instance value is 8.96% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       808.15
    Elapsed Cycles                cycle         5868
    Memory Throughput                 %        31.83
    DRAM Throughput                   %        31.83
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.76
    L2 Cache Throughput               %        17.84
    SM Active Cycles              cycle      3181.53
    Compute (SM) Throughput           %        10.56
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.61
    Achieved Active Warps Per SM           warp        36.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3181.53
    Total L1 Elapsed Cycles          cycle       310180
    Average L2 Active Cycles         cycle      2621.71
    Total L2 Elapsed Cycles          cycle       140088
    Average SM Active Cycles         cycle      3181.53
    Total SM Elapsed Cycles          cycle       310180
    Average SMSP Active Cycles       cycle      3241.19
    Total SMSP Elapsed Cycles        cycle      1240720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.274%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.86% above the average, while the minimum instance value is 9.95% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.086%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.04% above the average, while the minimum instance value is 10.19% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.274%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.86% above the average, while the minimum instance value is 9.95% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.33
    Elapsed Cycles                cycle      1298132
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.83
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.46
    SM Active Cycles              cycle   1292494.29
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203965.33
    Total DRAM Elapsed Cycles        cycle     61039616
    Average L1 Active Cycles         cycle   1292494.29
    Total L1 Elapsed Cycles          cycle     75141084
    Average L2 Active Cycles         cycle      1173484
    Total L2 Elapsed Cycles          cycle     32255856
    Average SM Active Cycles         cycle   1292494.29
    Total SM Elapsed Cycles          cycle     75141084
    Average SMSP Active Cycles       cycle   1292508.40
    Total SMSP Elapsed Cycles        cycle    300564336
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.76
    Elapsed Cycles                cycle       385847
    Memory Throughput                 %        87.11
    DRAM Throughput                   %        87.11
    Duration                         us       483.55
    L1/TEX Cache Throughput           %        48.56
    L2 Cache Throughput               %        57.76
    SM Active Cycles              cycle    380085.98
    Compute (SM) Throughput           %        30.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.29
    Achieved Active Warps Per SM           warp        45.74
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2629584
    Total DRAM Elapsed Cycles        cycle     18112512
    Average L1 Active Cycles         cycle    380085.98
    Total L1 Elapsed Cycles          cycle     22518796
    Average L2 Active Cycles         cycle    396340.83
    Total L2 Elapsed Cycles          cycle      9573048
    Average SM Active Cycles         cycle    380085.98
    Total SM Elapsed Cycles          cycle     22518796
    Average SMSP Active Cycles       cycle    383010.37
    Total SMSP Elapsed Cycles        cycle     90075184
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.54
    Elapsed Cycles                cycle      1215959
    Memory Throughput                 %        84.69
    DRAM Throughput                   %        23.02
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.51
    L2 Cache Throughput               %        13.90
    SM Active Cycles              cycle   1157399.12
    Compute (SM) Throughput           %        84.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.04
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148098.67
    Total DRAM Elapsed Cycles        cycle     55994368
    Average L1 Active Cycles         cycle   1157399.12
    Total L1 Elapsed Cycles          cycle     69365484
    Average L2 Active Cycles         cycle   1204088.92
    Total L2 Elapsed Cycles          cycle     29590248
    Average SM Active Cycles         cycle   1157399.12
    Total SM Elapsed Cycles          cycle     69365484
    Average SMSP Active Cycles       cycle   1157258.07
    Total SMSP Elapsed Cycles        cycle    277461936
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.67
    Elapsed Cycles                cycle         5556
    Memory Throughput                 %        28.24
    DRAM Throughput                   %        28.24
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        19.14
    L2 Cache Throughput               %        15.71
    SM Active Cycles              cycle      2952.45
    Compute (SM) Throughput           %        11.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.78
    Achieved Active Warps Per SM           warp        33.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12146.67
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      2952.45
    Total L1 Elapsed Cycles          cycle       290380
    Average L2 Active Cycles         cycle      2402.71
    Total L2 Elapsed Cycles          cycle       135744
    Average SM Active Cycles         cycle      2952.45
    Total SM Elapsed Cycles          cycle       290380
    Average SMSP Active Cycles       cycle      2801.20
    Total SMSP Elapsed Cycles        cycle      1161520
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.081%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.08% above the average, while the minimum instance value is 22.35% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.15
    Elapsed Cycles                cycle         5619
    Memory Throughput                 %        31.81
    DRAM Throughput                   %        31.81
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.94
    L2 Cache Throughput               %        18.05
    SM Active Cycles              cycle      3149.16
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.74
    Achieved Active Warps Per SM           warp        35.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14005.33
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3149.16
    Total L1 Elapsed Cycles          cycle       302330
    Average L2 Active Cycles         cycle      2627.46
    Total L2 Elapsed Cycles          cycle       138456
    Average SM Active Cycles         cycle      3149.16
    Total SM Elapsed Cycles          cycle       302330
    Average SMSP Active Cycles       cycle      3113.83
    Total SMSP Elapsed Cycles        cycle      1209320
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.459%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.14% above the average, while the minimum instance value is 12.65% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       784.27
    Elapsed Cycles                cycle         5778
    Memory Throughput                 %        31.53
    DRAM Throughput                   %        31.53
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.53
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3222.53
    Compute (SM) Throughput           %        10.34
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.51
    Achieved Active Warps Per SM           warp        35.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3222.53
    Total L1 Elapsed Cycles          cycle       316962
    Average L2 Active Cycles         cycle      2593.38
    Total L2 Elapsed Cycles          cycle       141552
    Average SM Active Cycles         cycle      3222.53
    Total SM Elapsed Cycles          cycle       316962
    Average SMSP Active Cycles       cycle      3074.66
    Total SMSP Elapsed Cycles        cycle      1267848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.106%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.66% above the average, while the minimum instance value is 10.10% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.103%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 14.40% above the average, while the minimum instance value is 12.97% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.106%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.66% above the average, while the minimum instance value is 10.10% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.97
    Elapsed Cycles                cycle         5706
    Memory Throughput                 %        31.64
    DRAM Throughput                   %        31.64
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.25
    L2 Cache Throughput               %        17.81
    SM Active Cycles              cycle      3274.86
    Compute (SM) Throughput           %        10.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.53
    Achieved Active Warps Per SM           warp        34.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3274.86
    Total L1 Elapsed Cycles          cycle       301018
    Average L2 Active Cycles         cycle      2620.62
    Total L2 Elapsed Cycles          cycle       140352
    Average SM Active Cycles         cycle      3274.86
    Total SM Elapsed Cycles          cycle       301018
    Average SMSP Active Cycles       cycle      3191.75
    Total SMSP Elapsed Cycles        cycle      1204072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.656%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.20% above the average, while the minimum instance value is 13.43% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.20
    Elapsed Cycles                cycle      1312296
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.60
    SM Active Cycles              cycle   1292519.69
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203048
    Total DRAM Elapsed Cycles        cycle     60469248
    Average L1 Active Cycles         cycle   1292519.69
    Total L1 Elapsed Cycles          cycle     75131502
    Average L2 Active Cycles         cycle   1164528.29
    Total L2 Elapsed Cycles          cycle     31955112
    Average SM Active Cycles         cycle   1292519.69
    Total SM Elapsed Cycles          cycle     75131502
    Average SMSP Active Cycles       cycle   1292385.71
    Total SMSP Elapsed Cycles        cycle    300526008
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.79
    Elapsed Cycles                cycle       392085
    Memory Throughput                 %        87.48
    DRAM Throughput                   %        87.48
    Duration                         us       483.23
    L1/TEX Cache Throughput           %        48.62
    L2 Cache Throughput               %        57.85
    SM Active Cycles              cycle    380753.78
    Compute (SM) Throughput           %        30.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.80
    Achieved Active Warps Per SM           warp        45.98
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2639253.33
    Total DRAM Elapsed Cycles        cycle     18102272
    Average L1 Active Cycles         cycle    380753.78
    Total L1 Elapsed Cycles          cycle     22477240
    Average L2 Active Cycles         cycle       397912
    Total L2 Elapsed Cycles          cycle      9566880
    Average SM Active Cycles         cycle    380753.78
    Total SM Elapsed Cycles          cycle     22477240
    Average SMSP Active Cycles       cycle    383043.56
    Total SMSP Elapsed Cycles        cycle     89908960
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.44
    Elapsed Cycles                cycle      1213139
    Memory Throughput                 %        84.06
    DRAM Throughput                   %        23.09
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.44
    L2 Cache Throughput               %        13.93
    SM Active Cycles              cycle   1158319.74
    Compute (SM) Throughput           %        84.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.98
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2150149.33
    Total DRAM Elapsed Cycles        cycle     55864320
    Average L1 Active Cycles         cycle   1158319.74
    Total L1 Elapsed Cycles          cycle     69886962
    Average L2 Active Cycles         cycle   1184866.92
    Total L2 Elapsed Cycles          cycle     29520888
    Average SM Active Cycles         cycle   1158319.74
    Total SM Elapsed Cycles          cycle     69886962
    Average SMSP Active Cycles       cycle   1157987.45
    Total SMSP Elapsed Cycles        cycle    279547848
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       796.61
    Elapsed Cycles                cycle         5473
    Memory Throughput                 %        29.59
    DRAM Throughput                   %        29.59
    Duration                         us         6.78
    L1/TEX Cache Throughput           %        18.99
    L2 Cache Throughput               %        16.23
    SM Active Cycles              cycle      2974.97
    Compute (SM) Throughput           %        11.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.88
    Achieved Active Warps Per SM           warp        34.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12320
    Total DRAM Elapsed Cycles        cycle       249856
    Average L1 Active Cycles         cycle      2974.97
    Total L1 Elapsed Cycles          cycle       291322
    Average L2 Active Cycles         cycle      2525.62
    Total L2 Elapsed Cycles          cycle       131592
    Average SM Active Cycles         cycle      2974.97
    Total SM Elapsed Cycles          cycle       291322
    Average SMSP Active Cycles       cycle      2939.67
    Total SMSP Elapsed Cycles        cycle      1165288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.167%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.39% above the average, while the minimum instance value is 4.46% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       780.21
    Elapsed Cycles                cycle         5573
    Memory Throughput                 %        32.21
    DRAM Throughput                   %        32.21
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        18.00
    SM Active Cycles              cycle      3221.17
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.96
    Achieved Active Warps Per SM           warp        34.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14074.67
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3221.17
    Total L1 Elapsed Cycles          cycle       302178
    Average L2 Active Cycles         cycle      2612.42
    Total L2 Elapsed Cycles          cycle       138816
    Average SM Active Cycles         cycle      3221.17
    Total SM Elapsed Cycles          cycle       302178
    Average SMSP Active Cycles       cycle      3107.33
    Total SMSP Elapsed Cycles        cycle      1208712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.469%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.17% above the average, while the minimum instance value is 10.66% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       783.78
    Elapsed Cycles                cycle         5824
    Memory Throughput                 %        30.50
    DRAM Throughput                   %        30.50
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        17.68
    L2 Cache Throughput               %        17.25
    SM Active Cycles              cycle      3195.17
    Compute (SM) Throughput           %        10.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.80
    Achieved Active Warps Per SM           warp        34.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       276480
    Average L1 Active Cycles         cycle      3195.17
    Total L1 Elapsed Cycles          cycle       319028
    Average L2 Active Cycles         cycle      2626.96
    Total L2 Elapsed Cycles          cycle       145032
    Average SM Active Cycles         cycle      3195.17
    Total SM Elapsed Cycles          cycle       319028
    Average SMSP Active Cycles       cycle      3127.17
    Total SMSP Elapsed Cycles        cycle      1276112
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.27
    Elapsed Cycles                cycle         5813
    Memory Throughput                 %        31.26
    DRAM Throughput                   %        31.26
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.99
    L2 Cache Throughput               %        17.53
    SM Active Cycles              cycle      3141.07
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.92
    Achieved Active Warps Per SM           warp        36.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14085.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3141.07
    Total L1 Elapsed Cycles          cycle       309878
    Average L2 Active Cycles         cycle      2625.54
    Total L2 Elapsed Cycles          cycle       142632
    Average SM Active Cycles         cycle      3141.07
    Total SM Elapsed Cycles          cycle       309878
    Average SMSP Active Cycles       cycle      3130.43
    Total SMSP Elapsed Cycles        cycle      1239512
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.627%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.60% above the average, while the minimum instance value is 9.53% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.53
    Elapsed Cycles                cycle      1313937
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        12.30
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292532.17
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203530.67
    Total DRAM Elapsed Cycles        cycle     58707968
    Average L1 Active Cycles         cycle   1292532.17
    Total L1 Elapsed Cycles          cycle     75135262
    Average L2 Active Cycles         cycle      1145853
    Total L2 Elapsed Cycles          cycle     31308528
    Average SM Active Cycles         cycle   1292532.17
    Total SM Elapsed Cycles          cycle     75135262
    Average SMSP Active Cycles       cycle   1292343.03
    Total SMSP Elapsed Cycles        cycle    300541048
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       822.40
    Elapsed Cycles                cycle       400778
    Memory Throughput                 %        87.53
    DRAM Throughput                   %        87.53
    Duration                         us       481.15
    L1/TEX Cache Throughput           %        48.59
    L2 Cache Throughput               %        57.66
    SM Active Cycles              cycle    389137.34
    Compute (SM) Throughput           %        30.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.42
    Achieved Active Warps Per SM           warp        45.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2629242.67
    Total DRAM Elapsed Cycles        cycle     18023424
    Average L1 Active Cycles         cycle    389137.34
    Total L1 Elapsed Cycles          cycle     22505460
    Average L2 Active Cycles         cycle    397302.38
    Total L2 Elapsed Cycles          cycle      9590016
    Average SM Active Cycles         cycle    389137.34
    Total SM Elapsed Cycles          cycle     22505460
    Average SMSP Active Cycles       cycle    390191.23
    Total SMSP Elapsed Cycles        cycle     90021840
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.57
    Elapsed Cycles                cycle      1209516
    Memory Throughput                 %        84.39
    DRAM Throughput                   %        23.14
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.47
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1157896.91
    Compute (SM) Throughput           %        84.39
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148090.67
    Total DRAM Elapsed Cycles        cycle     55689216
    Average L1 Active Cycles         cycle   1157896.91
    Total L1 Elapsed Cycles          cycle     69607008
    Average L2 Active Cycles         cycle   1205022.38
    Total L2 Elapsed Cycles          cycle     29429280
    Average SM Active Cycles         cycle   1157896.91
    Total SM Elapsed Cycles          cycle     69607008
    Average SMSP Active Cycles       cycle   1158354.64
    Total SMSP Elapsed Cycles        cycle    278428032
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       787.53
    Elapsed Cycles                cycle         5578
    Memory Throughput                 %        28.14
    DRAM Throughput                   %        28.14
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        19.63
    L2 Cache Throughput               %        15.65
    SM Active Cycles              cycle      2878.67
    Compute (SM) Throughput           %        11.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.47
    Achieved Active Warps Per SM           warp        36.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12101.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      2878.67
    Total L1 Elapsed Cycles          cycle       291154
    Average L2 Active Cycles         cycle      2414.54
    Total L2 Elapsed Cycles          cycle       136464
    Average SM Active Cycles         cycle      2878.67
    Total SM Elapsed Cycles          cycle       291154
    Average SMSP Active Cycles       cycle      2782.26
    Total SMSP Elapsed Cycles        cycle      1164616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.146%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.28% above the average, while the minimum instance value is 23.34% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.72
    Elapsed Cycles                cycle         5702
    Memory Throughput                 %        32.03
    DRAM Throughput                   %        32.03
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        17.85
    SM Active Cycles              cycle      3220.64
    Compute (SM) Throughput           %        10.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14101.33
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3220.64
    Total L1 Elapsed Cycles          cycle       299902
    Average L2 Active Cycles         cycle      2570.17
    Total L2 Elapsed Cycles          cycle       139992
    Average SM Active Cycles         cycle      3220.64
    Total SM Elapsed Cycles          cycle       299902
    Average SMSP Active Cycles       cycle      3045.91
    Total SMSP Elapsed Cycles        cycle      1199608
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.32%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.73% above the average, while the minimum instance value is 10.37% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.51
    Elapsed Cycles                cycle         5804
    Memory Throughput                 %        31.13
    DRAM Throughput                   %        31.13
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.62
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3206.14
    Compute (SM) Throughput           %        10.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.20
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14024
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3206.14
    Total L1 Elapsed Cycles          cycle       308720
    Average L2 Active Cycles         cycle      2575.12
    Total L2 Elapsed Cycles          cycle       142512
    Average SM Active Cycles         cycle      3206.14
    Total SM Elapsed Cycles          cycle       308720
    Average SMSP Active Cycles       cycle      3043.53
    Total SMSP Elapsed Cycles        cycle      1234880
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.707%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.98% above the average, while the minimum instance value is 12.01% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.71
    Elapsed Cycles                cycle         5690
    Memory Throughput                 %        31.87
    DRAM Throughput                   %        31.87
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.58
    L2 Cache Throughput               %        17.94
    SM Active Cycles              cycle      3214.09
    Compute (SM) Throughput           %        10.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.04
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3214.09
    Total L1 Elapsed Cycles          cycle       305078
    Average L2 Active Cycles         cycle      2672.58
    Total L2 Elapsed Cycles          cycle       139416
    Average SM Active Cycles         cycle      3214.09
    Total SM Elapsed Cycles          cycle       305078
    Average SMSP Active Cycles       cycle      3198.22
    Total SMSP Elapsed Cycles        cycle      1220312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.626%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.21% above the average, while the minimum instance value is 10.95% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.87%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.65% above the average, while the minimum instance value is 12.08% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.626%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.21% above the average, while the minimum instance value is 10.95% below   
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.40
    Elapsed Cycles                cycle      1312620
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.60
    SM Active Cycles              cycle   1292449.91
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1205173.33
    Total DRAM Elapsed Cycles        cycle     60451840
    Average L1 Active Cycles         cycle   1292449.91
    Total L1 Elapsed Cycles          cycle     75120230
    Average L2 Active Cycles         cycle   1165001.29
    Total L2 Elapsed Cycles          cycle     31945848
    Average SM Active Cycles         cycle   1292449.91
    Total SM Elapsed Cycles          cycle     75120230
    Average SMSP Active Cycles       cycle   1292479.19
    Total SMSP Elapsed Cycles        cycle    300480920
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.90
    Elapsed Cycles                cycle       390331
    Memory Throughput                 %        87.46
    DRAM Throughput                   %        87.46
    Duration                         us       481.02
    L1/TEX Cache Throughput           %        48.43
    L2 Cache Throughput               %        58.19
    SM Active Cycles              cycle    383421.45
    Compute (SM) Throughput           %        30.41
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.35
    Achieved Active Warps Per SM           warp        45.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2626216
    Total DRAM Elapsed Cycles        cycle     18017280
    Average L1 Active Cycles         cycle    383421.45
    Total L1 Elapsed Cycles          cycle     22575746
    Average L2 Active Cycles         cycle    398021.96
    Total L2 Elapsed Cycles          cycle      9522840
    Average SM Active Cycles         cycle    383421.45
    Total SM Elapsed Cycles          cycle     22575746
    Average SMSP Active Cycles       cycle    384726.73
    Total SMSP Elapsed Cycles        cycle     90302984
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.61
    Elapsed Cycles                cycle      1213985
    Memory Throughput                 %        84.72
    DRAM Throughput                   %        23.06
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158019.16
    Compute (SM) Throughput           %        84.72
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148514.67
    Total DRAM Elapsed Cycles        cycle     55901184
    Average L1 Active Cycles         cycle   1158019.16
    Total L1 Elapsed Cycles          cycle     69336982
    Average L2 Active Cycles         cycle   1202504.54
    Total L2 Elapsed Cycles          cycle     29541600
    Average SM Active Cycles         cycle   1158019.16
    Total SM Elapsed Cycles          cycle     69336982
    Average SMSP Active Cycles       cycle   1156335.97
    Total SMSP Elapsed Cycles        cycle    277347928
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.16
    Elapsed Cycles                cycle         5478
    Memory Throughput                 %        28.91
    DRAM Throughput                   %        28.91
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.28
    L2 Cache Throughput               %        15.89
    SM Active Cycles              cycle      2930.34
    Compute (SM) Throughput           %        11.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12285.33
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2930.34
    Total L1 Elapsed Cycles          cycle       288780
    Average L2 Active Cycles         cycle      2521.42
    Total L2 Elapsed Cycles          cycle       134232
    Average SM Active Cycles         cycle      2930.34
    Total SM Elapsed Cycles          cycle       288780
    Average SMSP Active Cycles       cycle      2911.66
    Total SMSP Elapsed Cycles        cycle      1155120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.032%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.25% above the average, while the minimum instance value is 13.66% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.032%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.25% above the average, while the minimum instance value is 13.66% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.602%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.64% above the average, while the minimum instance value is 5.25% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.49
    Elapsed Cycles                cycle         5938
    Memory Throughput                 %        30.58
    DRAM Throughput                   %        30.58
    Duration                         us         7.49
    L1/TEX Cache Throughput           %        17.97
    L2 Cache Throughput               %        17.14
    SM Active Cycles              cycle      3143.26
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.46
    Achieved Active Warps Per SM           warp        36.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14090.67
    Total DRAM Elapsed Cycles        cycle       276480
    Average L1 Active Cycles         cycle      3143.26
    Total L1 Elapsed Cycles          cycle       306254
    Average L2 Active Cycles         cycle      2606.67
    Total L2 Elapsed Cycles          cycle       145872
    Average SM Active Cycles         cycle      3143.26
    Total SM Elapsed Cycles          cycle       306254
    Average SMSP Active Cycles       cycle      3099.08
    Total SMSP Elapsed Cycles        cycle      1225016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.084%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.66% above the average, while the minimum instance value is 11.01% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.89
    Elapsed Cycles                cycle         5923
    Memory Throughput                 %        30.65
    DRAM Throughput                   %        30.65
    Duration                         us         7.46
    L1/TEX Cache Throughput           %        17.90
    L2 Cache Throughput               %        17.26
    SM Active Cycles              cycle      3155.74
    Compute (SM) Throughput           %        10.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 21.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.19
    Achieved Active Warps Per SM           warp        37.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 21.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14069.33
    Total DRAM Elapsed Cycles        cycle       275456
    Average L1 Active Cycles         cycle      3155.74
    Total L1 Elapsed Cycles          cycle       314050
    Average L2 Active Cycles         cycle         2680
    Total L2 Elapsed Cycles          cycle       144888
    Average SM Active Cycles         cycle      3155.74
    Total SM Elapsed Cycles          cycle       314050
    Average SMSP Active Cycles       cycle      3185.12
    Total SMSP Elapsed Cycles        cycle      1256200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.253%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.63% above the average, while the minimum instance value is 14.67% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       789.10
    Elapsed Cycles                cycle         5786
    Memory Throughput                 %        31.15
    DRAM Throughput                   %        31.15
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        18.03
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3133.45
    Compute (SM) Throughput           %        10.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.00
    Achieved Active Warps Per SM           warp        36.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3133.45
    Total L1 Elapsed Cycles          cycle       306696
    Average L2 Active Cycles         cycle      2595.33
    Total L2 Elapsed Cycles          cycle       141840
    Average SM Active Cycles         cycle      3133.45
    Total SM Elapsed Cycles          cycle       306696
    Average SMSP Active Cycles       cycle         3097
    Total SMSP Elapsed Cycles        cycle      1226784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.671%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.68% above the average, while the minimum instance value is 9.20% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.49
    Elapsed Cycles                cycle      1312586
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292441.41
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.59
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203090.67
    Total DRAM Elapsed Cycles        cycle     60451840
    Average L1 Active Cycles         cycle   1292441.41
    Total L1 Elapsed Cycles          cycle     75133370
    Average L2 Active Cycles         cycle   1165540.08
    Total L2 Elapsed Cycles          cycle     31946328
    Average SM Active Cycles         cycle   1292441.41
    Total SM Elapsed Cycles          cycle     75133370
    Average SMSP Active Cycles       cycle   1292430.05
    Total SMSP Elapsed Cycles        cycle    300533480
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.03
    Elapsed Cycles                cycle       393164
    Memory Throughput                 %        87.08
    DRAM Throughput                   %        87.08
    Duration                         us       484.42
    L1/TEX Cache Throughput           %        48.81
    L2 Cache Throughput               %        57.67
    SM Active Cycles              cycle    382798.02
    Compute (SM) Throughput           %        30.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.51
    Achieved Active Warps Per SM           warp        45.85
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2633888
    Total DRAM Elapsed Cycles        cycle     18147328
    Average L1 Active Cycles         cycle    382798.02
    Total L1 Elapsed Cycles          cycle     22430578
    Average L2 Active Cycles         cycle    397556.04
    Total L2 Elapsed Cycles          cycle      9590568
    Average SM Active Cycles         cycle    382798.02
    Total SM Elapsed Cycles          cycle     22430578
    Average SMSP Active Cycles       cycle    383961.00
    Total SMSP Elapsed Cycles        cycle     89722312
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.64
    Elapsed Cycles                cycle      1212740
    Memory Throughput                 %        84.55
    DRAM Throughput                   %        23.07
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.44
    L2 Cache Throughput               %        13.94
    SM Active Cycles              cycle   1158391.81
    Compute (SM) Throughput           %        84.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.98
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147234.67
    Total DRAM Elapsed Cycles        cycle     55841792
    Average L1 Active Cycles         cycle   1158391.81
    Total L1 Elapsed Cycles          cycle     69475464
    Average L2 Active Cycles         cycle   1205058.17
    Total L2 Elapsed Cycles          cycle     29509464
    Average SM Active Cycles         cycle   1158391.81
    Total SM Elapsed Cycles          cycle     69475464
    Average SMSP Active Cycles       cycle   1157895.74
    Total SMSP Elapsed Cycles        cycle    277901856
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.09
    SM Frequency                    Mhz       783.20
    Elapsed Cycles                cycle         5521
    Memory Throughput                 %        28.46
    DRAM Throughput                   %        28.46
    Duration                         us         6.98
    L1/TEX Cache Throughput           %        19.24
    L2 Cache Throughput               %        15.77
    SM Active Cycles              cycle      2936.71
    Compute (SM) Throughput           %        11.33
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12096
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2936.71
    Total L1 Elapsed Cycles          cycle       289212
    Average L2 Active Cycles         cycle      2406.71
    Total L2 Elapsed Cycles          cycle       135216
    Average SM Active Cycles         cycle      2936.71
    Total SM Elapsed Cycles          cycle       289212
    Average SMSP Active Cycles       cycle      2788.71
    Total SMSP Elapsed Cycles        cycle      1156848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.858%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.47% above the average, while the minimum instance value is 21.61% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       787.73
    Elapsed Cycles                cycle         5648
    Memory Throughput                 %        32.22
    DRAM Throughput                   %        32.22
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.31
    L2 Cache Throughput               %        18.07
    SM Active Cycles              cycle      3264.38
    Compute (SM) Throughput           %        10.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.07
    Achieved Active Warps Per SM           warp        34.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14077.33
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3264.38
    Total L1 Elapsed Cycles          cycle       305746
    Average L2 Active Cycles         cycle      2627.29
    Total L2 Elapsed Cycles          cycle       138504
    Average SM Active Cycles         cycle      3264.38
    Total SM Elapsed Cycles          cycle       305746
    Average SMSP Active Cycles       cycle      3123.88
    Total SMSP Elapsed Cycles        cycle      1222984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.186%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.44% above the average, while the minimum instance value is 16.19% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.72
    Elapsed Cycles                cycle         5709
    Memory Throughput                 %        31.78
    DRAM Throughput                   %        31.78
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.50
    L2 Cache Throughput               %        17.84
    SM Active Cycles              cycle      3228.02
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14101.33
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3228.02
    Total L1 Elapsed Cycles          cycle       306306
    Average L2 Active Cycles         cycle      2590.42
    Total L2 Elapsed Cycles          cycle       140208
    Average SM Active Cycles         cycle      3228.02
    Total SM Elapsed Cycles          cycle       306306
    Average SMSP Active Cycles       cycle      3087.38
    Total SMSP Elapsed Cycles        cycle      1225224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.92
    Elapsed Cycles                cycle         5768
    Memory Throughput                 %        30.94
    DRAM Throughput                   %        30.94
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.74
    L2 Cache Throughput               %        17.66
    SM Active Cycles              cycle      3184.74
    Compute (SM) Throughput           %        10.38
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.21
    Achieved Active Warps Per SM           warp        36.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13938.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3184.74
    Total L1 Elapsed Cycles          cycle       315586
    Average L2 Active Cycles         cycle      2656.08
    Total L2 Elapsed Cycles          cycle       141576
    Average SM Active Cycles         cycle      3184.74
    Total SM Elapsed Cycles          cycle       315586
    Average SMSP Active Cycles       cycle      3178.88
    Total SMSP Elapsed Cycles        cycle      1262344
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.55
    Elapsed Cycles                cycle      1312162
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.54
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292316.45
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.58
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203816
    Total DRAM Elapsed Cycles        cycle     60438528
    Average L1 Active Cycles         cycle   1292316.45
    Total L1 Elapsed Cycles          cycle     75118542
    Average L2 Active Cycles         cycle      1165245
    Total L2 Elapsed Cycles          cycle     31938816
    Average SM Active Cycles         cycle   1292316.45
    Total SM Elapsed Cycles          cycle     75118542
    Average SMSP Active Cycles       cycle      1292703
    Total SMSP Elapsed Cycles        cycle    300474168
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.99
    Elapsed Cycles                cycle       402877
    Memory Throughput                 %        87.53
    DRAM Throughput                   %        87.53
    Duration                         us       481.47
    L1/TEX Cache Throughput           %        47.75
    L2 Cache Throughput               %        57.50
    SM Active Cycles              cycle    390768.71
    Compute (SM) Throughput           %        30.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.68
    Achieved Active Warps Per SM           warp        45.92
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2631186.67
    Total DRAM Elapsed Cycles        cycle     18035712
    Average L1 Active Cycles         cycle    390768.71
    Total L1 Elapsed Cycles          cycle     22884634
    Average L2 Active Cycles         cycle    398694.42
    Total L2 Elapsed Cycles          cycle      9621408
    Average SM Active Cycles         cycle    390768.71
    Total SM Elapsed Cycles          cycle     22884634
    Average SMSP Active Cycles       cycle    391661.03
    Total SMSP Elapsed Cycles        cycle     91538536
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       821.29
    Elapsed Cycles                cycle      1218673
    Memory Throughput                 %        84.53
    DRAM Throughput                   %        23.48
    Duration                         ms         1.47
    L1/TEX Cache Throughput           %        87.57
    L2 Cache Throughput               %        14.13
    SM Active Cycles              cycle   1156582.97
    Compute (SM) Throughput           %        84.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.15
    Achieved Active Warps Per SM           warp        37.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148085.33
    Total DRAM Elapsed Cycles        cycle     54890496
    Average L1 Active Cycles         cycle   1156582.97
    Total L1 Elapsed Cycles          cycle     69493026
    Average L2 Active Cycles         cycle   1181421.38
    Total L2 Elapsed Cycles          cycle     29072712
    Average SM Active Cycles         cycle   1156582.97
    Total SM Elapsed Cycles          cycle     69493026
    Average SMSP Active Cycles       cycle   1157347.83
    Total SMSP Elapsed Cycles        cycle    277972104
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       798.40
    Elapsed Cycles                cycle         5568
    Memory Throughput                 %        29.24
    DRAM Throughput                   %        29.24
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        18.99
    L2 Cache Throughput               %        15.93
    SM Active Cycles              cycle      2975.66
    Compute (SM) Throughput           %        11.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.68
    Achieved Active Warps Per SM           warp        34.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12325.33
    Total DRAM Elapsed Cycles        cycle       252928
    Average L1 Active Cycles         cycle      2975.66
    Total L1 Elapsed Cycles          cycle       291012
    Average L2 Active Cycles         cycle      2525.92
    Total L2 Elapsed Cycles          cycle       133824
    Average SM Active Cycles         cycle      2975.66
    Total SM Elapsed Cycles          cycle       291012
    Average SMSP Active Cycles       cycle      2896.05
    Total SMSP Elapsed Cycles        cycle      1164048
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.023%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.16% above the average, while the minimum instance value is 19.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.023%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.16% above the average, while the minimum instance value is 19.68% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.25%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.59% above the average, while the minimum instance value is 6.05% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       828.38
    Elapsed Cycles                cycle         6015
    Memory Throughput                 %        31.93
    DRAM Throughput                   %        31.93
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        16.87
    L2 Cache Throughput               %        17.50
    SM Active Cycles              cycle      3349.47
    Compute (SM) Throughput           %         9.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3349.47
    Total L1 Elapsed Cycles          cycle       330268
    Average L2 Active Cycles         cycle      2628.08
    Total L2 Elapsed Cycles          cycle       142968
    Average SM Active Cycles         cycle      3349.47
    Total SM Elapsed Cycles          cycle       330268
    Average SMSP Active Cycles       cycle      3205.36
    Total SMSP Elapsed Cycles        cycle      1321072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.755%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 11.48% above the average, while the minimum instance value is 14.61% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.31%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.21% above the average, while the minimum instance value is 14.21% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.755%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.48% above the average, while the minimum instance value is 14.61% below the      
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       797.74
    Elapsed Cycles                cycle         5869
    Memory Throughput                 %        31.72
    DRAM Throughput                   %        31.72
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        17.74
    SM Active Cycles              cycle      3278.21
    Compute (SM) Throughput           %        10.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.81
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14128
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3278.21
    Total L1 Elapsed Cycles          cycle       311672
    Average L2 Active Cycles         cycle      2699.33
    Total L2 Elapsed Cycles          cycle       140976
    Average SM Active Cycles         cycle      3278.21
    Total SM Elapsed Cycles          cycle       311672
    Average SMSP Active Cycles       cycle      3255.02
    Total SMSP Elapsed Cycles        cycle      1246688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.189%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.51% above the average, while the minimum instance value is 10.50% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.585%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.87% above the average, while the minimum instance value is 16.19% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.189%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.51% above the average, while the minimum instance value is 10.50% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.62
    Elapsed Cycles                cycle         5702
    Memory Throughput                 %        31.23
    DRAM Throughput                   %        31.23
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.84
    L2 Cache Throughput               %        17.62
    SM Active Cycles              cycle      3166.83
    Compute (SM) Throughput           %        10.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.53
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3166.83
    Total L1 Elapsed Cycles          cycle       305084
    Average L2 Active Cycles         cycle      2658.88
    Total L2 Elapsed Cycles          cycle       141936
    Average SM Active Cycles         cycle      3166.83
    Total SM Elapsed Cycles          cycle       305084
    Average SMSP Active Cycles       cycle      3185.52
    Total SMSP Elapsed Cycles        cycle      1220336
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.157%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.51% above the average, while the minimum instance value is 11.82% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.39
    Elapsed Cycles                cycle      1298578
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.83
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.47
    SM Active Cycles              cycle   1292657.02
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203752
    Total DRAM Elapsed Cycles        cycle     61034496
    Average L1 Active Cycles         cycle   1292657.02
    Total L1 Elapsed Cycles          cycle     75129032
    Average L2 Active Cycles         cycle   1148485.17
    Total L2 Elapsed Cycles          cycle     32254728
    Average SM Active Cycles         cycle   1292657.02
    Total SM Elapsed Cycles          cycle     75129032
    Average SMSP Active Cycles       cycle   1292753.69
    Total SMSP Elapsed Cycles        cycle    300516128
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.99
    Elapsed Cycles                cycle       401333
    Memory Throughput                 %        87.55
    DRAM Throughput                   %        87.55
    Duration                         us       479.65
    L1/TEX Cache Throughput           %        46.81
    L2 Cache Throughput               %        57.70
    SM Active Cycles              cycle    390611.10
    Compute (SM) Throughput           %        29.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.32
    Achieved Active Warps Per SM           warp        45.75
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2621514.67
    Total DRAM Elapsed Cycles        cycle     17965056
    Average L1 Active Cycles         cycle    390611.10
    Total L1 Elapsed Cycles          cycle     23348816
    Average L2 Active Cycles         cycle    399703.12
    Total L2 Elapsed Cycles          cycle      9582888
    Average SM Active Cycles         cycle    390611.10
    Total SM Elapsed Cycles          cycle     23348816
    Average SMSP Active Cycles       cycle    393638.69
    Total SMSP Elapsed Cycles        cycle     93395264
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.82
    Elapsed Cycles                cycle      1211185
    Memory Throughput                 %        84.74
    DRAM Throughput                   %        23.12
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle   1157997.76
    Compute (SM) Throughput           %        84.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2149160
    Total DRAM Elapsed Cycles        cycle     55763968
    Average L1 Active Cycles         cycle   1157997.76
    Total L1 Elapsed Cycles          cycle     69321806
    Average L2 Active Cycles         cycle   1203295.08
    Total L2 Elapsed Cycles          cycle     29468088
    Average SM Active Cycles         cycle   1157997.76
    Total SM Elapsed Cycles          cycle     69321806
    Average SMSP Active Cycles       cycle      1157433
    Total SMSP Elapsed Cycles        cycle    277287224
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.01
    Elapsed Cycles                cycle         5569
    Memory Throughput                 %        27.92
    DRAM Throughput                   %        27.92
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        19.79
    L2 Cache Throughput               %        15.64
    SM Active Cycles              cycle      2854.41
    Compute (SM) Throughput           %        11.29
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.01
    Achieved Active Warps Per SM           warp        36.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12053.33
    Total DRAM Elapsed Cycles        cycle       259072
    Average L1 Active Cycles         cycle      2854.41
    Total L1 Elapsed Cycles          cycle       290276
    Average L2 Active Cycles         cycle      2414.83
    Total L2 Elapsed Cycles          cycle       136392
    Average SM Active Cycles         cycle      2854.41
    Total SM Elapsed Cycles          cycle       290276
    Average SMSP Active Cycles       cycle      2749.73
    Total SMSP Elapsed Cycles        cycle      1161104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.506%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.02% above the average, while the minimum instance value is 23.16% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.648%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.29% above the average, while the minimum instance value is 6.74% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.19
    Elapsed Cycles                cycle         5755
    Memory Throughput                 %        31.52
    DRAM Throughput                   %        31.52
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.38
    L2 Cache Throughput               %        17.70
    SM Active Cycles              cycle      3250.57
    Compute (SM) Throughput           %        10.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.99
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14096
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3250.57
    Total L1 Elapsed Cycles          cycle       311218
    Average L2 Active Cycles         cycle      2568.21
    Total L2 Elapsed Cycles          cycle       141216
    Average SM Active Cycles         cycle      3250.57
    Total SM Elapsed Cycles          cycle       311218
    Average SMSP Active Cycles       cycle      3083.38
    Total SMSP Elapsed Cycles        cycle      1244872
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.182%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 10.21% above the average, while the minimum instance value is 11.46% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.855%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.93% above the average, while the minimum instance value is 14.22% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.182%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 10.21% above the average, while the minimum instance value is 11.46% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.90
    Elapsed Cycles                cycle         5594
    Memory Throughput                 %        32.36
    DRAM Throughput                   %        32.36
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.04
    L2 Cache Throughput               %        18.17
    SM Active Cycles              cycle      3314.98
    Compute (SM) Throughput           %        11.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.86
    Achieved Active Warps Per SM           warp        33.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14082.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3314.98
    Total L1 Elapsed Cycles          cycle       297610
    Average L2 Active Cycles         cycle      2639.12
    Total L2 Elapsed Cycles          cycle       137640
    Average SM Active Cycles         cycle      3314.98
    Total SM Elapsed Cycles          cycle       297610
    Average SMSP Active Cycles       cycle      3130.75
    Total SMSP Elapsed Cycles        cycle      1190440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.817%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.00% above the average, while the minimum instance value is 11.34% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.53%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.70% above the average, while the minimum instance value is 12.29% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.817%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.00% above the average, while the minimum instance value is 11.34% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.43
    Elapsed Cycles                cycle         5702
    Memory Throughput                 %        31.63
    DRAM Throughput                   %        31.63
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        18.08
    L2 Cache Throughput               %        17.82
    SM Active Cycles              cycle      3125.33
    Compute (SM) Throughput           %        10.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.98
    Achieved Active Warps Per SM           warp        35.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14090.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3125.33
    Total L1 Elapsed Cycles          cycle       306988
    Average L2 Active Cycles         cycle      2567.92
    Total L2 Elapsed Cycles          cycle       140544
    Average SM Active Cycles         cycle      3125.33
    Total SM Elapsed Cycles          cycle       306988
    Average SMSP Active Cycles       cycle      3049.17
    Total SMSP Elapsed Cycles        cycle      1227952
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.83
    Elapsed Cycles                cycle      1313970
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.30
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292370.22
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204405.33
    Total DRAM Elapsed Cycles        cycle     58742784
    Average L1 Active Cycles         cycle   1292370.22
    Total L1 Elapsed Cycles          cycle     75119708
    Average L2 Active Cycles         cycle   1146785.25
    Total L2 Elapsed Cycles          cycle     31310592
    Average SM Active Cycles         cycle   1292370.22
    Total SM Elapsed Cycles          cycle     75119708
    Average SMSP Active Cycles       cycle   1292603.64
    Total SMSP Elapsed Cycles        cycle    300478832
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       819.72
    Elapsed Cycles                cycle       398966
    Memory Throughput                 %        87.69
    DRAM Throughput                   %        87.69
    Duration                         us       480.42
    L1/TEX Cache Throughput           %        47.85
    L2 Cache Throughput               %        57.93
    SM Active Cycles              cycle    388542.59
    Compute (SM) Throughput           %        30.06
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.21
    Achieved Active Warps Per SM           warp        45.70
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2630080
    Total DRAM Elapsed Cycles        cycle     17995776
    Average L1 Active Cycles         cycle    388542.59
    Total L1 Elapsed Cycles          cycle     22839592
    Average L2 Active Cycles         cycle    396397.08
    Total L2 Elapsed Cycles          cycle      9545472
    Average SM Active Cycles         cycle    388542.59
    Total SM Elapsed Cycles          cycle     22839592
    Average SMSP Active Cycles       cycle    388560.08
    Total SMSP Elapsed Cycles        cycle     91358368
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.64
    Elapsed Cycles                cycle      1213326
    Memory Throughput                 %        84.57
    DRAM Throughput                   %        23.84
    Duration                         ms         1.44
    L1/TEX Cache Throughput           %        87.41
    L2 Cache Throughput               %        14.21
    SM Active Cycles              cycle   1158784.07
    Compute (SM) Throughput           %        84.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.97
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2149096
    Total DRAM Elapsed Cycles        cycle     54091776
    Average L1 Active Cycles         cycle   1158784.07
    Total L1 Elapsed Cycles          cycle     69462958
    Average L2 Active Cycles         cycle   1182968.42
    Total L2 Elapsed Cycles          cycle     28904736
    Average SM Active Cycles         cycle   1158784.07
    Total SM Elapsed Cycles          cycle     69462958
    Average SMSP Active Cycles       cycle   1158366.79
    Total SMSP Elapsed Cycles        cycle    277851832
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.89
    Elapsed Cycles                cycle         5409
    Memory Throughput                 %        28.90
    DRAM Throughput                   %        28.90
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.39
    L2 Cache Throughput               %        15.82
    SM Active Cycles              cycle      2913.90
    Compute (SM) Throughput           %        11.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.86
    Achieved Active Warps Per SM           warp        34.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12282.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2913.90
    Total L1 Elapsed Cycles          cycle       293256
    Average L2 Active Cycles         cycle      2458.04
    Total L2 Elapsed Cycles          cycle       134736
    Average SM Active Cycles         cycle      2913.90
    Total SM Elapsed Cycles          cycle       293256
    Average SMSP Active Cycles       cycle      2847.50
    Total SMSP Elapsed Cycles        cycle      1173024
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.048%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.81% above the average, while the minimum instance value is 4.60% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       781.41
    Elapsed Cycles                cycle         5759
    Memory Throughput                 %        30.84
    DRAM Throughput                   %        30.84
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.86
    L2 Cache Throughput               %        17.43
    SM Active Cycles              cycle      3162.52
    Compute (SM) Throughput           %        10.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.01
    Achieved Active Warps Per SM           warp        36.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14053.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3162.52
    Total L1 Elapsed Cycles          cycle       325110
    Average L2 Active Cycles         cycle         2580
    Total L2 Elapsed Cycles          cycle       143376
    Average SM Active Cycles         cycle      3162.52
    Total SM Elapsed Cycles          cycle       325110
    Average SMSP Active Cycles       cycle      3079.38
    Total SMSP Elapsed Cycles        cycle      1300440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.498%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.75% above the average, while the minimum instance value is 10.01% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.716%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.41% above the average, while the minimum instance value is 11.64% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.498%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.75% above the average, while the minimum instance value is 10.01% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.28
    Elapsed Cycles                cycle         5743
    Memory Throughput                 %        31.55
    DRAM Throughput                   %        31.55
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.40
    L2 Cache Throughput               %        17.73
    SM Active Cycles              cycle      3246.79
    Compute (SM) Throughput           %        10.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.91
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14053.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3246.79
    Total L1 Elapsed Cycles          cycle       310692
    Average L2 Active Cycles         cycle      2627.54
    Total L2 Elapsed Cycles          cycle       141000
    Average SM Active Cycles         cycle      3246.79
    Total SM Elapsed Cycles          cycle       310692
    Average SMSP Active Cycles       cycle      3121.64
    Total SMSP Elapsed Cycles        cycle      1242768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.09%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.38% above the average, while the minimum instance value is 5.42% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       785.76
    Elapsed Cycles                cycle         5659
    Memory Throughput                 %        32.22
    DRAM Throughput                   %        32.22
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        18.00
    SM Active Cycles              cycle      3138.98
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.76
    Achieved Active Warps Per SM           warp        35.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14024
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3138.98
    Total L1 Elapsed Cycles          cycle       306588
    Average L2 Active Cycles         cycle      2581.12
    Total L2 Elapsed Cycles          cycle       138696
    Average SM Active Cycles         cycle      3138.98
    Total SM Elapsed Cycles          cycle       306588
    Average SMSP Active Cycles       cycle      3087.72
    Total SMSP Elapsed Cycles        cycle      1226352
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.38
    Elapsed Cycles                cycle      1312206
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292393.50
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.58
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204370.67
    Total DRAM Elapsed Cycles        cycle     60460032
    Average L1 Active Cycles         cycle   1292393.50
    Total L1 Elapsed Cycles          cycle     75137852
    Average L2 Active Cycles         cycle   1164825.50
    Total L2 Elapsed Cycles          cycle     31950168
    Average SM Active Cycles         cycle   1292393.50
    Total SM Elapsed Cycles          cycle     75137852
    Average SMSP Active Cycles       cycle   1292384.28
    Total SMSP Elapsed Cycles        cycle    300551408
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.25
    Elapsed Cycles                cycle       391124
    Memory Throughput                 %        87.35
    DRAM Throughput                   %        87.35
    Duration                         us       481.73
    L1/TEX Cache Throughput           %        48.60
    L2 Cache Throughput               %        58.02
    SM Active Cycles              cycle    383918.50
    Compute (SM) Throughput           %        30.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.54
    Achieved Active Warps Per SM           warp        45.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2627005.33
    Total DRAM Elapsed Cycles        cycle     18044928
    Average L1 Active Cycles         cycle    383918.50
    Total L1 Elapsed Cycles          cycle     22492640
    Average L2 Active Cycles         cycle    395971.83
    Total L2 Elapsed Cycles          cycle      9537216
    Average SM Active Cycles         cycle    383918.50
    Total SM Elapsed Cycles          cycle     22492640
    Average SMSP Active Cycles       cycle    382650.37
    Total SMSP Elapsed Cycles        cycle     89970560
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.81
    Elapsed Cycles                cycle      1210996
    Memory Throughput                 %        84.90
    DRAM Throughput                   %        23.12
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle   1158032.47
    Compute (SM) Throughput           %        84.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148082.67
    Total DRAM Elapsed Cycles        cycle     55755776
    Average L1 Active Cycles         cycle   1158032.47
    Total L1 Elapsed Cycles          cycle     69189608
    Average L2 Active Cycles         cycle   1204234.67
    Total L2 Elapsed Cycles          cycle     29464296
    Average SM Active Cycles         cycle   1158032.47
    Total SM Elapsed Cycles          cycle     69189608
    Average SMSP Active Cycles       cycle   1158113.22
    Total SMSP Elapsed Cycles        cycle    276758432
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       788.67
    Elapsed Cycles                cycle         5460
    Memory Throughput                 %        28.47
    DRAM Throughput                   %        28.47
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.57
    L2 Cache Throughput               %        15.98
    SM Active Cycles              cycle         2887
    Compute (SM) Throughput           %        11.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12050.67
    Total DRAM Elapsed Cycles        cycle       253952
    Average L1 Active Cycles         cycle         2887
    Total L1 Elapsed Cycles          cycle       288694
    Average L2 Active Cycles         cycle      2442.50
    Total L2 Elapsed Cycles          cycle       133416
    Average SM Active Cycles         cycle         2887
    Total SM Elapsed Cycles          cycle       288694
    Average SMSP Active Cycles       cycle      2799.12
    Total SMSP Elapsed Cycles        cycle      1154776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.239%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.76% above the average, while the minimum instance value is 19.95% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.977%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.63% above the average, while the minimum instance value is 30.19% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.239%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.76% above the average, while the minimum instance value is 19.95% below the      
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.17
    Elapsed Cycles                cycle         5908
    Memory Throughput                 %        30.71
    DRAM Throughput                   %        30.71
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        17.50
    L2 Cache Throughput               %        17.25
    SM Active Cycles              cycle      3228.10
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.34
    Achieved Active Warps Per SM           warp        34.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       274432
    Average L1 Active Cycles         cycle      3228.10
    Total L1 Elapsed Cycles          cycle       307694
    Average L2 Active Cycles         cycle      2656.54
    Total L2 Elapsed Cycles          cycle       144984
    Average SM Active Cycles         cycle      3228.10
    Total SM Elapsed Cycles          cycle       307694
    Average SMSP Active Cycles       cycle      3147.29
    Total SMSP Elapsed Cycles        cycle      1230776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.537%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.02% above the average, while the minimum instance value is 16.72% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       784.98
    Elapsed Cycles                cycle         5774
    Memory Throughput                 %        31.67
    DRAM Throughput                   %        31.67
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.50
    L2 Cache Throughput               %        17.61
    SM Active Cycles              cycle      3227.90
    Compute (SM) Throughput           %        10.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.04
    Achieved Active Warps Per SM           warp        35.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14106.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3227.90
    Total L1 Elapsed Cycles          cycle       307874
    Average L2 Active Cycles         cycle      2563.12
    Total L2 Elapsed Cycles          cycle       142104
    Average SM Active Cycles         cycle      3227.90
    Total SM Elapsed Cycles          cycle       307874
    Average SMSP Active Cycles       cycle      3101.24
    Total SMSP Elapsed Cycles        cycle      1231496
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.062%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.32% above the average, while the minimum instance value is 11.03% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.43%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.29% above the average, while the minimum instance value is 11.45% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.062%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.32% above the average, while the minimum instance value is 11.03% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.92
    Elapsed Cycles                cycle         5745
    Memory Throughput                 %        31.96
    DRAM Throughput                   %        31.96
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.17
    L2 Cache Throughput               %        18.12
    SM Active Cycles              cycle      3291.22
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.67
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13962.67
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3291.22
    Total L1 Elapsed Cycles          cycle       305860
    Average L2 Active Cycles         cycle      2541.38
    Total L2 Elapsed Cycles          cycle       137976
    Average SM Active Cycles         cycle      3291.22
    Total SM Elapsed Cycles          cycle       305860
    Average SMSP Active Cycles       cycle      3103.28
    Total SMSP Elapsed Cycles        cycle      1223440
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.36
    Elapsed Cycles                cycle      1298487
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.83
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.49
    SM Active Cycles              cycle   1292418.62
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204069.33
    Total DRAM Elapsed Cycles        cycle     61043712
    Average L1 Active Cycles         cycle   1292418.62
    Total L1 Elapsed Cycles          cycle     75124792
    Average L2 Active Cycles         cycle   1173337.67
    Total L2 Elapsed Cycles          cycle     32257632
    Average SM Active Cycles         cycle   1292418.62
    Total SM Elapsed Cycles          cycle     75124792
    Average SMSP Active Cycles       cycle   1292439.34
    Total SMSP Elapsed Cycles        cycle    300499168
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.79
    Elapsed Cycles                cycle       387065
    Memory Throughput                 %        87.22
    DRAM Throughput                   %        87.22
    Duration                         us       484.96
    L1/TEX Cache Throughput           %        48.31
    L2 Cache Throughput               %        57.58
    SM Active Cycles              cycle    379783.31
    Compute (SM) Throughput           %        30.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.45
    Achieved Active Warps Per SM           warp        45.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2640890.67
    Total DRAM Elapsed Cycles        cycle     18167808
    Average L1 Active Cycles         cycle    379783.31
    Total L1 Elapsed Cycles          cycle     22632568
    Average L2 Active Cycles         cycle    396047.12
    Total L2 Elapsed Cycles          cycle      9600480
    Average SM Active Cycles         cycle    379783.31
    Total SM Elapsed Cycles          cycle     22632568
    Average SMSP Active Cycles       cycle    382087.13
    Total SMSP Elapsed Cycles        cycle     90530272
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.10
    Elapsed Cycles                cycle      1212633
    Memory Throughput                 %        84.66
    DRAM Throughput                   %        23.09
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.37
    L2 Cache Throughput               %        13.94
    SM Active Cycles              cycle   1159291.16
    Compute (SM) Throughput           %        84.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.93
    Achieved Active Warps Per SM           warp        36.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148248
    Total DRAM Elapsed Cycles        cycle     55824384
    Average L1 Active Cycles         cycle   1159291.16
    Total L1 Elapsed Cycles          cycle     69391570
    Average L2 Active Cycles         cycle   1204638.54
    Total L2 Elapsed Cycles          cycle     29499816
    Average SM Active Cycles         cycle   1159291.16
    Total SM Elapsed Cycles          cycle     69391570
    Average SMSP Active Cycles       cycle   1158327.64
    Total SMSP Elapsed Cycles        cycle    277566280
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.50
    Elapsed Cycles                cycle         5477
    Memory Throughput                 %        28.89
    DRAM Throughput                   %        28.89
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        18.91
    L2 Cache Throughput               %        15.82
    SM Active Cycles              cycle      2987.43
    Compute (SM) Throughput           %        11.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.68
    Achieved Active Warps Per SM           warp        33.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12325.33
    Total DRAM Elapsed Cycles        cycle       256000
    Average L1 Active Cycles         cycle      2987.43
    Total L1 Elapsed Cycles          cycle       287790
    Average L2 Active Cycles         cycle      2406.58
    Total L2 Elapsed Cycles          cycle       134832
    Average SM Active Cycles         cycle      2987.43
    Total SM Elapsed Cycles          cycle       287790
    Average SMSP Active Cycles       cycle      2820.44
    Total SMSP Elapsed Cycles        cycle      1151160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.241%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.57% above the average, while the minimum instance value is 4.89% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.43
    Elapsed Cycles                cycle         5909
    Memory Throughput                 %        30.62
    DRAM Throughput                   %        30.62
    Duration                         us         7.46
    L1/TEX Cache Throughput           %        17.63
    L2 Cache Throughput               %        17.19
    SM Active Cycles              cycle      3205.38
    Compute (SM) Throughput           %        10.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.18
    Achieved Active Warps Per SM           warp        35.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       275456
    Average L1 Active Cycles         cycle      3205.38
    Total L1 Elapsed Cycles          cycle       310916
    Average L2 Active Cycles         cycle      2649.71
    Total L2 Elapsed Cycles          cycle       145272
    Average SM Active Cycles         cycle      3205.38
    Total SM Elapsed Cycles          cycle       310916
    Average SMSP Active Cycles       cycle      3136.15
    Total SMSP Elapsed Cycles        cycle      1243664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.667%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.48% above the average, while the minimum instance value is 8.12% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.498%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 11.11% above the average, while the minimum instance value is 8.36% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.667%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.48% above the average, while the minimum instance value is 8.12% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       785.51
    Elapsed Cycles                cycle         5816
    Memory Throughput                 %        31.26
    DRAM Throughput                   %        31.26
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        18.17
    L2 Cache Throughput               %        17.59
    SM Active Cycles              cycle      3109.97
    Compute (SM) Throughput           %        10.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 21.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        79.04
    Achieved Active Warps Per SM           warp        37.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (79.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3109.97
    Total L1 Elapsed Cycles          cycle       308782
    Average L2 Active Cycles         cycle      2658.17
    Total L2 Elapsed Cycles          cycle       142176
    Average SM Active Cycles         cycle      3109.97
    Total SM Elapsed Cycles          cycle       308782
    Average SMSP Active Cycles       cycle      3165.32
    Total SMSP Elapsed Cycles        cycle      1235128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.404%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.25% above the average, while the minimum instance value is 9.39% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.5%                                                                                            
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.25% above the average, while the minimum instance value is 12.74% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.404%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.25% above the average, while the minimum instance value is 9.39% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.86
    Elapsed Cycles                cycle         5711
    Memory Throughput                 %        31.52
    DRAM Throughput                   %        31.52
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.62
    L2 Cache Throughput               %        17.84
    SM Active Cycles              cycle      3206.57
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.13
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3206.57
    Total L1 Elapsed Cycles          cycle       306300
    Average L2 Active Cycles         cycle      2676.58
    Total L2 Elapsed Cycles          cycle       140112
    Average SM Active Cycles         cycle      3206.57
    Total SM Elapsed Cycles          cycle       306300
    Average SMSP Active Cycles       cycle      3197.38
    Total SMSP Elapsed Cycles        cycle      1225200
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.108%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.44% above the average, while the minimum instance value is 12.83% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.62
    Elapsed Cycles                cycle      1312482
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.98
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292475.48
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1206490.67
    Total DRAM Elapsed Cycles        cycle     60435456
    Average L1 Active Cycles         cycle   1292475.48
    Total L1 Elapsed Cycles          cycle     75134872
    Average L2 Active Cycles         cycle   1164868.29
    Total L2 Elapsed Cycles          cycle     31936632
    Average SM Active Cycles         cycle   1292475.48
    Total SM Elapsed Cycles          cycle     75134872
    Average SMSP Active Cycles       cycle   1292596.22
    Total SMSP Elapsed Cycles        cycle    300539488
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.26
    Elapsed Cycles                cycle       402778
    Memory Throughput                 %        87.30
    DRAM Throughput                   %        87.30
    Duration                         us       478.98
    L1/TEX Cache Throughput           %        47.78
    L2 Cache Throughput               %        57.50
    SM Active Cycles              cycle    394523.74
    Compute (SM) Throughput           %        30.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.83
    Achieved Active Warps Per SM           warp        45.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2610624
    Total DRAM Elapsed Cycles        cycle     17942528
    Average L1 Active Cycles         cycle    394523.74
    Total L1 Elapsed Cycles          cycle     22873732
    Average L2 Active Cycles         cycle    394797.96
    Total L2 Elapsed Cycles          cycle      9619272
    Average SM Active Cycles         cycle    394523.74
    Total SM Elapsed Cycles          cycle     22873732
    Average SMSP Active Cycles       cycle    387339.80
    Total SMSP Elapsed Cycles        cycle     91494928
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.44
    Elapsed Cycles                cycle      1190750
    Memory Throughput                 %        85.12
    DRAM Throughput                   %        23.05
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.47
    L2 Cache Throughput               %        13.91
    SM Active Cycles              cycle   1157957.55
    Compute (SM) Throughput           %        85.12
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2149674.67
    Total DRAM Elapsed Cycles        cycle     55956480
    Average L1 Active Cycles         cycle   1157957.55
    Total L1 Elapsed Cycles          cycle     69015232
    Average L2 Active Cycles         cycle   1212729.92
    Total L2 Elapsed Cycles          cycle     29569608
    Average SM Active Cycles         cycle   1157957.55
    Total SM Elapsed Cycles          cycle     69015232
    Average SMSP Active Cycles       cycle   1157608.26
    Total SMSP Elapsed Cycles        cycle    276060928
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       780.76
    Elapsed Cycles                cycle         5427
    Memory Throughput                 %        28.26
    DRAM Throughput                   %        28.26
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        20.07
    L2 Cache Throughput               %        15.76
    SM Active Cycles              cycle      2815.09
    Compute (SM) Throughput           %        11.22
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12104
    Total DRAM Elapsed Cycles        cycle       257024
    Average L1 Active Cycles         cycle      2815.09
    Total L1 Elapsed Cycles          cycle       292074
    Average L2 Active Cycles         cycle      2471.83
    Total L2 Elapsed Cycles          cycle       135312
    Average SM Active Cycles         cycle      2815.09
    Total SM Elapsed Cycles          cycle       292074
    Average SMSP Active Cycles       cycle      2897.30
    Total SMSP Elapsed Cycles        cycle      1168296
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.496%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.83% above the average, while the minimum instance value is 17.30% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.131%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.92% above the average, while the minimum instance value is 21.31% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.496%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.83% above the average, while the minimum instance value is 17.30% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.60
    Elapsed Cycles                cycle         5841
    Memory Throughput                 %        30.90
    DRAM Throughput                   %        30.90
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        17.50
    SM Active Cycles              cycle      3278.76
    Compute (SM) Throughput           %        10.48
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.39
    Achieved Active Warps Per SM           warp        34.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       272384
    Average L1 Active Cycles         cycle      3278.76
    Total L1 Elapsed Cycles          cycle       312576
    Average L2 Active Cycles         cycle      2693.62
    Total L2 Elapsed Cycles          cycle       142992
    Average SM Active Cycles         cycle      3278.76
    Total SM Elapsed Cycles          cycle       312576
    Average SMSP Active Cycles       cycle      3202.47
    Total SMSP Elapsed Cycles        cycle      1250304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.503%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.94% above the average, while the minimum instance value is 16.66% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       788.90
    Elapsed Cycles                cycle         5789
    Memory Throughput                 %        31.22
    DRAM Throughput                   %        31.22
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.21
    L2 Cache Throughput               %        17.65
    SM Active Cycles              cycle      3283.07
    Compute (SM) Throughput           %        10.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.29
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14066.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3283.07
    Total L1 Elapsed Cycles          cycle       300410
    Average L2 Active Cycles         cycle      2637.54
    Total L2 Elapsed Cycles          cycle       141648
    Average SM Active Cycles         cycle      3283.07
    Total SM Elapsed Cycles          cycle       300410
    Average SMSP Active Cycles       cycle      3129.14
    Total SMSP Elapsed Cycles        cycle      1201640
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       782.65
    Elapsed Cycles                cycle         5626
    Memory Throughput                 %        32.19
    DRAM Throughput                   %        32.19
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.94
    L2 Cache Throughput               %        18.08
    SM Active Cycles              cycle      3148.98
    Compute (SM) Throughput           %        11.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14008
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3148.98
    Total L1 Elapsed Cycles          cycle       296398
    Average L2 Active Cycles         cycle      2537.71
    Total L2 Elapsed Cycles          cycle       138408
    Average SM Active Cycles         cycle      3148.98
    Total SM Elapsed Cycles          cycle       296398
    Average SMSP Active Cycles       cycle      3043.20
    Total SMSP Elapsed Cycles        cycle      1185592
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.63
    Elapsed Cycles                cycle      1311866
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292650.33
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.53
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203338.67
    Total DRAM Elapsed Cycles        cycle     60414976
    Average L1 Active Cycles         cycle   1292650.33
    Total L1 Elapsed Cycles          cycle     75118446
    Average L2 Active Cycles         cycle   1165194.67
    Total L2 Elapsed Cycles          cycle     31926480
    Average SM Active Cycles         cycle   1292650.33
    Total SM Elapsed Cycles          cycle     75118446
    Average SMSP Active Cycles       cycle   1292314.76
    Total SMSP Elapsed Cycles        cycle    300473784
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.30
    Elapsed Cycles                cycle       390837
    Memory Throughput                 %        87.30
    DRAM Throughput                   %        87.30
    Duration                         us       481.31
    L1/TEX Cache Throughput           %        48.88
    L2 Cache Throughput               %        58.11
    SM Active Cycles              cycle    382889.47
    Compute (SM) Throughput           %        30.68
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.80
    Achieved Active Warps Per SM           warp        45.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2623290.67
    Total DRAM Elapsed Cycles        cycle     18029568
    Average L1 Active Cycles         cycle    382889.47
    Total L1 Elapsed Cycles          cycle     22372700
    Average L2 Active Cycles         cycle    395198.46
    Total L2 Elapsed Cycles          cycle      9528936
    Average SM Active Cycles         cycle    382889.47
    Total SM Elapsed Cycles          cycle     22372700
    Average SMSP Active Cycles       cycle    381276.20
    Total SMSP Elapsed Cycles        cycle     89490800
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.94
    Elapsed Cycles                cycle      1212709
    Memory Throughput                 %        84.75
    DRAM Throughput                   %        23.09
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.47
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1157950.14
    Compute (SM) Throughput           %        84.75
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148117.33
    Total DRAM Elapsed Cycles        cycle     55810048
    Average L1 Active Cycles         cycle   1157950.14
    Total L1 Elapsed Cycles          cycle     69312680
    Average L2 Active Cycles         cycle   1200623.67
    Total L2 Elapsed Cycles          cycle     29493072
    Average SM Active Cycles         cycle   1157950.14
    Total SM Elapsed Cycles          cycle     69312680
    Average SMSP Active Cycles       cycle   1157801.67
    Total SMSP Elapsed Cycles        cycle    277250720
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       800.56
    Elapsed Cycles                cycle         5531
    Memory Throughput                 %        29.27
    DRAM Throughput                   %        29.27
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.34
    L2 Cache Throughput               %        16.12
    SM Active Cycles              cycle      2921.91
    Compute (SM) Throughput           %        11.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.86
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12240
    Total DRAM Elapsed Cycles        cycle       250880
    Average L1 Active Cycles         cycle      2921.91
    Total L1 Elapsed Cycles          cycle       291530
    Average L2 Active Cycles         cycle      2458.17
    Total L2 Elapsed Cycles          cycle       132360
    Average SM Active Cycles         cycle      2921.91
    Total SM Elapsed Cycles          cycle       291530
    Average SMSP Active Cycles       cycle      2904.13
    Total SMSP Elapsed Cycles        cycle      1166120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.489%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.50% above the average, while the minimum instance value is 17.98% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.409%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.38% above the average, while the minimum instance value is 4.32% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       801.71
    Elapsed Cycles                cycle         5877
    Memory Throughput                 %        31.63
    DRAM Throughput                   %        31.63
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.03
    L2 Cache Throughput               %        17.83
    SM Active Cycles              cycle      3317.09
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.54
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3317.09
    Total L1 Elapsed Cycles          cycle       320014
    Average L2 Active Cycles         cycle      2649.92
    Total L2 Elapsed Cycles          cycle       140664
    Average SM Active Cycles         cycle      3317.09
    Total SM Elapsed Cycles          cycle       320014
    Average SMSP Active Cycles       cycle      3202.87
    Total SMSP Elapsed Cycles        cycle      1280056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.389%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.01% above the average, while the minimum instance value is 13.73% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       782.79
    Elapsed Cycles                cycle         5718
    Memory Throughput                 %        31.50
    DRAM Throughput                   %        31.50
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.53
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3223.47
    Compute (SM) Throughput           %        10.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.00
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3223.47
    Total L1 Elapsed Cycles          cycle       304678
    Average L2 Active Cycles         cycle      2669.33
    Total L2 Elapsed Cycles          cycle       142368
    Average SM Active Cycles         cycle      3223.47
    Total SM Elapsed Cycles          cycle       304678
    Average SMSP Active Cycles       cycle      3172.05
    Total SMSP Elapsed Cycles        cycle      1218712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.015%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.96% above the average, while the minimum instance value is 11.48% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       813.52
    Elapsed Cycles                cycle         5798
    Memory Throughput                 %        32.23
    DRAM Throughput                   %        32.23
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        17.22
    L2 Cache Throughput               %        18.12
    SM Active Cycles              cycle      3281.12
    Compute (SM) Throughput           %        10.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.40
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3281.12
    Total L1 Elapsed Cycles          cycle       312844
    Average L2 Active Cycles         cycle      2613.17
    Total L2 Elapsed Cycles          cycle       137904
    Average SM Active Cycles         cycle      3281.12
    Total SM Elapsed Cycles          cycle       312844
    Average SMSP Active Cycles       cycle      3164.41
    Total SMSP Elapsed Cycles        cycle      1251376
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.09
    Elapsed Cycles                cycle      1313670
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        12.35
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.93
    SM Active Cycles              cycle   1292605.12
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203792
    Total DRAM Elapsed Cycles        cycle     58504192
    Average L1 Active Cycles         cycle   1292605.12
    Total L1 Elapsed Cycles          cycle     75135730
    Average L2 Active Cycles         cycle   1146806.12
    Total L2 Elapsed Cycles          cycle     31272744
    Average SM Active Cycles         cycle   1292605.12
    Total SM Elapsed Cycles          cycle     75135730
    Average SMSP Active Cycles       cycle   1292608.45
    Total SMSP Elapsed Cycles        cycle    300542920
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.85
    Elapsed Cycles                cycle       386089
    Memory Throughput                 %        87.09
    DRAM Throughput                   %        87.09
    Duration                         us       483.65
    L1/TEX Cache Throughput           %        49.06
    L2 Cache Throughput               %        57.81
    SM Active Cycles              cycle    380900.05
    Compute (SM) Throughput           %        30.79
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.80
    Achieved Active Warps Per SM           warp        45.50
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2629426.67
    Total DRAM Elapsed Cycles        cycle     18115584
    Average L1 Active Cycles         cycle    380900.05
    Total L1 Elapsed Cycles          cycle     22293210
    Average L2 Active Cycles         cycle    397866.67
    Total L2 Elapsed Cycles          cycle      9574152
    Average SM Active Cycles         cycle    380900.05
    Total SM Elapsed Cycles          cycle     22293210
    Average SMSP Active Cycles       cycle    380500.10
    Total SMSP Elapsed Cycles        cycle     89172840
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.46
    Elapsed Cycles                cycle      1188752
    Memory Throughput                 %        84.72
    DRAM Throughput                   %        23.08
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.49
    L2 Cache Throughput               %        13.94
    SM Active Cycles              cycle   1157679.48
    Compute (SM) Throughput           %        84.72
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.04
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2149144
    Total DRAM Elapsed Cycles        cycle     55862272
    Average L1 Active Cycles         cycle   1157679.48
    Total L1 Elapsed Cycles          cycle     69337068
    Average L2 Active Cycles         cycle   1183098.83
    Total L2 Elapsed Cycles          cycle     29519592
    Average SM Active Cycles         cycle   1157679.48
    Total SM Elapsed Cycles          cycle     69337068
    Average SMSP Active Cycles       cycle   1158405.41
    Total SMSP Elapsed Cycles        cycle    277348272
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       799.26
    Elapsed Cycles                cycle         5496
    Memory Throughput                 %        29.02
    DRAM Throughput                   %        29.02
    Duration                         us         6.78
    L1/TEX Cache Throughput           %        18.99
    L2 Cache Throughput               %        16.18
    SM Active Cycles              cycle      2974.62
    Compute (SM) Throughput           %        11.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.34
    Achieved Active Warps Per SM           warp        34.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12082.67
    Total DRAM Elapsed Cycles        cycle       249856
    Average L1 Active Cycles         cycle      2974.62
    Total L1 Elapsed Cycles          cycle       295128
    Average L2 Active Cycles         cycle      2462.71
    Total L2 Elapsed Cycles          cycle       131736
    Average SM Active Cycles         cycle      2974.62
    Total SM Elapsed Cycles          cycle       295128
    Average SMSP Active Cycles       cycle      2915.70
    Total SMSP Elapsed Cycles        cycle      1180512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.96
    Elapsed Cycles                cycle         5882
    Memory Throughput                 %        30.40
    DRAM Throughput                   %        30.40
    Duration                         us         7.52
    L1/TEX Cache Throughput           %        17.26
    L2 Cache Throughput               %        17.07
    SM Active Cycles              cycle      3273.19
    Compute (SM) Throughput           %        10.37
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.60
    Achieved Active Warps Per SM           warp        35.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14058.67
    Total DRAM Elapsed Cycles        cycle       277504
    Average L1 Active Cycles         cycle      3273.19
    Total L1 Elapsed Cycles          cycle       315872
    Average L2 Active Cycles         cycle      2646.96
    Total L2 Elapsed Cycles          cycle       146448
    Average SM Active Cycles         cycle      3273.19
    Total SM Elapsed Cycles          cycle       315872
    Average SMSP Active Cycles       cycle      3133.44
    Total SMSP Elapsed Cycles        cycle      1263488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.212%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.67% above the average, while the minimum instance value is 10.91% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.212%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.67% above the average, while the minimum instance value is 10.91% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.98
    Elapsed Cycles                cycle         5543
    Memory Throughput                 %        32.28
    DRAM Throughput                   %        32.28
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.93
    L2 Cache Throughput               %        18.10
    SM Active Cycles              cycle      3150.78
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.92
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3150.78
    Total L1 Elapsed Cycles          cycle       306540
    Average L2 Active Cycles         cycle      2666.17
    Total L2 Elapsed Cycles          cycle       138072
    Average SM Active Cycles         cycle      3150.78
    Total SM Elapsed Cycles          cycle       306540
    Average SMSP Active Cycles       cycle      3149.49
    Total SMSP Elapsed Cycles        cycle      1226160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.675%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.52% above the average, while the minimum instance value is 13.41% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.66
    Elapsed Cycles                cycle         5566
    Memory Throughput                 %        32.31
    DRAM Throughput                   %        32.31
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.14
    L2 Cache Throughput               %        18.04
    SM Active Cycles              cycle      3296.88
    Compute (SM) Throughput           %        10.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.13
    Achieved Active Warps Per SM           warp        33.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3296.88
    Total L1 Elapsed Cycles          cycle       311726
    Average L2 Active Cycles         cycle      2644.38
    Total L2 Elapsed Cycles          cycle       138672
    Average SM Active Cycles         cycle      3296.88
    Total SM Elapsed Cycles          cycle       311726
    Average SMSP Active Cycles       cycle      3125.72
    Total SMSP Elapsed Cycles        cycle      1246904
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.63
    Elapsed Cycles                cycle      1312999
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292380.36
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.58
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203602.67
    Total DRAM Elapsed Cycles        cycle     60436480
    Average L1 Active Cycles         cycle   1292380.36
    Total L1 Elapsed Cycles          cycle     75115370
    Average L2 Active Cycles         cycle   1165321.17
    Total L2 Elapsed Cycles          cycle     31937736
    Average SM Active Cycles         cycle   1292380.36
    Total SM Elapsed Cycles          cycle     75115370
    Average SMSP Active Cycles       cycle   1292367.59
    Total SMSP Elapsed Cycles        cycle    300461480
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.31
    Elapsed Cycles                cycle       390927
    Memory Throughput                 %        87.64
    DRAM Throughput                   %        87.64
    Duration                         us       481.38
    L1/TEX Cache Throughput           %        48.63
    L2 Cache Throughput               %        58.04
    SM Active Cycles              cycle    383598.88
    Compute (SM) Throughput           %        30.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.66
    Achieved Active Warps Per SM           warp        45.44
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2634114.67
    Total DRAM Elapsed Cycles        cycle     18032640
    Average L1 Active Cycles         cycle    383598.88
    Total L1 Elapsed Cycles          cycle     22477848
    Average L2 Active Cycles         cycle    392660.25
    Total L2 Elapsed Cycles          cycle      9529992
    Average SM Active Cycles         cycle    383598.88
    Total SM Elapsed Cycles          cycle     22477848
    Average SMSP Active Cycles       cycle    379198.08
    Total SMSP Elapsed Cycles        cycle     89911392
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.07
    Elapsed Cycles                cycle      1212692
    Memory Throughput                 %        84.40
    DRAM Throughput                   %        23.09
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.44
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1158334.07
    Compute (SM) Throughput           %        84.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148330.67
    Total DRAM Elapsed Cycles        cycle     55816192
    Average L1 Active Cycles         cycle   1158334.07
    Total L1 Elapsed Cycles          cycle     69603672
    Average L2 Active Cycles         cycle   1204334.67
    Total L2 Elapsed Cycles          cycle     29496336
    Average SM Active Cycles         cycle   1158334.07
    Total SM Elapsed Cycles          cycle     69603672
    Average SMSP Active Cycles       cycle   1158110.97
    Total SMSP Elapsed Cycles        cycle    278414688
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.64
    Elapsed Cycles                cycle         5563
    Memory Throughput                 %        28.70
    DRAM Throughput                   %        28.70
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        18.88
    L2 Cache Throughput               %        15.64
    SM Active Cycles              cycle      2991.97
    Compute (SM) Throughput           %        10.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.75
    Achieved Active Warps Per SM           warp        33.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12341.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      2991.97
    Total L1 Elapsed Cycles          cycle       298588
    Average L2 Active Cycles         cycle      2447.21
    Total L2 Elapsed Cycles          cycle       136368
    Average SM Active Cycles         cycle      2991.97
    Total SM Elapsed Cycles          cycle       298588
    Average SMSP Active Cycles       cycle      2909.37
    Total SMSP Elapsed Cycles        cycle      1194352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.443%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 11.09% above the average, while the minimum instance value is 18.31% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.443%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 11.09% above the average, while the minimum instance value is 18.31% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.961%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 16.16% above the average, while the minimum instance value is 4.99% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       796.60
    Elapsed Cycles                cycle         5938
    Memory Throughput                 %        31.28
    DRAM Throughput                   %        31.28
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.68
    L2 Cache Throughput               %        17.45
    SM Active Cycles              cycle      3195.33
    Compute (SM) Throughput           %        10.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.22
    Achieved Active Warps Per SM           warp        36.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14093.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3195.33
    Total L1 Elapsed Cycles          cycle       315448
    Average L2 Active Cycles         cycle      2618.79
    Total L2 Elapsed Cycles          cycle       143760
    Average SM Active Cycles         cycle      3195.33
    Total SM Elapsed Cycles          cycle       315448
    Average SMSP Active Cycles       cycle      3145.02
    Total SMSP Elapsed Cycles        cycle      1261792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.188%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.97% above the average, while the minimum instance value is 10.87% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       794.90
    Elapsed Cycles                cycle         5871
    Memory Throughput                 %        31.03
    DRAM Throughput                   %        31.03
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.43
    L2 Cache Throughput               %        17.64
    SM Active Cycles              cycle      3241.69
    Compute (SM) Throughput           %        10.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.56
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13981.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3241.69
    Total L1 Elapsed Cycles          cycle       316590
    Average L2 Active Cycles         cycle      2619.17
    Total L2 Elapsed Cycles          cycle       142224
    Average SM Active Cycles         cycle      3241.69
    Total SM Elapsed Cycles          cycle       316590
    Average SMSP Active Cycles       cycle      3147.07
    Total SMSP Elapsed Cycles        cycle      1266360
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       822.72
    Elapsed Cycles                cycle         5782
    Memory Throughput                 %        32.98
    DRAM Throughput                   %        32.98
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        17.37
    L2 Cache Throughput               %        18.25
    SM Active Cycles              cycle      3252.07
    Compute (SM) Throughput           %        10.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.55
    Achieved Active Warps Per SM           warp        35.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14013.33
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      3252.07
    Total L1 Elapsed Cycles          cycle       305684
    Average L2 Active Cycles         cycle      2627.83
    Total L2 Elapsed Cycles          cycle       137544
    Average SM Active Cycles         cycle      3252.07
    Total SM Elapsed Cycles          cycle       305684
    Average SMSP Active Cycles       cycle      3196.98
    Total SMSP Elapsed Cycles        cycle      1222736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.188%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.55% above the average, while the minimum instance value is 17.89% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.38
    Elapsed Cycles                cycle      1298497
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.84
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.48
    SM Active Cycles              cycle   1292588.21
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.54
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204354.67
    Total DRAM Elapsed Cycles        cycle     61043712
    Average L1 Active Cycles         cycle   1292588.21
    Total L1 Elapsed Cycles          cycle     75139306
    Average L2 Active Cycles         cycle   1171605.58
    Total L2 Elapsed Cycles          cycle     32257536
    Average SM Active Cycles         cycle   1292588.21
    Total SM Elapsed Cycles          cycle     75139306
    Average SMSP Active Cycles       cycle   1292157.40
    Total SMSP Elapsed Cycles        cycle    300557224
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.95
    Elapsed Cycles                cycle       384912
    Memory Throughput                 %        87.68
    DRAM Throughput                   %        87.68
    Duration                         us       482.30
    L1/TEX Cache Throughput           %        49.24
    L2 Cache Throughput               %        58.02
    SM Active Cycles              cycle    375094.47
    Compute (SM) Throughput           %        30.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.22
    Achieved Active Warps Per SM           warp        46.19
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2640122.67
    Total DRAM Elapsed Cycles        cycle     18067456
    Average L1 Active Cycles         cycle    375094.47
    Total L1 Elapsed Cycles          cycle     22215062
    Average L2 Active Cycles         cycle    390543.38
    Total L2 Elapsed Cycles          cycle      9547824
    Average SM Active Cycles         cycle    375094.47
    Total SM Elapsed Cycles          cycle     22215062
    Average SMSP Active Cycles       cycle    373608.11
    Total SMSP Elapsed Cycles        cycle     88860248
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.70
    Elapsed Cycles                cycle      1211355
    Memory Throughput                 %        84.95
    DRAM Throughput                   %        23.11
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.51
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1157462.12
    Compute (SM) Throughput           %        84.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.05
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147962.67
    Total DRAM Elapsed Cycles        cycle     55773184
    Average L1 Active Cycles         cycle   1157462.12
    Total L1 Elapsed Cycles          cycle     69150200
    Average L2 Active Cycles         cycle   1205749.17
    Total L2 Elapsed Cycles          cycle     29472648
    Average SM Active Cycles         cycle   1157462.12
    Total SM Elapsed Cycles          cycle     69150200
    Average SMSP Active Cycles       cycle   1157145.26
    Total SMSP Elapsed Cycles        cycle    276600800
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.30
    Elapsed Cycles                cycle         5431
    Memory Throughput                 %        28.52
    DRAM Throughput                   %        28.52
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        19.57
    L2 Cache Throughput               %        15.97
    SM Active Cycles              cycle      2886.72
    Compute (SM) Throughput           %        11.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.04
    Achieved Active Warps Per SM           warp        35.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12120
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2886.72
    Total L1 Elapsed Cycles          cycle       297964
    Average L2 Active Cycles         cycle      2369.42
    Total L2 Elapsed Cycles          cycle       133608
    Average SM Active Cycles         cycle      2886.72
    Total SM Elapsed Cycles          cycle       297964
    Average SMSP Active Cycles       cycle      2757.74
    Total SMSP Elapsed Cycles        cycle      1191856
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.262%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.37% above the average, while the minimum instance value is 18.07% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.262%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.37% above the average, while the minimum instance value is 18.07% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.944%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.96% above the average, while the minimum instance value is 4.41% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.82
    Elapsed Cycles                cycle         5708
    Memory Throughput                 %        31.65
    DRAM Throughput                   %        31.65
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.99
    L2 Cache Throughput               %        17.85
    SM Active Cycles              cycle      3140.47
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.63
    Achieved Active Warps Per SM           warp        36.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3140.47
    Total L1 Elapsed Cycles          cycle       306038
    Average L2 Active Cycles         cycle      2633.54
    Total L2 Elapsed Cycles          cycle       140760
    Average SM Active Cycles         cycle      3140.47
    Total SM Elapsed Cycles          cycle       306038
    Average SMSP Active Cycles       cycle      3121.76
    Total SMSP Elapsed Cycles        cycle      1224152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.528%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.29% above the average, while the minimum instance value is 8.20% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.528%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.29% above the average, while the minimum instance value is 8.20% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.61
    Elapsed Cycles                cycle         5751
    Memory Throughput                 %        31.40
    DRAM Throughput                   %        31.40
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        18.08
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3125.55
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.68
    Achieved Active Warps Per SM           warp        36.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3125.55
    Total L1 Elapsed Cycles          cycle       302150
    Average L2 Active Cycles         cycle      2631.79
    Total L2 Elapsed Cycles          cycle       141888
    Average SM Active Cycles         cycle      3125.55
    Total SM Elapsed Cycles          cycle       302150
    Average SMSP Active Cycles       cycle      3142.83
    Total SMSP Elapsed Cycles        cycle      1208600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.939%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.84% above the average, while the minimum instance value is 11.83% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       785.19
    Elapsed Cycles                cycle         5743
    Memory Throughput                 %        31.57
    DRAM Throughput                   %        31.57
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.83
    L2 Cache Throughput               %        17.77
    SM Active Cycles              cycle      3168.76
    Compute (SM) Throughput           %        10.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.25
    Achieved Active Warps Per SM           warp        36.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14064
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3168.76
    Total L1 Elapsed Cycles          cycle       300946
    Average L2 Active Cycles         cycle      2668.58
    Total L2 Elapsed Cycles          cycle       141168
    Average SM Active Cycles         cycle      3168.76
    Total SM Elapsed Cycles          cycle       300946
    Average SMSP Active Cycles       cycle      3170.30
    Total SMSP Elapsed Cycles        cycle      1203784
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.33
    Elapsed Cycles                cycle      1312381
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.60
    SM Active Cycles              cycle   1292474.34
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203941.33
    Total DRAM Elapsed Cycles        cycle     60453888
    Average L1 Active Cycles         cycle   1292474.34
    Total L1 Elapsed Cycles          cycle     75120942
    Average L2 Active Cycles         cycle   1164818.96
    Total L2 Elapsed Cycles          cycle     31946616
    Average SM Active Cycles         cycle   1292474.34
    Total SM Elapsed Cycles          cycle     75120942
    Average SMSP Active Cycles       cycle      1292520
    Total SMSP Elapsed Cycles        cycle    300483768
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.14
    Elapsed Cycles                cycle       390629
    Memory Throughput                 %        87.10
    DRAM Throughput                   %        87.10
    Duration                         us       481.47
    L1/TEX Cache Throughput           %        49.20
    L2 Cache Throughput               %        58.09
    SM Active Cycles              cycle    378399.16
    Compute (SM) Throughput           %        30.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.15
    Achieved Active Warps Per SM           warp        46.15
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2618082.67
    Total DRAM Elapsed Cycles        cycle     18035712
    Average L1 Active Cycles         cycle    378399.16
    Total L1 Elapsed Cycles          cycle     22214878
    Average L2 Active Cycles         cycle    391319.12
    Total L2 Elapsed Cycles          cycle      9533136
    Average SM Active Cycles         cycle    378399.16
    Total SM Elapsed Cycles          cycle     22214878
    Average SMSP Active Cycles       cycle    377566.79
    Total SMSP Elapsed Cycles        cycle     88859512
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.04
    Elapsed Cycles                cycle      1212892
    Memory Throughput                 %        84.36
    DRAM Throughput                   %        23.73
    Duration                         ms         1.45
    L1/TEX Cache Throughput           %        87.43
    L2 Cache Throughput               %        14.20
    SM Active Cycles              cycle   1158434.53
    Compute (SM) Throughput           %        84.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147437.33
    Total DRAM Elapsed Cycles        cycle     54294528
    Average L1 Active Cycles         cycle   1158434.53
    Total L1 Elapsed Cycles          cycle     69636674
    Average L2 Active Cycles         cycle      1184583
    Total L2 Elapsed Cycles          cycle     28925376
    Average SM Active Cycles         cycle   1158434.53
    Total SM Elapsed Cycles          cycle     69636674
    Average SMSP Active Cycles       cycle   1158141.88
    Total SMSP Elapsed Cycles        cycle    278546696
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       815.42
    Elapsed Cycles                cycle         5658
    Memory Throughput                 %        28.90
    DRAM Throughput                   %        28.90
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.27
    L2 Cache Throughput               %        15.84
    SM Active Cycles              cycle      2931.16
    Compute (SM) Throughput           %        11.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.65
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12282.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2931.16
    Total L1 Elapsed Cycles          cycle       292742
    Average L2 Active Cycles         cycle      2461.29
    Total L2 Elapsed Cycles          cycle       134616
    Average SM Active Cycles         cycle      2931.16
    Total SM Elapsed Cycles          cycle       292742
    Average SMSP Active Cycles       cycle      2850.09
    Total SMSP Elapsed Cycles        cycle      1170968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.47
    Elapsed Cycles                cycle         5722
    Memory Throughput                 %        31.22
    DRAM Throughput                   %        31.22
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        18.07
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3127.16
    Compute (SM) Throughput           %        10.38
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.94
    Achieved Active Warps Per SM           warp        36.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13960
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3127.16
    Total L1 Elapsed Cycles          cycle       315622
    Average L2 Active Cycles         cycle      2624.58
    Total L2 Elapsed Cycles          cycle       142440
    Average SM Active Cycles         cycle      3127.16
    Total SM Elapsed Cycles          cycle       315622
    Average SMSP Active Cycles       cycle      3090.04
    Total SMSP Elapsed Cycles        cycle      1262488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.373%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.46% above the average, while the minimum instance value is 10.65% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.17
    Elapsed Cycles                cycle         5756
    Memory Throughput                 %        31.24
    DRAM Throughput                   %        31.24
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.59
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3212.29
    Compute (SM) Throughput           %        10.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.43
    Achieved Active Warps Per SM           warp        35.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14128
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3212.29
    Total L1 Elapsed Cycles          cycle       311990
    Average L2 Active Cycles         cycle      2570.12
    Total L2 Elapsed Cycles          cycle       143208
    Average SM Active Cycles         cycle      3212.29
    Total SM Elapsed Cycles          cycle       311990
    Average SMSP Active Cycles       cycle      3035.09
    Total SMSP Elapsed Cycles        cycle      1247960
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       779.55
    Elapsed Cycles                cycle         5592
    Memory Throughput                 %        32.17
    DRAM Throughput                   %        32.17
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.59
    L2 Cache Throughput               %        18.00
    SM Active Cycles              cycle      3212.66
    Compute (SM) Throughput           %        10.42
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.30
    Achieved Active Warps Per SM           warp        34.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14053.33
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3212.66
    Total L1 Elapsed Cycles          cycle       314382
    Average L2 Active Cycles         cycle      2643.67
    Total L2 Elapsed Cycles          cycle       139344
    Average SM Active Cycles         cycle      3212.66
    Total SM Elapsed Cycles          cycle       314382
    Average SMSP Active Cycles       cycle      3151.63
    Total SMSP Elapsed Cycles        cycle      1257528
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.30
    Elapsed Cycles                cycle      1314114
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        12.25
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292576.07
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.54
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203248
    Total DRAM Elapsed Cycles        cycle     58927104
    Average L1 Active Cycles         cycle   1292576.07
    Total L1 Elapsed Cycles          cycle     75130480
    Average L2 Active Cycles         cycle   1147977.33
    Total L2 Elapsed Cycles          cycle     31327416
    Average SM Active Cycles         cycle   1292576.07
    Total SM Elapsed Cycles          cycle     75130480
    Average SMSP Active Cycles       cycle   1292697.14
    Total SMSP Elapsed Cycles        cycle    300521920
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.42
    Elapsed Cycles                cycle       408708
    Memory Throughput                 %        87.30
    DRAM Throughput                   %        87.30
    Duration                         us       480.64
    L1/TEX Cache Throughput           %        47.78
    L2 Cache Throughput               %        56.66
    SM Active Cycles              cycle    387819.90
    Compute (SM) Throughput           %        30.01
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.77
    Achieved Active Warps Per SM           warp        46.93
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2619714.67
    Total DRAM Elapsed Cycles        cycle     18004992
    Average L1 Active Cycles         cycle    387819.90
    Total L1 Elapsed Cycles          cycle     22876930
    Average L2 Active Cycles         cycle    396294.67
    Total L2 Elapsed Cycles          cycle      9759624
    Average SM Active Cycles         cycle    387819.90
    Total SM Elapsed Cycles          cycle     22876930
    Average SMSP Active Cycles       cycle    389487.63
    Total SMSP Elapsed Cycles        cycle     91507720
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.26
    Elapsed Cycles                cycle      1221092
    Memory Throughput                 %        85.46
    DRAM Throughput                   %        23.56
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.34
    L2 Cache Throughput               %        14.11
    SM Active Cycles              cycle   1159633.09
    Compute (SM) Throughput           %        85.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.93
    Achieved Active Warps Per SM           warp        36.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148432
    Total DRAM Elapsed Cycles        cycle     54711296
    Average L1 Active Cycles         cycle   1159633.09
    Total L1 Elapsed Cycles          cycle     68742444
    Average L2 Active Cycles         cycle   1215270.92
    Total L2 Elapsed Cycles          cycle     29106144
    Average SM Active Cycles         cycle   1159633.09
    Total SM Elapsed Cycles          cycle     68742444
    Average SMSP Active Cycles       cycle   1159724.81
    Total SMSP Elapsed Cycles        cycle    274969776
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       780.66
    Elapsed Cycles                cycle         5325
    Memory Throughput                 %        29.11
    DRAM Throughput                   %        29.11
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        20.06
    L2 Cache Throughput               %        16.08
    SM Active Cycles              cycle      2815.97
    Compute (SM) Throughput           %        11.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12122.67
    Total DRAM Elapsed Cycles        cycle       249856
    Average L1 Active Cycles         cycle      2815.97
    Total L1 Elapsed Cycles          cycle       292206
    Average L2 Active Cycles         cycle      2419.71
    Total L2 Elapsed Cycles          cycle       132648
    Average SM Active Cycles         cycle      2815.97
    Total SM Elapsed Cycles          cycle       292206
    Average SMSP Active Cycles       cycle      2827.26
    Total SMSP Elapsed Cycles        cycle      1168824
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.518%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.89% above the average, while the minimum instance value is 4.70% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       793.09
    Elapsed Cycles                cycle         5860
    Memory Throughput                 %        31.52
    DRAM Throughput                   %        31.52
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.26
    L2 Cache Throughput               %        17.70
    SM Active Cycles              cycle      3273.33
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.48
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3273.33
    Total L1 Elapsed Cycles          cycle       306078
    Average L2 Active Cycles         cycle      2651.79
    Total L2 Elapsed Cycles          cycle       141744
    Average SM Active Cycles         cycle      3273.33
    Total SM Elapsed Cycles          cycle       306078
    Average SMSP Active Cycles       cycle      3192.48
    Total SMSP Elapsed Cycles        cycle      1224312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.785%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.33% above the average, while the minimum instance value is 9.36% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.863%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.34% above the average, while the minimum instance value is 9.76% below   
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.785%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.33% above the average, while the minimum instance value is 9.36% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       796.09
    Elapsed Cycles                cycle         5857
    Memory Throughput                 %        31.53
    DRAM Throughput                   %        31.53
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.60
    L2 Cache Throughput               %        17.81
    SM Active Cycles              cycle      3209.69
    Compute (SM) Throughput           %        10.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.93
    Achieved Active Warps Per SM           warp        35.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3209.69
    Total L1 Elapsed Cycles          cycle       303364
    Average L2 Active Cycles         cycle      2599.04
    Total L2 Elapsed Cycles          cycle       141000
    Average SM Active Cycles         cycle      3209.69
    Total SM Elapsed Cycles          cycle       303364
    Average SMSP Active Cycles       cycle      3134.86
    Total SMSP Elapsed Cycles        cycle      1213456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.141%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.58% above the average, while the minimum instance value is 7.91% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       792.19
    Elapsed Cycles                cycle         5776
    Memory Throughput                 %        31.67
    DRAM Throughput                   %        31.67
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        16.97
    L2 Cache Throughput               %        17.93
    SM Active Cycles              cycle      3329.67
    Compute (SM) Throughput           %        10.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.55
    Achieved Active Warps Per SM           warp        34.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14106.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3329.67
    Total L1 Elapsed Cycles          cycle       307456
    Average L2 Active Cycles         cycle      2512.58
    Total L2 Elapsed Cycles          cycle       139968
    Average SM Active Cycles         cycle      3329.67
    Total SM Elapsed Cycles          cycle       307456
    Average SMSP Active Cycles       cycle      3071.40
    Total SMSP Elapsed Cycles        cycle      1229824
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.17
    Elapsed Cycles                cycle      1314214
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.30
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292419.17
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1205637.33
    Total DRAM Elapsed Cycles        cycle     58788864
    Average L1 Active Cycles         cycle   1292419.17
    Total L1 Elapsed Cycles          cycle     75115782
    Average L2 Active Cycles         cycle   1172537.33
    Total L2 Elapsed Cycles          cycle     31321584
    Average SM Active Cycles         cycle   1292419.17
    Total SM Elapsed Cycles          cycle     75115782
    Average SMSP Active Cycles       cycle   1292333.66
    Total SMSP Elapsed Cycles        cycle    300463128
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.05
    Elapsed Cycles                cycle       386794
    Memory Throughput                 %        87.22
    DRAM Throughput                   %        87.22
    Duration                         us       484.29
    L1/TEX Cache Throughput           %        49.26
    L2 Cache Throughput               %        57.79
    SM Active Cycles              cycle    378540.57
    Compute (SM) Throughput           %        30.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.94
    Achieved Active Warps Per SM           warp        46.05
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2636978.67
    Total DRAM Elapsed Cycles        cycle     18140160
    Average L1 Active Cycles         cycle    378540.57
    Total L1 Elapsed Cycles          cycle     22198298
    Average L2 Active Cycles         cycle    391342.04
    Total L2 Elapsed Cycles          cycle      9586584
    Average SM Active Cycles         cycle    378540.57
    Total SM Elapsed Cycles          cycle     22198298
    Average SMSP Active Cycles       cycle    374093.49
    Total SMSP Elapsed Cycles        cycle     88793192
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.86
    Elapsed Cycles                cycle      1211880
    Memory Throughput                 %        84.84
    DRAM Throughput                   %        23.10
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.42
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1158611.52
    Compute (SM) Throughput           %        84.84
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2147296
    Total DRAM Elapsed Cycles        cycle     55785472
    Average L1 Active Cycles         cycle   1158611.52
    Total L1 Elapsed Cycles          cycle     69238202
    Average L2 Active Cycles         cycle   1204461.38
    Total L2 Elapsed Cycles          cycle     29479368
    Average SM Active Cycles         cycle   1158611.52
    Total SM Elapsed Cycles          cycle     69238202
    Average SMSP Active Cycles       cycle   1158606.05
    Total SMSP Elapsed Cycles        cycle    276952808
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.23
    Elapsed Cycles                cycle         5436
    Memory Throughput                 %        28.94
    DRAM Throughput                   %        28.94
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        18.75
    L2 Cache Throughput               %        15.90
    SM Active Cycles              cycle      3013.24
    Compute (SM) Throughput           %        11.29
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.39
    Achieved Active Warps Per SM           warp        33.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12298.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      3013.24
    Total L1 Elapsed Cycles          cycle       290348
    Average L2 Active Cycles         cycle      2455.17
    Total L2 Elapsed Cycles          cycle       134064
    Average SM Active Cycles         cycle      3013.24
    Total SM Elapsed Cycles          cycle       290348
    Average SMSP Active Cycles       cycle      2834.43
    Total SMSP Elapsed Cycles        cycle      1161392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.97
    Elapsed Cycles                cycle         5597
    Memory Throughput                 %        32.06
    DRAM Throughput                   %        32.06
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.55
    L2 Cache Throughput               %        18.23
    SM Active Cycles              cycle      3218.90
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.60
    Achieved Active Warps Per SM           warp        34.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       263168
    Average L1 Active Cycles         cycle      3218.90
    Total L1 Elapsed Cycles          cycle       310078
    Average L2 Active Cycles         cycle      2542.79
    Total L2 Elapsed Cycles          cycle       137832
    Average SM Active Cycles         cycle      3218.90
    Total SM Elapsed Cycles          cycle       310078
    Average SMSP Active Cycles       cycle      3054.53
    Total SMSP Elapsed Cycles        cycle      1240312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.54
    Elapsed Cycles                cycle         5930
    Memory Throughput                 %        31.18
    DRAM Throughput                   %        31.18
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.72
    L2 Cache Throughput               %        17.64
    SM Active Cycles              cycle      3188.81
    Compute (SM) Throughput           %        10.48
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.91
    Achieved Active Warps Per SM           warp        36.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3188.81
    Total L1 Elapsed Cycles          cycle       312820
    Average L2 Active Cycles         cycle      2603.08
    Total L2 Elapsed Cycles          cycle       142200
    Average SM Active Cycles         cycle      3188.81
    Total SM Elapsed Cycles          cycle       312820
    Average SMSP Active Cycles       cycle      3201.97
    Total SMSP Elapsed Cycles        cycle      1251280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.79%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.75% above the average, while the minimum instance value is 12.77% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       796.15
    Elapsed Cycles                cycle         5780
    Memory Throughput                 %        31.76
    DRAM Throughput                   %        31.76
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.71
    L2 Cache Throughput               %        17.99
    SM Active Cycles              cycle      3190.28
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.99
    Achieved Active Warps Per SM           warp        35.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14093.33
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3190.28
    Total L1 Elapsed Cycles          cycle       309950
    Average L2 Active Cycles         cycle      2539.67
    Total L2 Elapsed Cycles          cycle       139440
    Average SM Active Cycles         cycle      3190.28
    Total SM Elapsed Cycles          cycle       309950
    Average SMSP Active Cycles       cycle      3032.31
    Total SMSP Elapsed Cycles        cycle      1239800
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.40
    Elapsed Cycles                cycle      1298500
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.84
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.50
    SM Active Cycles              cycle   1292489.29
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204314.67
    Total DRAM Elapsed Cycles        cycle     61031424
    Average L1 Active Cycles         cycle   1292489.29
    Total L1 Elapsed Cycles          cycle     75136846
    Average L2 Active Cycles         cycle   1172975.58
    Total L2 Elapsed Cycles          cycle     32251992
    Average SM Active Cycles         cycle   1292489.29
    Total SM Elapsed Cycles          cycle     75136846
    Average SMSP Active Cycles       cycle   1292297.10
    Total SMSP Elapsed Cycles        cycle    300547384
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.47
    Elapsed Cycles                cycle       393163
    Memory Throughput                 %        87.28
    DRAM Throughput                   %        87.28
    Duration                         us       484.22
    L1/TEX Cache Throughput           %        49.30
    L2 Cache Throughput               %        57.71
    SM Active Cycles              cycle    379155.43
    Compute (SM) Throughput           %        30.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.15
    Achieved Active Warps Per SM           warp        46.15
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2638821.33
    Total DRAM Elapsed Cycles        cycle     18140160
    Average L1 Active Cycles         cycle    379155.43
    Total L1 Elapsed Cycles          cycle     22179062
    Average L2 Active Cycles         cycle    392604.25
    Total L2 Elapsed Cycles          cycle      9589200
    Average SM Active Cycles         cycle    379155.43
    Total SM Elapsed Cycles          cycle     22179062
    Average SMSP Active Cycles       cycle    379261.14
    Total SMSP Elapsed Cycles        cycle     88716248
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.91
    Elapsed Cycles                cycle      1209286
    Memory Throughput                 %        84.78
    DRAM Throughput                   %        23.16
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.52
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1157263.79
    Compute (SM) Throughput           %        84.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.05
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2149088
    Total DRAM Elapsed Cycles        cycle     55670784
    Average L1 Active Cycles         cycle   1157263.79
    Total L1 Elapsed Cycles          cycle     69290618
    Average L2 Active Cycles         cycle   1203128.71
    Total L2 Elapsed Cycles          cycle     29419368
    Average SM Active Cycles         cycle   1157263.79
    Total SM Elapsed Cycles          cycle     69290618
    Average SMSP Active Cycles       cycle   1158218.74
    Total SMSP Elapsed Cycles        cycle    277162472
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       781.74
    Elapsed Cycles                cycle         5463
    Memory Throughput                 %        28.56
    DRAM Throughput                   %        28.56
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        19.76
    L2 Cache Throughput               %        15.86
    SM Active Cycles              cycle      2859.64
    Compute (SM) Throughput           %        11.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12138.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2859.64
    Total L1 Elapsed Cycles          cycle       292652
    Average L2 Active Cycles         cycle      2523.42
    Total L2 Elapsed Cycles          cycle       134616
    Average SM Active Cycles         cycle      2859.64
    Total SM Elapsed Cycles          cycle       292652
    Average SMSP Active Cycles       cycle      2877.70
    Total SMSP Elapsed Cycles        cycle      1170608
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.33%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.10% above the average, while the minimum instance value is 24.38% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.309%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.02% above the average, while the minimum instance value is 4.49% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       788.26
    Elapsed Cycles                cycle         5817
    Memory Throughput                 %        31.04
    DRAM Throughput                   %        31.04
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        17.55
    SM Active Cycles              cycle      3220.59
    Compute (SM) Throughput           %        10.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.06
    Achieved Active Warps Per SM           warp        36.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3220.59
    Total L1 Elapsed Cycles          cycle       300944
    Average L2 Active Cycles         cycle      2580.38
    Total L2 Elapsed Cycles          cycle       142920
    Average SM Active Cycles         cycle      3220.59
    Total SM Elapsed Cycles          cycle       300944
    Average SMSP Active Cycles       cycle      3108.23
    Total SMSP Elapsed Cycles        cycle      1203776
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.317%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.88% above the average, while the minimum instance value is 14.13% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.96
    Elapsed Cycles                cycle         5731
    Memory Throughput                 %        31.47
    DRAM Throughput                   %        31.47
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        17.81
    SM Active Cycles              cycle      3220.55
    Compute (SM) Throughput           %        10.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.98
    Achieved Active Warps Per SM           warp        35.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3220.55
    Total L1 Elapsed Cycles          cycle       308002
    Average L2 Active Cycles         cycle      2545.75
    Total L2 Elapsed Cycles          cycle       140808
    Average SM Active Cycles         cycle      3220.55
    Total SM Elapsed Cycles          cycle       308002
    Average SMSP Active Cycles       cycle      3040.78
    Total SMSP Elapsed Cycles        cycle      1232008
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.66
    Elapsed Cycles                cycle         5566
    Memory Throughput                 %        32.53
    DRAM Throughput                   %        32.53
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        17.65
    L2 Cache Throughput               %        18.35
    SM Active Cycles              cycle         3201
    Compute (SM) Throughput           %        10.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.47
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14045.33
    Total DRAM Elapsed Cycles        cycle       259072
    Average L1 Active Cycles         cycle         3201
    Total L1 Elapsed Cycles          cycle       298902
    Average L2 Active Cycles         cycle      2615.25
    Total L2 Elapsed Cycles          cycle       136728
    Average SM Active Cycles         cycle         3201
    Total SM Elapsed Cycles          cycle       298902
    Average SMSP Active Cycles       cycle      3126.69
    Total SMSP Elapsed Cycles        cycle      1195608
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.041%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.31% above the average, while the minimum instance value is 10.83% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.16
    Elapsed Cycles                cycle      1313852
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        12.34
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292446.86
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203330.67
    Total DRAM Elapsed Cycles        cycle     58512384
    Average L1 Active Cycles         cycle   1292446.86
    Total L1 Elapsed Cycles          cycle     75142950
    Average L2 Active Cycles         cycle   1147340.79
    Total L2 Elapsed Cycles          cycle     31298160
    Average SM Active Cycles         cycle   1292446.86
    Total SM Elapsed Cycles          cycle     75142950
    Average SMSP Active Cycles       cycle   1292491.67
    Total SMSP Elapsed Cycles        cycle    300571800
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.44
    Elapsed Cycles                cycle       400520
    Memory Throughput                 %        87.47
    DRAM Throughput                   %        87.47
    Duration                         us       479.62
    L1/TEX Cache Throughput           %        49.09
    L2 Cache Throughput               %        57.70
    SM Active Cycles              cycle    375950.60
    Compute (SM) Throughput           %        30.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.88
    Achieved Active Warps Per SM           warp        47.46
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2618973.33
    Total DRAM Elapsed Cycles        cycle     17965056
    Average L1 Active Cycles         cycle    375950.60
    Total L1 Elapsed Cycles          cycle     22271544
    Average L2 Active Cycles         cycle    394159.88
    Total L2 Elapsed Cycles          cycle      9589728
    Average SM Active Cycles         cycle    375950.60
    Total SM Elapsed Cycles          cycle     22271544
    Average SMSP Active Cycles       cycle    377050.86
    Total SMSP Elapsed Cycles        cycle     89086176
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.56
    Elapsed Cycles                cycle      1191348
    Memory Throughput                 %        85.35
    DRAM Throughput                   %        23.04
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.41
    L2 Cache Throughput               %        13.91
    SM Active Cycles              cycle   1158685.09
    Compute (SM) Throughput           %        85.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.98
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148264
    Total DRAM Elapsed Cycles        cycle     55956480
    Average L1 Active Cycles         cycle   1158685.09
    Total L1 Elapsed Cycles          cycle     68826572
    Average L2 Active Cycles         cycle   1211623.50
    Total L2 Elapsed Cycles          cycle     29571216
    Average SM Active Cycles         cycle   1158685.09
    Total SM Elapsed Cycles          cycle     68826572
    Average SMSP Active Cycles       cycle   1158785.93
    Total SMSP Elapsed Cycles        cycle    275306288
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.19
    Elapsed Cycles                cycle         5482
    Memory Throughput                 %        28.51
    DRAM Throughput                   %        28.51
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        18.77
    L2 Cache Throughput               %        15.81
    SM Active Cycles              cycle      3009.91
    Compute (SM) Throughput           %        11.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.29
    Achieved Active Warps Per SM           warp        34.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12261.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3009.91
    Total L1 Elapsed Cycles          cycle       291832
    Average L2 Active Cycles         cycle      2543.71
    Total L2 Elapsed Cycles          cycle       134952
    Average SM Active Cycles         cycle      3009.91
    Total SM Elapsed Cycles          cycle       291832
    Average SMSP Active Cycles       cycle      2917.77
    Total SMSP Elapsed Cycles        cycle      1167328
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       780.86
    Elapsed Cycles                cycle         5655
    Memory Throughput                 %        31.68
    DRAM Throughput                   %        31.68
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.68
    L2 Cache Throughput               %        17.96
    SM Active Cycles              cycle      3195.78
    Compute (SM) Throughput           %        10.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14005.33
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3195.78
    Total L1 Elapsed Cycles          cycle       314972
    Average L2 Active Cycles         cycle      2550.58
    Total L2 Elapsed Cycles          cycle       139464
    Average SM Active Cycles         cycle      3195.78
    Total SM Elapsed Cycles          cycle       314972
    Average SMSP Active Cycles       cycle      3094.02
    Total SMSP Elapsed Cycles        cycle      1259888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.551%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.74% above the average, while the minimum instance value is 14.06% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       822.13
    Elapsed Cycles                cycle         5862
    Memory Throughput                 %        32.21
    DRAM Throughput                   %        32.21
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        17.41
    L2 Cache Throughput               %        18.02
    SM Active Cycles              cycle      3244.69
    Compute (SM) Throughput           %        10.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.07
    Achieved Active Warps Per SM           warp        36.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3244.69
    Total L1 Elapsed Cycles          cycle       316576
    Average L2 Active Cycles         cycle      2603.25
    Total L2 Elapsed Cycles          cycle       139368
    Average SM Active Cycles         cycle      3244.69
    Total SM Elapsed Cycles          cycle       316576
    Average SMSP Active Cycles       cycle      3102.67
    Total SMSP Elapsed Cycles        cycle      1266304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.352%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.17% above the average, while the minimum instance value is 15.20% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       793.84
    Elapsed Cycles                cycle         5631
    Memory Throughput                 %        32.53
    DRAM Throughput                   %        32.53
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        18.04
    L2 Cache Throughput               %        18.41
    SM Active Cycles              cycle      3131.62
    Compute (SM) Throughput           %        10.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.34
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13989.33
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3131.62
    Total L1 Elapsed Cycles          cycle       315366
    Average L2 Active Cycles         cycle      2586.79
    Total L2 Elapsed Cycles          cycle       136344
    Average SM Active Cycles         cycle      3131.62
    Total SM Elapsed Cycles          cycle       315366
    Average SMSP Active Cycles       cycle      3149.25
    Total SMSP Elapsed Cycles        cycle      1261464
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       832.37
    Elapsed Cycles                cycle      1313179
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        12.39
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292460.22
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204922.67
    Total DRAM Elapsed Cycles        cycle     58343424
    Average L1 Active Cycles         cycle   1292460.22
    Total L1 Elapsed Cycles          cycle     75129742
    Average L2 Active Cycles         cycle   1147467.58
    Total L2 Elapsed Cycles          cycle     31292568
    Average SM Active Cycles         cycle   1292460.22
    Total SM Elapsed Cycles          cycle     75129742
    Average SMSP Active Cycles       cycle   1292698.74
    Total SMSP Elapsed Cycles        cycle    300518968
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.49
    Elapsed Cycles                cycle       404696
    Memory Throughput                 %        87.26
    DRAM Throughput                   %        87.26
    Duration                         us       481.70
    L1/TEX Cache Throughput           %        47.06
    L2 Cache Throughput               %        57.14
    SM Active Cycles              cycle    393274.50
    Compute (SM) Throughput           %        29.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.48
    Achieved Active Warps Per SM           warp        45.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2624325.33
    Total DRAM Elapsed Cycles        cycle     18044928
    Average L1 Active Cycles         cycle    393274.50
    Total L1 Elapsed Cycles          cycle     23214680
    Average L2 Active Cycles         cycle    398918.38
    Total L2 Elapsed Cycles          cycle      9677832
    Average SM Active Cycles         cycle    393274.50
    Total SM Elapsed Cycles          cycle     23214680
    Average SMSP Active Cycles       cycle    391665.91
    Total SMSP Elapsed Cycles        cycle     92858720
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.51
    Elapsed Cycles                cycle      1189348
    Memory Throughput                 %        85.46
    DRAM Throughput                   %        23.07
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.42
    L2 Cache Throughput               %        13.93
    SM Active Cycles              cycle   1158617.67
    Compute (SM) Throughput           %        85.46
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.98
    Achieved Active Warps Per SM           warp        36.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148698.67
    Total DRAM Elapsed Cycles        cycle     55873536
    Average L1 Active Cycles         cycle   1158617.67
    Total L1 Elapsed Cycles          cycle     68742958
    Average L2 Active Cycles         cycle   1213317.33
    Total L2 Elapsed Cycles          cycle     29525856
    Average SM Active Cycles         cycle   1158617.67
    Total SM Elapsed Cycles          cycle     68742958
    Average SMSP Active Cycles       cycle   1157945.74
    Total SMSP Elapsed Cycles        cycle    274971832
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.29
    Elapsed Cycles                cycle         5411
    Memory Throughput                 %        28.54
    DRAM Throughput                   %        28.54
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.58
    L2 Cache Throughput               %        15.84
    SM Active Cycles              cycle      2885.97
    Compute (SM) Throughput           %        10.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.02
    Achieved Active Warps Per SM           warp        34.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12128
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2885.97
    Total L1 Elapsed Cycles          cycle       298438
    Average L2 Active Cycles         cycle      2419.54
    Total L2 Elapsed Cycles          cycle       134640
    Average SM Active Cycles         cycle      2885.97
    Total SM Elapsed Cycles          cycle       298438
    Average SMSP Active Cycles       cycle      2760.94
    Total SMSP Elapsed Cycles        cycle      1193752
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.181%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.66% above the average, while the minimum instance value is 23.54% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.782%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.72% above the average, while the minimum instance value is 4.57% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.68
    Elapsed Cycles                cycle         5708
    Memory Throughput                 %        31.69
    DRAM Throughput                   %        31.69
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        17.83
    SM Active Cycles              cycle      3138.71
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.07
    Achieved Active Warps Per SM           warp        36.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3138.71
    Total L1 Elapsed Cycles          cycle       306016
    Average L2 Active Cycles         cycle      2547.58
    Total L2 Elapsed Cycles          cycle       140760
    Average SM Active Cycles         cycle      3138.71
    Total SM Elapsed Cycles          cycle       306016
    Average SMSP Active Cycles       cycle      3052.26
    Total SMSP Elapsed Cycles        cycle      1224064
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.97
    Elapsed Cycles                cycle         5623
    Memory Throughput                 %        32.14
    DRAM Throughput                   %        32.14
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.69
    L2 Cache Throughput               %        18.19
    SM Active Cycles              cycle      3194.48
    Compute (SM) Throughput           %        10.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.75
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3194.48
    Total L1 Elapsed Cycles          cycle       303522
    Average L2 Active Cycles         cycle      2632.50
    Total L2 Elapsed Cycles          cycle       138000
    Average SM Active Cycles         cycle      3194.48
    Total SM Elapsed Cycles          cycle       303522
    Average SMSP Active Cycles       cycle      3137.30
    Total SMSP Elapsed Cycles        cycle      1214088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.904%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.85% above the average, while the minimum instance value is 11.17% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.07
    Elapsed Cycles                cycle         5666
    Memory Throughput                 %        31.66
    DRAM Throughput                   %        31.66
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.84
    L2 Cache Throughput               %        18.02
    SM Active Cycles              cycle      3167.40
    Compute (SM) Throughput           %        10.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.29
    Achieved Active Warps Per SM           warp        35.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13994.67
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3167.40
    Total L1 Elapsed Cycles          cycle       303926
    Average L2 Active Cycles         cycle      2627.79
    Total L2 Elapsed Cycles          cycle       139488
    Average SM Active Cycles         cycle      3167.40
    Total SM Elapsed Cycles          cycle       303926
    Average SMSP Active Cycles       cycle      3176.17
    Total SMSP Elapsed Cycles        cycle      1215704
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.53
    Elapsed Cycles                cycle      1312384
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292496.47
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203208
    Total DRAM Elapsed Cycles        cycle     60448768
    Average L1 Active Cycles         cycle   1292496.47
    Total L1 Elapsed Cycles          cycle     75112384
    Average L2 Active Cycles         cycle   1164056.12
    Total L2 Elapsed Cycles          cycle     31944216
    Average SM Active Cycles         cycle   1292496.47
    Total SM Elapsed Cycles          cycle     75112384
    Average SMSP Active Cycles       cycle   1292505.09
    Total SMSP Elapsed Cycles        cycle    300449536
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.33
    Elapsed Cycles                cycle       391231
    Memory Throughput                 %        87.34
    DRAM Throughput                   %        87.34
    Duration                         us       482.18
    L1/TEX Cache Throughput           %        49.27
    L2 Cache Throughput               %        57.99
    SM Active Cycles              cycle    376814.90
    Compute (SM) Throughput           %        30.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.36
    Achieved Active Warps Per SM           warp        46.25
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2629176
    Total DRAM Elapsed Cycles        cycle     18061312
    Average L1 Active Cycles         cycle    376814.90
    Total L1 Elapsed Cycles          cycle     22183346
    Average L2 Active Cycles         cycle    392067.58
    Total L2 Elapsed Cycles          cycle      9547752
    Average SM Active Cycles         cycle    376814.90
    Total SM Elapsed Cycles          cycle     22183346
    Average SMSP Active Cycles       cycle    378817.55
    Total SMSP Elapsed Cycles        cycle     88733384
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.92
    Elapsed Cycles                cycle      1211845
    Memory Throughput                 %        84.52
    DRAM Throughput                   %        23.13
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.50
    L2 Cache Throughput               %        13.95
    SM Active Cycles              cycle   1157473.52
    Compute (SM) Throughput           %        84.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.04
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2150578.67
    Total DRAM Elapsed Cycles        cycle     55790592
    Average L1 Active Cycles         cycle   1157473.52
    Total L1 Elapsed Cycles          cycle     69503606
    Average L2 Active Cycles         cycle   1205068.38
    Total L2 Elapsed Cycles          cycle     29482368
    Average SM Active Cycles         cycle   1157473.52
    Total SM Elapsed Cycles          cycle     69503606
    Average SMSP Active Cycles       cycle   1157396.90
    Total SMSP Elapsed Cycles        cycle    278014424
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.84
    Elapsed Cycles                cycle         5411
    Memory Throughput                 %        29.27
    DRAM Throughput                   %        29.27
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.39
    L2 Cache Throughput               %        16.04
    SM Active Cycles              cycle      2913.90
    Compute (SM) Throughput           %        11.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.66
    Achieved Active Warps Per SM           warp        34.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12290.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2913.90
    Total L1 Elapsed Cycles          cycle       291364
    Average L2 Active Cycles         cycle      2512.50
    Total L2 Elapsed Cycles          cycle       133200
    Average SM Active Cycles         cycle      2913.90
    Total SM Elapsed Cycles          cycle       291364
    Average SMSP Active Cycles       cycle      2936.53
    Total SMSP Elapsed Cycles        cycle      1165456
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       787.56
    Elapsed Cycles                cycle         5681
    Memory Throughput                 %        31.89
    DRAM Throughput                   %        31.89
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.47
    L2 Cache Throughput               %        17.92
    SM Active Cycles              cycle      3234.45
    Compute (SM) Throughput           %        10.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.48
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14042.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3234.45
    Total L1 Elapsed Cycles          cycle       311992
    Average L2 Active Cycles         cycle      2615.58
    Total L2 Elapsed Cycles          cycle       139800
    Average SM Active Cycles         cycle      3234.45
    Total SM Elapsed Cycles          cycle       311992
    Average SMSP Active Cycles       cycle      3098.61
    Total SMSP Elapsed Cycles        cycle      1247968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.77
    Elapsed Cycles                cycle         5794
    Memory Throughput                 %        31.08
    DRAM Throughput                   %        31.08
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.92
    L2 Cache Throughput               %        17.63
    SM Active Cycles              cycle      3153.59
    Compute (SM) Throughput           %        10.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.89
    Achieved Active Warps Per SM           warp        35.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14002.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3153.59
    Total L1 Elapsed Cycles          cycle       303976
    Average L2 Active Cycles         cycle      2545.50
    Total L2 Elapsed Cycles          cycle       142272
    Average SM Active Cycles         cycle      3153.59
    Total SM Elapsed Cycles          cycle       303976
    Average SMSP Active Cycles       cycle      3026.03
    Total SMSP Elapsed Cycles        cycle      1215904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.12
    Elapsed Cycles                cycle         5753
    Memory Throughput                 %        31.39
    DRAM Throughput                   %        31.39
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        18.10
    L2 Cache Throughput               %        17.74
    SM Active Cycles              cycle      3121.55
    Compute (SM) Throughput           %        10.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.18
    Achieved Active Warps Per SM           warp        37.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14034.67
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3121.55
    Total L1 Elapsed Cycles          cycle       301134
    Average L2 Active Cycles         cycle      2584.29
    Total L2 Elapsed Cycles          cycle       141384
    Average SM Active Cycles         cycle      3121.55
    Total SM Elapsed Cycles          cycle       301134
    Average SMSP Active Cycles       cycle      3096.64
    Total SMSP Elapsed Cycles        cycle      1204536
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.51
    Elapsed Cycles                cycle      1313007
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.54
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292306.17
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203704
    Total DRAM Elapsed Cycles        cycle     60433408
    Average L1 Active Cycles         cycle   1292306.17
    Total L1 Elapsed Cycles          cycle     75130532
    Average L2 Active Cycles         cycle   1164763.67
    Total L2 Elapsed Cycles          cycle     31936752
    Average SM Active Cycles         cycle   1292306.17
    Total SM Elapsed Cycles          cycle     75130532
    Average SMSP Active Cycles       cycle   1292164.10
    Total SMSP Elapsed Cycles        cycle    300522128
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.29
    Elapsed Cycles                cycle       389980
    Memory Throughput                 %        87.28
    DRAM Throughput                   %        87.28
    Duration                         us       480.54
    L1/TEX Cache Throughput           %        49.23
    L2 Cache Throughput               %        58.16
    SM Active Cycles              cycle    379004.83
    Compute (SM) Throughput           %        30.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.77
    Achieved Active Warps Per SM           warp        45.97
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2618669.33
    Total DRAM Elapsed Cycles        cycle     18000896
    Average L1 Active Cycles         cycle    379004.83
    Total L1 Elapsed Cycles          cycle     22208158
    Average L2 Active Cycles         cycle    393684.83
    Total L2 Elapsed Cycles          cycle      9514848
    Average SM Active Cycles         cycle    379004.83
    Total SM Elapsed Cycles          cycle     22208158
    Average SMSP Active Cycles       cycle    379625.69
    Total SMSP Elapsed Cycles        cycle     88832632
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.04
    Elapsed Cycles                cycle      1215622
    Memory Throughput                 %        84.59
    DRAM Throughput                   %        23.04
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        13.91
    SM Active Cycles              cycle   1158163.91
    Compute (SM) Throughput           %        84.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148018.67
    Total DRAM Elapsed Cycles        cycle     55938048
    Average L1 Active Cycles         cycle   1158163.91
    Total L1 Elapsed Cycles          cycle     69449348
    Average L2 Active Cycles         cycle   1205699.33
    Total L2 Elapsed Cycles          cycle     29560152
    Average SM Active Cycles         cycle   1158163.91
    Total SM Elapsed Cycles          cycle     69449348
    Average SMSP Active Cycles       cycle   1157970.41
    Total SMSP Elapsed Cycles        cycle    277797392
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       811.90
    Elapsed Cycles                cycle         5530
    Memory Throughput                 %        29.09
    DRAM Throughput                   %        29.09
    Duration                         us         6.72
    L1/TEX Cache Throughput           %        19.54
    L2 Cache Throughput               %        16.20
    SM Active Cycles              cycle      2891.14
    Compute (SM) Throughput           %        11.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.06
    Achieved Active Warps Per SM           warp        35.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12064
    Total DRAM Elapsed Cycles        cycle       248832
    Average L1 Active Cycles         cycle      2891.14
    Total L1 Elapsed Cycles          cycle       292924
    Average L2 Active Cycles         cycle      2417.33
    Total L2 Elapsed Cycles          cycle       131640
    Average SM Active Cycles         cycle      2891.14
    Total SM Elapsed Cycles          cycle       292924
    Average SMSP Active Cycles       cycle      2883.33
    Total SMSP Elapsed Cycles        cycle      1171696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.778%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.12% above the average, while the minimum instance value is 26.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.678%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.15% above the average, while the minimum instance value is 5.56% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       803.92
    Elapsed Cycles                cycle         5812
    Memory Throughput                 %        31.90
    DRAM Throughput                   %        31.90
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.78
    L2 Cache Throughput               %        18.03
    SM Active Cycles              cycle      3177.81
    Compute (SM) Throughput           %        10.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.88
    Achieved Active Warps Per SM           warp        35.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3177.81
    Total L1 Elapsed Cycles          cycle       312892
    Average L2 Active Cycles         cycle      2661.50
    Total L2 Elapsed Cycles          cycle       139224
    Average SM Active Cycles         cycle      3177.81
    Total SM Elapsed Cycles          cycle       312892
    Average SMSP Active Cycles       cycle      3200.36
    Total SMSP Elapsed Cycles        cycle      1251568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.99
    Elapsed Cycles                cycle         5845
    Memory Throughput                 %        30.80
    DRAM Throughput                   %        30.80
    Duration                         us         7.46
    L1/TEX Cache Throughput           %        17.76
    L2 Cache Throughput               %        17.26
    SM Active Cycles              cycle      3180.67
    Compute (SM) Throughput           %        10.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.62
    Achieved Active Warps Per SM           warp        35.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14088
    Total DRAM Elapsed Cycles        cycle       274432
    Average L1 Active Cycles         cycle      3180.67
    Total L1 Elapsed Cycles          cycle       301732
    Average L2 Active Cycles         cycle         2672
    Total L2 Elapsed Cycles          cycle       145536
    Average SM Active Cycles         cycle      3180.67
    Total SM Elapsed Cycles          cycle       301732
    Average SMSP Active Cycles       cycle      3144.13
    Total SMSP Elapsed Cycles        cycle      1206928
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.17
    Elapsed Cycles                cycle         5755
    Memory Throughput                 %        31.04
    DRAM Throughput                   %        31.04
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.43
    L2 Cache Throughput               %        17.52
    SM Active Cycles              cycle      3240.95
    Compute (SM) Throughput           %        10.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.93
    Achieved Active Warps Per SM           warp        35.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3240.95
    Total L1 Elapsed Cycles          cycle       306388
    Average L2 Active Cycles         cycle      2569.54
    Total L2 Elapsed Cycles          cycle       143256
    Average SM Active Cycles         cycle      3240.95
    Total SM Elapsed Cycles          cycle       306388
    Average SMSP Active Cycles       cycle      3045.01
    Total SMSP Elapsed Cycles        cycle      1225552
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.48
    Elapsed Cycles                cycle      1312789
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.94
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292621.40
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203368
    Total DRAM Elapsed Cycles        cycle     60450816
    Average L1 Active Cycles         cycle   1292621.40
    Total L1 Elapsed Cycles          cycle     75137170
    Average L2 Active Cycles         cycle   1165093.67
    Total L2 Elapsed Cycles          cycle     31945440
    Average SM Active Cycles         cycle   1292621.40
    Total SM Elapsed Cycles          cycle     75137170
    Average SMSP Active Cycles       cycle   1292509.17
    Total SMSP Elapsed Cycles        cycle    300548680
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.57
    Elapsed Cycles                cycle       389662
    Memory Throughput                 %        87.42
    DRAM Throughput                   %        87.42
    Duration                         us       480.03
    L1/TEX Cache Throughput           %        49.10
    L2 Cache Throughput               %        58.26
    SM Active Cycles              cycle    376816.26
    Compute (SM) Throughput           %        30.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.29
    Achieved Active Warps Per SM           warp        46.22
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2620026.67
    Total DRAM Elapsed Cycles        cycle     17981440
    Average L1 Active Cycles         cycle    376816.26
    Total L1 Elapsed Cycles          cycle     22264910
    Average L2 Active Cycles         cycle    391524.38
    Total L2 Elapsed Cycles          cycle      9505872
    Average SM Active Cycles         cycle    376816.26
    Total SM Elapsed Cycles          cycle     22264910
    Average SMSP Active Cycles       cycle    378042.50
    Total SMSP Elapsed Cycles        cycle     89059640
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.06
    Elapsed Cycles                cycle      1211079
    Memory Throughput                 %        84.49
    DRAM Throughput                   %        23.11
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.42
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle      1158616
    Compute (SM) Throughput           %        84.49
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2147296
    Total DRAM Elapsed Cycles        cycle     55749632
    Average L1 Active Cycles         cycle      1158616
    Total L1 Elapsed Cycles          cycle     69525566
    Average L2 Active Cycles         cycle   1205661.96
    Total L2 Elapsed Cycles          cycle     29459808
    Average SM Active Cycles         cycle      1158616
    Total SM Elapsed Cycles          cycle     69525566
    Average SMSP Active Cycles       cycle   1159441.21
    Total SMSP Elapsed Cycles        cycle    278102264
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.10
    Elapsed Cycles                cycle         5460
    Memory Throughput                 %        28.96
    DRAM Throughput                   %        28.96
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.69
    L2 Cache Throughput               %        15.88
    SM Active Cycles              cycle      2868.69
    Compute (SM) Throughput           %        11.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.99
    Achieved Active Warps Per SM           warp        35.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12306.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2868.69
    Total L1 Elapsed Cycles          cycle       291552
    Average L2 Active Cycles         cycle      2455.25
    Total L2 Elapsed Cycles          cycle       134304
    Average SM Active Cycles         cycle      2868.69
    Total SM Elapsed Cycles          cycle       291552
    Average SMSP Active Cycles       cycle      2839.72
    Total SMSP Elapsed Cycles        cycle      1166208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.08%                                                                                           
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.90% above the average, while the minimum instance value is 18.05% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.402%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.56% above the average, while the minimum instance value is 23.41% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.08%                                                                                           
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.90% above the average, while the minimum instance value is 18.05% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       819.50
    Elapsed Cycles                cycle         5975
    Memory Throughput                 %        31.94
    DRAM Throughput                   %        31.94
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        16.86
    L2 Cache Throughput               %        17.66
    SM Active Cycles              cycle      3350.69
    Compute (SM) Throughput           %        10.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14064
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3350.69
    Total L1 Elapsed Cycles          cycle       314114
    Average L2 Active Cycles         cycle      2646.08
    Total L2 Elapsed Cycles          cycle       142128
    Average SM Active Cycles         cycle      3350.69
    Total SM Elapsed Cycles          cycle       314114
    Average SMSP Active Cycles       cycle      3276.40
    Total SMSP Elapsed Cycles        cycle      1256456
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       797.67
    Elapsed Cycles                cycle         5764
    Memory Throughput                 %        31.98
    DRAM Throughput                   %        31.98
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        18.03
    SM Active Cycles              cycle      3221.55
    Compute (SM) Throughput           %        10.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.63
    Achieved Active Warps Per SM           warp        35.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14080
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3221.55
    Total L1 Elapsed Cycles          cycle       311504
    Average L2 Active Cycles         cycle         2660
    Total L2 Elapsed Cycles          cycle       139176
    Average SM Active Cycles         cycle      3221.55
    Total SM Elapsed Cycles          cycle       311504
    Average SMSP Active Cycles       cycle      3192.53
    Total SMSP Elapsed Cycles        cycle      1246016
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       780.67
    Elapsed Cycles                cycle         5708
    Memory Throughput                 %        31.16
    DRAM Throughput                   %        31.16
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.82
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3171.03
    Compute (SM) Throughput           %        10.56
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3171.03
    Total L1 Elapsed Cycles          cycle       310432
    Average L2 Active Cycles         cycle      2572.33
    Total L2 Elapsed Cycles          cycle       141864
    Average SM Active Cycles         cycle      3171.03
    Total SM Elapsed Cycles          cycle       310432
    Average SMSP Active Cycles       cycle      3042.47
    Total SMSP Elapsed Cycles        cycle      1241728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.357%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.42% above the average, while the minimum instance value is 11.06% below   
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.81
    Elapsed Cycles                cycle      1312608
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle      1292445
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203677.33
    Total DRAM Elapsed Cycles        cycle     60420096
    Average L1 Active Cycles         cycle      1292445
    Total L1 Elapsed Cycles          cycle     75125252
    Average L2 Active Cycles         cycle   1166285.04
    Total L2 Elapsed Cycles          cycle     31928304
    Average SM Active Cycles         cycle      1292445
    Total SM Elapsed Cycles          cycle     75125252
    Average SMSP Active Cycles       cycle   1292412.52
    Total SMSP Elapsed Cycles        cycle    300501008
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.50
    Elapsed Cycles                cycle       388811
    Memory Throughput                 %        87.66
    DRAM Throughput                   %        87.66
    Duration                         us       478.85
    L1/TEX Cache Throughput           %        49.22
    L2 Cache Throughput               %        58.38
    SM Active Cycles              cycle    382808.16
    Compute (SM) Throughput           %        30.86
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.68
    Achieved Active Warps Per SM           warp        45.45
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2620600
    Total DRAM Elapsed Cycles        cycle     17937408
    Average L1 Active Cycles         cycle    382808.16
    Total L1 Elapsed Cycles          cycle     22248084
    Average L2 Active Cycles         cycle    393376.83
    Total L2 Elapsed Cycles          cycle      9481824
    Average SM Active Cycles         cycle    382808.16
    Total SM Elapsed Cycles          cycle     22248084
    Average SMSP Active Cycles       cycle    380667.10
    Total SMSP Elapsed Cycles        cycle     88992336
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.17
    Elapsed Cycles                cycle      1211881
    Memory Throughput                 %        84.80
    DRAM Throughput                   %        23.11
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.49
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle   1157641.48
    Compute (SM) Throughput           %        84.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148480
    Total DRAM Elapsed Cycles        cycle     55781376
    Average L1 Active Cycles         cycle   1157641.48
    Total L1 Elapsed Cycles          cycle     69273002
    Average L2 Active Cycles         cycle   1205677.92
    Total L2 Elapsed Cycles          cycle     29476800
    Average SM Active Cycles         cycle   1157641.48
    Total SM Elapsed Cycles          cycle     69273002
    Average SMSP Active Cycles       cycle   1158346.33
    Total SMSP Elapsed Cycles        cycle    277092008
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.36
    Elapsed Cycles                cycle         5382
    Memory Throughput                 %        28.81
    DRAM Throughput                   %        28.81
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        20.01
    L2 Cache Throughput               %        16.13
    SM Active Cycles              cycle      2823.10
    Compute (SM) Throughput           %        11.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.92
    Achieved Active Warps Per SM           warp        35.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12096
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2823.10
    Total L1 Elapsed Cycles          cycle       294828
    Average L2 Active Cycles         cycle      2490.58
    Total L2 Elapsed Cycles          cycle       132120
    Average SM Active Cycles         cycle      2823.10
    Total SM Elapsed Cycles          cycle       294828
    Average SMSP Active Cycles       cycle      2825.93
    Total SMSP Elapsed Cycles        cycle      1179312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.32%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.57% above the average, while the minimum instance value is 23.56% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.587%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.56% above the average, while the minimum instance value is 4.84% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.86
    Elapsed Cycles                cycle         5872
    Memory Throughput                 %        30.95
    DRAM Throughput                   %        30.95
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        17.42
    L2 Cache Throughput               %        17.37
    SM Active Cycles              cycle      3243.88
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.80
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14101.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3243.88
    Total L1 Elapsed Cycles          cycle       305884
    Average L2 Active Cycles         cycle      2584.54
    Total L2 Elapsed Cycles          cycle       144528
    Average SM Active Cycles         cycle      3243.88
    Total SM Elapsed Cycles          cycle       305884
    Average SMSP Active Cycles       cycle      3082.94
    Total SMSP Elapsed Cycles        cycle      1223536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.011%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.57% above the average, while the minimum instance value is 14.92% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.45
    Elapsed Cycles                cycle         5728
    Memory Throughput                 %        31.37
    DRAM Throughput                   %        31.37
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.11
    L2 Cache Throughput               %        17.78
    SM Active Cycles              cycle      3302.03
    Compute (SM) Throughput           %        10.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.25
    Achieved Active Warps Per SM           warp        33.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14026.67
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3302.03
    Total L1 Elapsed Cycles          cycle       312432
    Average L2 Active Cycles         cycle      2575.71
    Total L2 Elapsed Cycles          cycle       141000
    Average SM Active Cycles         cycle      3302.03
    Total SM Elapsed Cycles          cycle       312432
    Average SMSP Active Cycles       cycle      3060.43
    Total SMSP Elapsed Cycles        cycle      1249728
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.415%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.83% above the average, while the minimum instance value is 9.90% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.644%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.93% above the average, while the minimum instance value is 10.11% below   
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.415%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.83% above the average, while the minimum instance value is 9.90% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.55
    Elapsed Cycles                cycle         5739
    Memory Throughput                 %        31.60
    DRAM Throughput                   %        31.60
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        18.23
    L2 Cache Throughput               %        17.77
    SM Active Cycles              cycle      3098.69
    Compute (SM) Throughput           %        10.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.10
    Achieved Active Warps Per SM           warp        37.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14077.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3098.69
    Total L1 Elapsed Cycles          cycle       307412
    Average L2 Active Cycles         cycle      2645.08
    Total L2 Elapsed Cycles          cycle       141096
    Average SM Active Cycles         cycle      3098.69
    Total SM Elapsed Cycles          cycle       307412
    Average SMSP Active Cycles       cycle      3139.42
    Total SMSP Elapsed Cycles        cycle      1229648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.065%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.55% above the average, while the minimum instance value is 11.38% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.63
    Elapsed Cycles                cycle      1312689
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292614.59
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204530.67
    Total DRAM Elapsed Cycles        cycle     60425216
    Average L1 Active Cycles         cycle   1292614.59
    Total L1 Elapsed Cycles          cycle     75124232
    Average L2 Active Cycles         cycle   1166077.42
    Total L2 Elapsed Cycles          cycle     31932024
    Average SM Active Cycles         cycle   1292614.59
    Total SM Elapsed Cycles          cycle     75124232
    Average SMSP Active Cycles       cycle   1292492.24
    Total SMSP Elapsed Cycles        cycle    300496928
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.51
    Elapsed Cycles                cycle       393092
    Memory Throughput                 %        87.45
    DRAM Throughput                   %        87.45
    Duration                         us       484.13
    L1/TEX Cache Throughput           %        49.05
    L2 Cache Throughput               %        57.73
    SM Active Cycles              cycle    379647.31
    Compute (SM) Throughput           %        30.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.43
    Achieved Active Warps Per SM           warp        46.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2643234.67
    Total DRAM Elapsed Cycles        cycle     18136064
    Average L1 Active Cycles         cycle    379647.31
    Total L1 Elapsed Cycles          cycle     22288218
    Average L2 Active Cycles         cycle    393147.25
    Total L2 Elapsed Cycles          cycle      9585264
    Average SM Active Cycles         cycle    379647.31
    Total SM Elapsed Cycles          cycle     22288218
    Average SMSP Active Cycles       cycle    379749.50
    Total SMSP Elapsed Cycles        cycle     89152872
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.24
    Elapsed Cycles                cycle      1214656
    Memory Throughput                 %        84.74
    DRAM Throughput                   %        23.05
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.47
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1157955.33
    Compute (SM) Throughput           %        84.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147301.33
    Total DRAM Elapsed Cycles        cycle     55885824
    Average L1 Active Cycles         cycle   1157955.33
    Total L1 Elapsed Cycles          cycle     69326236
    Average L2 Active Cycles         cycle   1203041.29
    Total L2 Elapsed Cycles          cycle     29532816
    Average SM Active Cycles         cycle   1157955.33
    Total SM Elapsed Cycles          cycle     69326236
    Average SMSP Active Cycles       cycle   1158149.14
    Total SMSP Elapsed Cycles        cycle    277304944
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.68
    Elapsed Cycles                cycle         5667
    Memory Throughput                 %        27.99
    DRAM Throughput                   %        27.99
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        19.20
    L2 Cache Throughput               %        15.32
    SM Active Cycles              cycle      2942.43
    Compute (SM) Throughput           %        11.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.75
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12322.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      2942.43
    Total L1 Elapsed Cycles          cycle       294510
    Average L2 Active Cycles         cycle      2479.88
    Total L2 Elapsed Cycles          cycle       139200
    Average SM Active Cycles         cycle      2942.43
    Total SM Elapsed Cycles          cycle       294510
    Average SMSP Active Cycles       cycle      2900.16
    Total SMSP Elapsed Cycles        cycle      1178040
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.83
    Elapsed Cycles                cycle         5742
    Memory Throughput                 %        31.60
    DRAM Throughput                   %        31.60
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.91
    L2 Cache Throughput               %        17.78
    SM Active Cycles              cycle      3153.67
    Compute (SM) Throughput           %        10.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.04
    Achieved Active Warps Per SM           warp        36.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14077.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3153.67
    Total L1 Elapsed Cycles          cycle       307912
    Average L2 Active Cycles         cycle      2619.21
    Total L2 Elapsed Cycles          cycle       141024
    Average SM Active Cycles         cycle      3153.67
    Total SM Elapsed Cycles          cycle       307912
    Average SMSP Active Cycles       cycle      3098.75
    Total SMSP Elapsed Cycles        cycle      1231648
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.09
    Elapsed Cycles                cycle         5824
    Memory Throughput                 %        30.84
    DRAM Throughput                   %        30.84
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.53
    L2 Cache Throughput               %        17.54
    SM Active Cycles              cycle      3223.41
    Compute (SM) Throughput           %        10.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14002.67
    Total DRAM Elapsed Cycles        cycle       272384
    Average L1 Active Cycles         cycle      3223.41
    Total L1 Elapsed Cycles          cycle       311556
    Average L2 Active Cycles         cycle      2628.25
    Total L2 Elapsed Cycles          cycle       143208
    Average SM Active Cycles         cycle      3223.41
    Total SM Elapsed Cycles          cycle       311556
    Average SMSP Active Cycles       cycle      3149.70
    Total SMSP Elapsed Cycles        cycle      1246224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.49%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.15% above the average, while the minimum instance value is 8.36% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.443%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.28% above the average, while the minimum instance value is 13.33% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.49%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.15% above the average, while the minimum instance value is 8.36% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.342%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.13% above the average, while the minimum instance value is 5.56% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       785.14
    Elapsed Cycles                cycle         5615
    Memory Throughput                 %        32.32
    DRAM Throughput                   %        32.32
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.73
    L2 Cache Throughput               %        18.17
    SM Active Cycles              cycle      3186.36
    Compute (SM) Throughput           %        10.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14066.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3186.36
    Total L1 Elapsed Cycles          cycle       301334
    Average L2 Active Cycles         cycle      2538.42
    Total L2 Elapsed Cycles          cycle       138120
    Average SM Active Cycles         cycle      3186.36
    Total SM Elapsed Cycles          cycle       301334
    Average SMSP Active Cycles       cycle      3044.61
    Total SMSP Elapsed Cycles        cycle      1205336
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.68
    Elapsed Cycles                cycle      1312171
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.64
    SM Active Cycles              cycle   1292548.95
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203386.67
    Total DRAM Elapsed Cycles        cycle     60432384
    Average L1 Active Cycles         cycle   1292548.95
    Total L1 Elapsed Cycles          cycle     75128902
    Average L2 Active Cycles         cycle   1164854.92
    Total L2 Elapsed Cycles          cycle     31936128
    Average SM Active Cycles         cycle   1292548.95
    Total SM Elapsed Cycles          cycle     75128902
    Average SMSP Active Cycles       cycle   1292509.41
    Total SMSP Elapsed Cycles        cycle    300515608
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.06
    Elapsed Cycles                cycle       406641
    Memory Throughput                 %        87.31
    DRAM Throughput                   %        87.31
    Duration                         us       478.72
    L1/TEX Cache Throughput           %        47.38
    L2 Cache Throughput               %        56.92
    SM Active Cycles              cycle    393067.21
    Compute (SM) Throughput           %        29.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.05
    Achieved Active Warps Per SM           warp        46.10
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2609376
    Total DRAM Elapsed Cycles        cycle     17931264
    Average L1 Active Cycles         cycle    393067.21
    Total L1 Elapsed Cycles          cycle     23062758
    Average L2 Active Cycles         cycle    396493.83
    Total L2 Elapsed Cycles          cycle      9719880
    Average SM Active Cycles         cycle    393067.21
    Total SM Elapsed Cycles          cycle     23062758
    Average SMSP Active Cycles       cycle    389319.00
    Total SMSP Elapsed Cycles        cycle     92251032
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.58
    Elapsed Cycles                cycle      1221124
    Memory Throughput                 %        84.48
    DRAM Throughput                   %        23.74
    Duration                         ms         1.45
    L1/TEX Cache Throughput           %        87.41
    L2 Cache Throughput               %        14.11
    SM Active Cycles              cycle   1158684.98
    Compute (SM) Throughput           %        84.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148869.33
    Total DRAM Elapsed Cycles        cycle     54306816
    Average L1 Active Cycles         cycle   1158684.98
    Total L1 Elapsed Cycles          cycle     69538304
    Average L2 Active Cycles         cycle   1183955.79
    Total L2 Elapsed Cycles          cycle     29117088
    Average SM Active Cycles         cycle   1158684.98
    Total SM Elapsed Cycles          cycle     69538304
    Average SMSP Active Cycles       cycle   1158521.69
    Total SMSP Elapsed Cycles        cycle    278153216
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       796.10
    Elapsed Cycles                cycle         5497
    Memory Throughput                 %        29.05
    DRAM Throughput                   %        29.05
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.52
    L2 Cache Throughput               %        16.10
    SM Active Cycles              cycle      2894.88
    Compute (SM) Throughput           %        11.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.45
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12096
    Total DRAM Elapsed Cycles        cycle       249856
    Average L1 Active Cycles         cycle      2894.88
    Total L1 Elapsed Cycles          cycle       295728
    Average L2 Active Cycles         cycle      2456.67
    Total L2 Elapsed Cycles          cycle       132576
    Average SM Active Cycles         cycle      2894.88
    Total SM Elapsed Cycles          cycle       295728
    Average SMSP Active Cycles       cycle      2866.59
    Total SMSP Elapsed Cycles        cycle      1182912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.717%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.07% above the average, while the minimum instance value is 16.65% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.445%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.69% above the average, while the minimum instance value is 25.35% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.717%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.07% above the average, while the minimum instance value is 16.65% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.245%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.04% above the average, while the minimum instance value is 4.71% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       807.46
    Elapsed Cycles                cycle         5941
    Memory Throughput                 %        31.54
    DRAM Throughput                   %        31.54
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.85
    L2 Cache Throughput               %        17.72
    SM Active Cycles              cycle      3164.50
    Compute (SM) Throughput           %        10.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.99
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3164.50
    Total L1 Elapsed Cycles          cycle       305868
    Average L2 Active Cycles         cycle      2653.88
    Total L2 Elapsed Cycles          cycle       141672
    Average SM Active Cycles         cycle      3164.50
    Total SM Elapsed Cycles          cycle       305868
    Average SMSP Active Cycles       cycle      3207.89
    Total SMSP Elapsed Cycles        cycle      1223472
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       796.42
    Elapsed Cycles                cycle         5808
    Memory Throughput                 %        31.74
    DRAM Throughput                   %        31.74
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.18
    L2 Cache Throughput               %        17.94
    SM Active Cycles              cycle      3289.28
    Compute (SM) Throughput           %        10.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.78
    Achieved Active Warps Per SM           warp        34.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3289.28
    Total L1 Elapsed Cycles          cycle       308860
    Average L2 Active Cycles         cycle      2544.08
    Total L2 Elapsed Cycles          cycle       139992
    Average SM Active Cycles         cycle      3289.28
    Total SM Elapsed Cycles          cycle       308860
    Average SMSP Active Cycles       cycle      3074.51
    Total SMSP Elapsed Cycles        cycle      1235440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.269%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.53% above the average, while the minimum instance value is 8.73% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.311%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.20% above the average, while the minimum instance value is 13.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.269%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.53% above the average, while the minimum instance value is 8.73% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.66
    Elapsed Cycles                cycle         5567
    Memory Throughput                 %        32.01
    DRAM Throughput                   %        32.01
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.67
    L2 Cache Throughput               %        18.10
    SM Active Cycles              cycle      3197.98
    Compute (SM) Throughput           %        11.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.27
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       263168
    Average L1 Active Cycles         cycle      3197.98
    Total L1 Elapsed Cycles          cycle       297948
    Average L2 Active Cycles         cycle      2597.33
    Total L2 Elapsed Cycles          cycle       138624
    Average SM Active Cycles         cycle      3197.98
    Total SM Elapsed Cycles          cycle       297948
    Average SMSP Active Cycles       cycle      3048.40
    Total SMSP Elapsed Cycles        cycle      1191792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.051%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.51% above the average, while the minimum instance value is 12.54% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.41
    Elapsed Cycles                cycle      1298655
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.84
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.48
    SM Active Cycles              cycle   1292630.26
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.54
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1204192
    Total DRAM Elapsed Cycles        cycle     61031424
    Average L1 Active Cycles         cycle   1292630.26
    Total L1 Elapsed Cycles          cycle     75118558
    Average L2 Active Cycles         cycle   1165322.67
    Total L2 Elapsed Cycles          cycle     32251656
    Average SM Active Cycles         cycle   1292630.26
    Total SM Elapsed Cycles          cycle     75118558
    Average SMSP Active Cycles       cycle   1292432.84
    Total SMSP Elapsed Cycles        cycle    300474232
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.69
    Elapsed Cycles                cycle       394474
    Memory Throughput                 %        87.27
    DRAM Throughput                   %        87.27
    Duration                         us       485.79
    L1/TEX Cache Throughput           %        49.09
    L2 Cache Throughput               %        57.53
    SM Active Cycles              cycle    379863.34
    Compute (SM) Throughput           %        30.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.31
    Achieved Active Warps Per SM           warp        46.23
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2646912
    Total DRAM Elapsed Cycles        cycle     18197504
    Average L1 Active Cycles         cycle    379863.34
    Total L1 Elapsed Cycles          cycle     22263812
    Average L2 Active Cycles         cycle    390195.17
    Total L2 Elapsed Cycles          cycle      9618840
    Average SM Active Cycles         cycle    379863.34
    Total SM Elapsed Cycles          cycle     22263812
    Average SMSP Active Cycles       cycle    376881.06
    Total SMSP Elapsed Cycles        cycle     89055248
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.35
    Elapsed Cycles                cycle      1210364
    Memory Throughput                 %        84.47
    DRAM Throughput                   %        23.15
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1158043.66
    Compute (SM) Throughput           %        84.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148450.67
    Total DRAM Elapsed Cycles        cycle     55689216
    Average L1 Active Cycles         cycle   1158043.66
    Total L1 Elapsed Cycles          cycle     69542094
    Average L2 Active Cycles         cycle   1204265.83
    Total L2 Elapsed Cycles          cycle     29428392
    Average SM Active Cycles         cycle   1158043.66
    Total SM Elapsed Cycles          cycle     69542094
    Average SMSP Active Cycles       cycle   1158540.76
    Total SMSP Elapsed Cycles        cycle    278168376
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       798.77
    Elapsed Cycles                cycle         5544
    Memory Throughput                 %        29.29
    DRAM Throughput                   %        29.29
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.19
    L2 Cache Throughput               %        15.99
    SM Active Cycles              cycle      2943.88
    Compute (SM) Throughput           %        11.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.63
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12296
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2943.88
    Total L1 Elapsed Cycles          cycle       291898
    Average L2 Active Cycles         cycle      2438.33
    Total L2 Elapsed Cycles          cycle       133392
    Average SM Active Cycles         cycle      2943.88
    Total SM Elapsed Cycles          cycle       291898
    Average SMSP Active Cycles       cycle      2862.69
    Total SMSP Elapsed Cycles        cycle      1167592
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.802%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.92% above the average, while the minimum instance value is 19.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.802%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.92% above the average, while the minimum instance value is 19.22% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       796.46
    Elapsed Cycles                cycle         5837
    Memory Throughput                 %        31.66
    DRAM Throughput                   %        31.66
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        16.94
    L2 Cache Throughput               %        17.81
    SM Active Cycles              cycle      3335.16
    Compute (SM) Throughput           %        10.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.39
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3335.16
    Total L1 Elapsed Cycles          cycle       320200
    Average L2 Active Cycles         cycle      2666.42
    Total L2 Elapsed Cycles          cycle       141000
    Average SM Active Cycles         cycle      3335.16
    Total SM Elapsed Cycles          cycle       320200
    Average SMSP Active Cycles       cycle      3213.53
    Total SMSP Elapsed Cycles        cycle      1280800
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       782.48
    Elapsed Cycles                cycle         5846
    Memory Throughput                 %        31.16
    DRAM Throughput                   %        31.16
    Duration                         us         7.46
    L1/TEX Cache Throughput           %        17.51
    L2 Cache Throughput               %        17.25
    SM Active Cycles              cycle      3226.72
    Compute (SM) Throughput           %        10.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.95
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14200
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3226.72
    Total L1 Elapsed Cycles          cycle       312976
    Average L2 Active Cycles         cycle      2618.46
    Total L2 Elapsed Cycles          cycle       145464
    Average SM Active Cycles         cycle      3226.72
    Total SM Elapsed Cycles          cycle       312976
    Average SMSP Active Cycles       cycle      3122.99
    Total SMSP Elapsed Cycles        cycle      1251904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.41
    Elapsed Cycles                cycle         5666
    Memory Throughput                 %        31.62
    DRAM Throughput                   %        31.62
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.40
    L2 Cache Throughput               %        17.79
    SM Active Cycles              cycle      3247.72
    Compute (SM) Throughput           %        10.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14085.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3247.72
    Total L1 Elapsed Cycles          cycle       319780
    Average L2 Active Cycles         cycle      2602.17
    Total L2 Elapsed Cycles          cycle       141000
    Average SM Active Cycles         cycle      3247.72
    Total SM Elapsed Cycles          cycle       319780
    Average SMSP Active Cycles       cycle      3115.75
    Total SMSP Elapsed Cycles        cycle      1279120
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.62
    Elapsed Cycles                cycle      1313340
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        12.30
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292528.24
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203317.33
    Total DRAM Elapsed Cycles        cycle     58688512
    Average L1 Active Cycles         cycle   1292528.24
    Total L1 Elapsed Cycles          cycle     75135366
    Average L2 Active Cycles         cycle   1147237.38
    Total L2 Elapsed Cycles          cycle     31317528
    Average SM Active Cycles         cycle   1292528.24
    Total SM Elapsed Cycles          cycle     75135366
    Average SMSP Active Cycles       cycle   1292532.81
    Total SMSP Elapsed Cycles        cycle    300541464
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.13
    Elapsed Cycles                cycle       385393
    Memory Throughput                 %        87.21
    DRAM Throughput                   %        87.21
    Duration                         us       482.43
    L1/TEX Cache Throughput           %        49.38
    L2 Cache Throughput               %        57.93
    SM Active Cycles              cycle    375475.50
    Compute (SM) Throughput           %        30.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.22
    Achieved Active Warps Per SM           warp        46.19
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2626392
    Total DRAM Elapsed Cycles        cycle     18069504
    Average L1 Active Cycles         cycle    375475.50
    Total L1 Elapsed Cycles          cycle     22150468
    Average L2 Active Cycles         cycle    395871.96
    Total L2 Elapsed Cycles          cycle      9550248
    Average SM Active Cycles         cycle    375475.50
    Total SM Elapsed Cycles          cycle     22150468
    Average SMSP Active Cycles       cycle    379052.42
    Total SMSP Elapsed Cycles        cycle     88601872
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.57
    Elapsed Cycles                cycle      1193114
    Memory Throughput                 %        84.60
    DRAM Throughput                   %        23.00
    Duration                         ms         1.50
    L1/TEX Cache Throughput           %        87.51
    L2 Cache Throughput               %        13.89
    SM Active Cycles              cycle   1157366.53
    Compute (SM) Throughput           %        84.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148128
    Total DRAM Elapsed Cycles        cycle     56033280
    Average L1 Active Cycles         cycle   1157366.53
    Total L1 Elapsed Cycles          cycle     69439124
    Average L2 Active Cycles         cycle   1202761.21
    Total L2 Elapsed Cycles          cycle     29611248
    Average SM Active Cycles         cycle   1157366.53
    Total SM Elapsed Cycles          cycle     69439124
    Average SMSP Active Cycles       cycle   1157866.52
    Total SMSP Elapsed Cycles        cycle    277756496
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.51
    Elapsed Cycles                cycle         5428
    Memory Throughput                 %        28.57
    DRAM Throughput                   %        28.57
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.98
    L2 Cache Throughput               %        15.99
    SM Active Cycles              cycle      2828.09
    Compute (SM) Throughput           %        11.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.10
    Achieved Active Warps Per SM           warp        35.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12093.33
    Total DRAM Elapsed Cycles        cycle       253952
    Average L1 Active Cycles         cycle      2828.09
    Total L1 Elapsed Cycles          cycle       291588
    Average L2 Active Cycles         cycle      2424.71
    Total L2 Elapsed Cycles          cycle       133320
    Average SM Active Cycles         cycle      2828.09
    Total SM Elapsed Cycles          cycle       291588
    Average SMSP Active Cycles       cycle      2756.31
    Total SMSP Elapsed Cycles        cycle      1166352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.937%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.83% above the average, while the minimum instance value is 24.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.37%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.59% above the average, while the minimum instance value is 5.02% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       786.28
    Elapsed Cycles                cycle         5703
    Memory Throughput                 %        31.89
    DRAM Throughput                   %        31.89
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        17.89
    SM Active Cycles              cycle      3278.26
    Compute (SM) Throughput           %        10.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.23
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3278.26
    Total L1 Elapsed Cycles          cycle       304882
    Average L2 Active Cycles         cycle      2625.96
    Total L2 Elapsed Cycles          cycle       140280
    Average SM Active Cycles         cycle      3278.26
    Total SM Elapsed Cycles          cycle       304882
    Average SMSP Active Cycles       cycle      3141.12
    Total SMSP Elapsed Cycles        cycle      1219528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.539%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.27% above the average, while the minimum instance value is 10.76% below   
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       787.47
    Elapsed Cycles                cycle         5739
    Memory Throughput                 %        31.59
    DRAM Throughput                   %        31.59
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.70
    L2 Cache Throughput               %        17.79
    SM Active Cycles              cycle      3191.84
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.20
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3191.84
    Total L1 Elapsed Cycles          cycle       307740
    Average L2 Active Cycles         cycle      2637.42
    Total L2 Elapsed Cycles          cycle       140976
    Average SM Active Cycles         cycle      3191.84
    Total SM Elapsed Cycles          cycle       307740
    Average SMSP Active Cycles       cycle      3148.97
    Total SMSP Elapsed Cycles        cycle      1230960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.366%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.04% above the average, while the minimum instance value is 9.75% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       788.80
    Elapsed Cycles                cycle         5654
    Memory Throughput                 %        32.29
    DRAM Throughput                   %        32.29
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.71
    L2 Cache Throughput               %        18.11
    SM Active Cycles              cycle      3189.84
    Compute (SM) Throughput           %        10.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14050.67
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3189.84
    Total L1 Elapsed Cycles          cycle       313084
    Average L2 Active Cycles         cycle      2536.83
    Total L2 Elapsed Cycles          cycle       138648
    Average SM Active Cycles         cycle      3189.84
    Total SM Elapsed Cycles          cycle       313084
    Average SMSP Active Cycles       cycle      3053.43
    Total SMSP Elapsed Cycles        cycle      1252336
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       808.65
    Elapsed Cycles                cycle      1313116
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        12.06
    Duration                         ms         1.60
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.66
    SM Active Cycles              cycle   1292648.59
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1207048
    Total DRAM Elapsed Cycles        cycle     60063744
    Average L1 Active Cycles         cycle   1292648.59
    Total L1 Elapsed Cycles          cycle     75119660
    Average L2 Active Cycles         cycle   1147743.58
    Total L2 Elapsed Cycles          cycle     31853208
    Average SM Active Cycles         cycle   1292648.59
    Total SM Elapsed Cycles          cycle     75119660
    Average SMSP Active Cycles       cycle   1292650.59
    Total SMSP Elapsed Cycles        cycle    300478640
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       832.20
    Elapsed Cycles                cycle       405906
    Memory Throughput                 %        87.63
    DRAM Throughput                   %        87.63
    Duration                         us       481.60
    L1/TEX Cache Throughput           %        47.45
    L2 Cache Throughput               %        56.96
    SM Active Cycles              cycle    389584.55
    Compute (SM) Throughput           %        29.83
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.61
    Achieved Active Warps Per SM           warp        46.37
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2634768
    Total DRAM Elapsed Cycles        cycle     18039808
    Average L1 Active Cycles         cycle    389584.55
    Total L1 Elapsed Cycles          cycle     23014930
    Average L2 Active Cycles         cycle    397717.17
    Total L2 Elapsed Cycles          cycle      9706128
    Average SM Active Cycles         cycle    389584.55
    Total SM Elapsed Cycles          cycle     23014930
    Average SMSP Active Cycles       cycle    390597.34
    Total SMSP Elapsed Cycles        cycle     92059720
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.54
    Elapsed Cycles                cycle      1191100
    Memory Throughput                 %        85.40
    DRAM Throughput                   %        23.03
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.43
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158399.76
    Compute (SM) Throughput           %        85.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.99
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147533.33
    Total DRAM Elapsed Cycles        cycle     55940096
    Average L1 Active Cycles         cycle   1158399.76
    Total L1 Elapsed Cycles          cycle     68788608
    Average L2 Active Cycles         cycle   1212549.88
    Total L2 Elapsed Cycles          cycle     29560488
    Average SM Active Cycles         cycle   1158399.76
    Total SM Elapsed Cycles          cycle     68788608
    Average SMSP Active Cycles       cycle   1157495.03
    Total SMSP Elapsed Cycles        cycle    275154432
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       794.79
    Elapsed Cycles                cycle         5589
    Memory Throughput                 %        28.97
    DRAM Throughput                   %        28.97
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        19.15
    L2 Cache Throughput               %        15.81
    SM Active Cycles              cycle      2949.79
    Compute (SM) Throughput           %        11.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.74
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12312
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2949.79
    Total L1 Elapsed Cycles          cycle       296758
    Average L2 Active Cycles         cycle      2472.17
    Total L2 Elapsed Cycles          cycle       134760
    Average SM Active Cycles         cycle      2949.79
    Total SM Elapsed Cycles          cycle       296758
    Average SMSP Active Cycles       cycle      2912.19
    Total SMSP Elapsed Cycles        cycle      1187032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.034%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.73% above the average, while the minimum instance value is 20.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.034%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.73% above the average, while the minimum instance value is 20.84% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.24
    Elapsed Cycles                cycle         5725
    Memory Throughput                 %        32.30
    DRAM Throughput                   %        32.30
    Duration                         us         7.07
    L1/TEX Cache Throughput           %        17.20
    L2 Cache Throughput               %        18.19
    SM Active Cycles              cycle      3283.84
    Compute (SM) Throughput           %        10.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.26
    Achieved Active Warps Per SM           warp        34.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       261120
    Average L1 Active Cycles         cycle      3283.84
    Total L1 Elapsed Cycles          cycle       310752
    Average L2 Active Cycles         cycle      2627.67
    Total L2 Elapsed Cycles          cycle       137952
    Average SM Active Cycles         cycle      3283.84
    Total SM Elapsed Cycles          cycle       310752
    Average SMSP Active Cycles       cycle      3157.77
    Total SMSP Elapsed Cycles        cycle      1243008
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.02
    Elapsed Cycles                cycle         5729
    Memory Throughput                 %        31.34
    DRAM Throughput                   %        31.34
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.76
    L2 Cache Throughput               %        17.61
    SM Active Cycles              cycle      3181.05
    Compute (SM) Throughput           %        10.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.63
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14066.67
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3181.05
    Total L1 Elapsed Cycles          cycle       314198
    Average L2 Active Cycles         cycle      2642.62
    Total L2 Elapsed Cycles          cycle       142536
    Average SM Active Cycles         cycle      3181.05
    Total SM Elapsed Cycles          cycle       314198
    Average SMSP Active Cycles       cycle      3135.40
    Total SMSP Elapsed Cycles        cycle      1256792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.048%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.72% above the average, while the minimum instance value is 16.85% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.78
    Elapsed Cycles                cycle         5661
    Memory Throughput                 %        31.47
    DRAM Throughput                   %        31.47
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.54
    L2 Cache Throughput               %        17.83
    SM Active Cycles              cycle      3221.05
    Compute (SM) Throughput           %        10.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.36
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3221.05
    Total L1 Elapsed Cycles          cycle       311434
    Average L2 Active Cycles         cycle      2613.12
    Total L2 Elapsed Cycles          cycle       140856
    Average SM Active Cycles         cycle      3221.05
    Total SM Elapsed Cycles          cycle       311434
    Average SMSP Active Cycles       cycle      3110.09
    Total SMSP Elapsed Cycles        cycle      1245736
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.84
    Elapsed Cycles                cycle      1313255
    Memory Throughput                 %        82.34
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292395.55
    Compute (SM) Throughput           %        82.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.59
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203709.33
    Total DRAM Elapsed Cycles        cycle     60438528
    Average L1 Active Cycles         cycle   1292395.55
    Total L1 Elapsed Cycles          cycle     75136714
    Average L2 Active Cycles         cycle   1165674.38
    Total L2 Elapsed Cycles          cycle     31939176
    Average SM Active Cycles         cycle   1292395.55
    Total SM Elapsed Cycles          cycle     75136714
    Average SMSP Active Cycles       cycle   1292625.83
    Total SMSP Elapsed Cycles        cycle    300546856
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.72
    Elapsed Cycles                cycle       391696
    Memory Throughput                 %        87.68
    DRAM Throughput                   %        87.68
    Duration                         us       482.30
    L1/TEX Cache Throughput           %        49.30
    L2 Cache Throughput               %        57.90
    SM Active Cycles              cycle    378239.48
    Compute (SM) Throughput           %        30.96
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.33
    Achieved Active Warps Per SM           warp        46.24
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2640341.33
    Total DRAM Elapsed Cycles        cycle     18068480
    Average L1 Active Cycles         cycle    378239.48
    Total L1 Elapsed Cycles          cycle     22176374
    Average L2 Active Cycles         cycle    394339.04
    Total L2 Elapsed Cycles          cycle      9550416
    Average SM Active Cycles         cycle    378239.48
    Total SM Elapsed Cycles          cycle     22176374
    Average SMSP Active Cycles       cycle    380758.73
    Total SMSP Elapsed Cycles        cycle     88705496
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.44
    Elapsed Cycles                cycle      1211083
    Memory Throughput                 %        84.48
    DRAM Throughput                   %        23.14
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.50
    L2 Cache Throughput               %        13.97
    SM Active Cycles              cycle   1157472.93
    Compute (SM) Throughput           %        84.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.05
    Achieved Active Warps Per SM           warp        36.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148501.33
    Total DRAM Elapsed Cycles        cycle     55711744
    Average L1 Active Cycles         cycle   1157472.93
    Total L1 Elapsed Cycles          cycle     69534158
    Average L2 Active Cycles         cycle   1205244.50
    Total L2 Elapsed Cycles          cycle     29440800
    Average SM Active Cycles         cycle   1157472.93
    Total SM Elapsed Cycles          cycle     69534158
    Average SMSP Active Cycles       cycle   1157876.29
    Total SMSP Elapsed Cycles        cycle    278136632
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.45
    Elapsed Cycles                cycle         5390
    Memory Throughput                 %        28.74
    DRAM Throughput                   %        28.74
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.22
    L2 Cache Throughput               %        16.08
    SM Active Cycles              cycle      2939.09
    Compute (SM) Throughput           %        11.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.65
    Achieved Active Warps Per SM           warp        33.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12066.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2939.09
    Total L1 Elapsed Cycles          cycle       295252
    Average L2 Active Cycles         cycle      2428.17
    Total L2 Elapsed Cycles          cycle       132672
    Average SM Active Cycles         cycle      2939.09
    Total SM Elapsed Cycles          cycle       295252
    Average SMSP Active Cycles       cycle      2798.76
    Total SMSP Elapsed Cycles        cycle      1181008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.101%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.28% above the average, while the minimum instance value is 23.04% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.449%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.68% above the average, while the minimum instance value is 5.32% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.67
    Elapsed Cycles                cycle         5726
    Memory Throughput                 %        31.39
    DRAM Throughput                   %        31.39
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.43
    L2 Cache Throughput               %        17.76
    SM Active Cycles              cycle      3242.17
    Compute (SM) Throughput           %        10.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3242.17
    Total L1 Elapsed Cycles          cycle       309152
    Average L2 Active Cycles         cycle      2554.25
    Total L2 Elapsed Cycles          cycle       141192
    Average SM Active Cycles         cycle      3242.17
    Total SM Elapsed Cycles          cycle       309152
    Average SMSP Active Cycles       cycle      3055.52
    Total SMSP Elapsed Cycles        cycle      1236608
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.38%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.39% above the average, while the minimum instance value is 13.93% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.22
    Elapsed Cycles                cycle         5789
    Memory Throughput                 %        31.21
    DRAM Throughput                   %        31.21
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.90
    L2 Cache Throughput               %        17.65
    SM Active Cycles              cycle         3156
    Compute (SM) Throughput           %        10.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.72
    Achieved Active Warps Per SM           warp        35.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14061.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle         3156
    Total L1 Elapsed Cycles          cycle       311216
    Average L2 Active Cycles         cycle      2647.04
    Total L2 Elapsed Cycles          cycle       142296
    Average SM Active Cycles         cycle         3156
    Total SM Elapsed Cycles          cycle       311216
    Average SMSP Active Cycles       cycle      3137.46
    Total SMSP Elapsed Cycles        cycle      1244864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.012%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.52% above the average, while the minimum instance value is 16.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.751%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.55% above the average, while the minimum instance value is 17.67% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.012%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.52% above the average, while the minimum instance value is 16.13% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.19
    Elapsed Cycles                cycle         5698
    Memory Throughput                 %        31.72
    DRAM Throughput                   %        31.72
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.31
    L2 Cache Throughput               %        17.92
    SM Active Cycles              cycle      3264.53
    Compute (SM) Throughput           %        10.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.68
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14021.33
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3264.53
    Total L1 Elapsed Cycles          cycle       303254
    Average L2 Active Cycles         cycle      2549.71
    Total L2 Elapsed Cycles          cycle       140136
    Average SM Active Cycles         cycle      3264.53
    Total SM Elapsed Cycles          cycle       303254
    Average SMSP Active Cycles       cycle      3058.39
    Total SMSP Elapsed Cycles        cycle      1213016
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.88
    Elapsed Cycles                cycle      1312524
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.54
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292325.31
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.59
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204149.33
    Total DRAM Elapsed Cycles        cycle     60417024
    Average L1 Active Cycles         cycle   1292325.31
    Total L1 Elapsed Cycles          cycle     75115216
    Average L2 Active Cycles         cycle   1164068.50
    Total L2 Elapsed Cycles          cycle     31926984
    Average SM Active Cycles         cycle   1292325.31
    Total SM Elapsed Cycles          cycle     75115216
    Average SMSP Active Cycles       cycle   1292507.81
    Total SMSP Elapsed Cycles        cycle    300460864
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.45
    Elapsed Cycles                cycle       388559
    Memory Throughput                 %        87.62
    DRAM Throughput                   %        87.62
    Duration                         us       478.69
    L1/TEX Cache Throughput           %        48.86
    L2 Cache Throughput               %        58.35
    SM Active Cycles              cycle    378906.33
    Compute (SM) Throughput           %        30.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.46
    Achieved Active Warps Per SM           warp        45.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2618682.67
    Total DRAM Elapsed Cycles        cycle     17931264
    Average L1 Active Cycles         cycle    378906.33
    Total L1 Elapsed Cycles          cycle     22380524
    Average L2 Active Cycles         cycle    393287.42
    Total L2 Elapsed Cycles          cycle      9477888
    Average SM Active Cycles         cycle    378906.33
    Total SM Elapsed Cycles          cycle     22380524
    Average SMSP Active Cycles       cycle    379403.15
    Total SMSP Elapsed Cycles        cycle     89522096
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.44
    Elapsed Cycles                cycle      1209676
    Memory Throughput                 %        84.74
    DRAM Throughput                   %        23.15
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.52
    L2 Cache Throughput               %        13.99
    SM Active Cycles              cycle   1157267.60
    Compute (SM) Throughput           %        84.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.07
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2147992
    Total DRAM Elapsed Cycles        cycle     55659520
    Average L1 Active Cycles         cycle   1157267.60
    Total L1 Elapsed Cycles          cycle     69321764
    Average L2 Active Cycles         cycle   1206021.88
    Total L2 Elapsed Cycles          cycle     29413344
    Average SM Active Cycles         cycle   1157267.60
    Total SM Elapsed Cycles          cycle     69321764
    Average SMSP Active Cycles       cycle   1158359.02
    Total SMSP Elapsed Cycles        cycle    277287056
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       796.01
    Elapsed Cycles                cycle         5492
    Memory Throughput                 %        29.31
    DRAM Throughput                   %        29.31
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.13
    L2 Cache Throughput               %        16.04
    SM Active Cycles              cycle      2953.55
    Compute (SM) Throughput           %        11.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.00
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12306.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2953.55
    Total L1 Elapsed Cycles          cycle       290690
    Average L2 Active Cycles         cycle      2420.75
    Total L2 Elapsed Cycles          cycle       132840
    Average SM Active Cycles         cycle      2953.55
    Total SM Elapsed Cycles          cycle       290690
    Average SMSP Active Cycles       cycle      2921.85
    Total SMSP Elapsed Cycles        cycle      1162760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.2%                                                                                            
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.92% above the average, while the minimum instance value is 20.56% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.00
    Elapsed Cycles                cycle         5744
    Memory Throughput                 %        32.01
    DRAM Throughput                   %        32.01
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        18.24
    L2 Cache Throughput               %        18.16
    SM Active Cycles              cycle      3097.97
    Compute (SM) Throughput           %        10.56
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.56
    Achieved Active Warps Per SM           warp        36.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13984
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3097.97
    Total L1 Elapsed Cycles          cycle       310218
    Average L2 Active Cycles         cycle      2632.71
    Total L2 Elapsed Cycles          cycle       138192
    Average SM Active Cycles         cycle      3097.97
    Total SM Elapsed Cycles          cycle       310218
    Average SMSP Active Cycles       cycle      3136.81
    Total SMSP Elapsed Cycles        cycle      1240872
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.70
    Elapsed Cycles                cycle         5716
    Memory Throughput                 %        31.38
    DRAM Throughput                   %        31.38
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.58
    L2 Cache Throughput               %        17.65
    SM Active Cycles              cycle      3214.36
    Compute (SM) Throughput           %        10.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3214.36
    Total L1 Elapsed Cycles          cycle       311370
    Average L2 Active Cycles         cycle      2615.42
    Total L2 Elapsed Cycles          cycle       142200
    Average SM Active Cycles         cycle      3214.36
    Total SM Elapsed Cycles          cycle       311370
    Average SMSP Active Cycles       cycle      3067.95
    Total SMSP Elapsed Cycles        cycle      1245480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.055%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.34% above the average, while the minimum instance value is 15.09% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.02
    Elapsed Cycles                cycle         5611
    Memory Throughput                 %        32.01
    DRAM Throughput                   %        32.01
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.74
    L2 Cache Throughput               %        18.16
    SM Active Cycles              cycle      3184.76
    Compute (SM) Throughput           %        10.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       263168
    Average L1 Active Cycles         cycle      3184.76
    Total L1 Elapsed Cycles          cycle       307998
    Average L2 Active Cycles         cycle      2618.33
    Total L2 Elapsed Cycles          cycle       138120
    Average SM Active Cycles         cycle      3184.76
    Total SM Elapsed Cycles          cycle       307998
    Average SMSP Active Cycles       cycle      3100.18
    Total SMSP Elapsed Cycles        cycle      1231992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.49%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.40% above the average, while the minimum instance value is 15.36% below the average.      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.91
    Elapsed Cycles                cycle      1312640
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292490.95
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.58
    Achieved Active Warps Per SM           warp        46.36
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204805.33
    Total DRAM Elapsed Cycles        cycle     60421120
    Average L1 Active Cycles         cycle   1292490.95
    Total L1 Elapsed Cycles          cycle     75139852
    Average L2 Active Cycles         cycle   1164982.83
    Total L2 Elapsed Cycles          cycle     31930080
    Average SM Active Cycles         cycle   1292490.95
    Total SM Elapsed Cycles          cycle     75139852
    Average SMSP Active Cycles       cycle   1292428.41
    Total SMSP Elapsed Cycles        cycle    300559408
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.77
    Elapsed Cycles                cycle       392399
    Memory Throughput                 %        87.03
    DRAM Throughput                   %        87.03
    Duration                         us       483.17
    L1/TEX Cache Throughput           %        48.91
    L2 Cache Throughput               %        57.92
    SM Active Cycles              cycle    378363.28
    Compute (SM) Throughput           %        30.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.50
    Achieved Active Warps Per SM           warp        46.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2625434.67
    Total DRAM Elapsed Cycles        cycle     18100224
    Average L1 Active Cycles         cycle    378363.28
    Total L1 Elapsed Cycles          cycle     22354318
    Average L2 Active Cycles         cycle    390129.25
    Total L2 Elapsed Cycles          cycle      9566664
    Average SM Active Cycles         cycle    378363.28
    Total SM Elapsed Cycles          cycle     22354318
    Average SMSP Active Cycles       cycle    376344.85
    Total SMSP Elapsed Cycles        cycle     89417272
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.36
    Elapsed Cycles                cycle      1215626
    Memory Throughput                 %        84.80
    DRAM Throughput                   %        23.05
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.44
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158383.02
    Compute (SM) Throughput           %        84.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148506.67
    Total DRAM Elapsed Cycles        cycle     55918592
    Average L1 Active Cycles         cycle   1158383.02
    Total L1 Elapsed Cycles          cycle     69272014
    Average L2 Active Cycles         cycle   1203103.46
    Total L2 Elapsed Cycles          cycle     29550000
    Average SM Active Cycles         cycle   1158383.02
    Total SM Elapsed Cycles          cycle     69272014
    Average SMSP Active Cycles       cycle   1157777.52
    Total SMSP Elapsed Cycles        cycle    277088056
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.18
    Elapsed Cycles                cycle         5392
    Memory Throughput                 %        28.79
    DRAM Throughput                   %        28.79
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.90
    L2 Cache Throughput               %        16.05
    SM Active Cycles              cycle      2839.17
    Compute (SM) Throughput           %        11.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.18
    Achieved Active Warps Per SM           warp        35.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12088
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2839.17
    Total L1 Elapsed Cycles          cycle       295958
    Average L2 Active Cycles         cycle      2395.38
    Total L2 Elapsed Cycles          cycle       132768
    Average SM Active Cycles         cycle      2839.17
    Total SM Elapsed Cycles          cycle       295958
    Average SMSP Active Cycles       cycle      2764.95
    Total SMSP Elapsed Cycles        cycle      1183832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.572%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 15.18% above the average, while the minimum instance value is 4.44% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       787.13
    Elapsed Cycles                cycle         5630
    Memory Throughput                 %        32.03
    DRAM Throughput                   %        32.03
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.89
    L2 Cache Throughput               %        18.11
    SM Active Cycles              cycle      3158.47
    Compute (SM) Throughput           %        10.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.53
    Achieved Active Warps Per SM           warp        35.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       263168
    Average L1 Active Cycles         cycle      3158.47
    Total L1 Elapsed Cycles          cycle       302822
    Average L2 Active Cycles         cycle      2604.04
    Total L2 Elapsed Cycles          cycle       138432
    Average SM Active Cycles         cycle      3158.47
    Total SM Elapsed Cycles          cycle       302822
    Average SMSP Active Cycles       cycle      3115.72
    Total SMSP Elapsed Cycles        cycle      1211288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.118%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.58% above the average, while the minimum instance value is 16.84% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.89
    Elapsed Cycles                cycle         5812
    Memory Throughput                 %        31.16
    DRAM Throughput                   %        31.16
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.77
    L2 Cache Throughput               %        17.53
    SM Active Cycles              cycle      3179.74
    Compute (SM) Throughput           %        10.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.47
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14040
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3179.74
    Total L1 Elapsed Cycles          cycle       307466
    Average L2 Active Cycles         cycle      2579.92
    Total L2 Elapsed Cycles          cycle       143136
    Average SM Active Cycles         cycle      3179.74
    Total SM Elapsed Cycles          cycle       307466
    Average SMSP Active Cycles       cycle      3075.64
    Total SMSP Elapsed Cycles        cycle      1229864
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.706%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.51% above the average, while the minimum instance value is 9.65% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.496%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.92% above the average, while the minimum instance value is 19.04% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.706%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.51% above the average, while the minimum instance value is 9.65% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.77
    Elapsed Cycles                cycle         5793
    Memory Throughput                 %        31.09
    DRAM Throughput                   %        31.09
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.41
    L2 Cache Throughput               %        17.65
    SM Active Cycles              cycle      3245.16
    Compute (SM) Throughput           %        10.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.46
    Achieved Active Warps Per SM           warp        35.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14008
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3245.16
    Total L1 Elapsed Cycles          cycle       301912
    Average L2 Active Cycles         cycle      2559.25
    Total L2 Elapsed Cycles          cycle       142128
    Average SM Active Cycles         cycle      3245.16
    Total SM Elapsed Cycles          cycle       301912
    Average SMSP Active Cycles       cycle      3076.57
    Total SMSP Elapsed Cycles        cycle      1207648
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.97
    Elapsed Cycles                cycle      1311956
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.61
    SM Active Cycles              cycle   1292555.86
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1202989.33
    Total DRAM Elapsed Cycles        cycle     60410880
    Average L1 Active Cycles         cycle   1292555.86
    Total L1 Elapsed Cycles          cycle     75127740
    Average L2 Active Cycles         cycle   1164308.75
    Total L2 Elapsed Cycles          cycle     31924416
    Average SM Active Cycles         cycle   1292555.86
    Total SM Elapsed Cycles          cycle     75127740
    Average SMSP Active Cycles       cycle   1292482.83
    Total SMSP Elapsed Cycles        cycle    300510960
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.51
    Elapsed Cycles                cycle       389120
    Memory Throughput                 %        87.46
    DRAM Throughput                   %        87.46
    Duration                         us       479.36
    L1/TEX Cache Throughput           %        48.92
    L2 Cache Throughput               %        58.28
    SM Active Cycles              cycle    377077.14
    Compute (SM) Throughput           %        30.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.92
    Achieved Active Warps Per SM           warp        46.04
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2617778.67
    Total DRAM Elapsed Cycles        cycle     17958912
    Average L1 Active Cycles         cycle    377077.14
    Total L1 Elapsed Cycles          cycle     22361860
    Average L2 Active Cycles         cycle       389740
    Total L2 Elapsed Cycles          cycle      9491904
    Average SM Active Cycles         cycle    377077.14
    Total SM Elapsed Cycles          cycle     22361860
    Average SMSP Active Cycles       cycle    376250.04
    Total SMSP Elapsed Cycles        cycle     89447440
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.38
    Elapsed Cycles                cycle      1210609
    Memory Throughput                 %        84.50
    DRAM Throughput                   %        23.15
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.48
    L2 Cache Throughput               %        13.97
    SM Active Cycles              cycle   1157762.62
    Compute (SM) Throughput           %        84.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148944
    Total DRAM Elapsed Cycles        cycle     55704576
    Average L1 Active Cycles         cycle   1157762.62
    Total L1 Elapsed Cycles          cycle     69524302
    Average L2 Active Cycles         cycle   1201254.08
    Total L2 Elapsed Cycles          cycle     29437608
    Average SM Active Cycles         cycle   1157762.62
    Total SM Elapsed Cycles          cycle     69524302
    Average SMSP Active Cycles       cycle   1158250.52
    Total SMSP Elapsed Cycles        cycle    278097208
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.87
    Elapsed Cycles                cycle         5414
    Memory Throughput                 %        29.34
    DRAM Throughput                   %        29.34
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        18.91
    L2 Cache Throughput               %        16.04
    SM Active Cycles              cycle      2987.52
    Compute (SM) Throughput           %        11.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.15
    Achieved Active Warps Per SM           warp        33.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12320
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2987.52
    Total L1 Elapsed Cycles          cycle       288648
    Average L2 Active Cycles         cycle      2477.12
    Total L2 Elapsed Cycles          cycle       132984
    Average SM Active Cycles         cycle      2987.52
    Total SM Elapsed Cycles          cycle       288648
    Average SMSP Active Cycles       cycle      2895.45
    Total SMSP Elapsed Cycles        cycle      1154592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       789.56
    Elapsed Cycles                cycle         5781
    Memory Throughput                 %        31.23
    DRAM Throughput                   %        31.23
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.56
    L2 Cache Throughput               %        17.67
    SM Active Cycles              cycle      3217.10
    Compute (SM) Throughput           %        10.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.76
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       269312
    Average L1 Active Cycles         cycle      3217.10
    Total L1 Elapsed Cycles          cycle       311672
    Average L2 Active Cycles         cycle      2624.50
    Total L2 Elapsed Cycles          cycle       141888
    Average SM Active Cycles         cycle      3217.10
    Total SM Elapsed Cycles          cycle       311672
    Average SMSP Active Cycles       cycle      3141.15
    Total SMSP Elapsed Cycles        cycle      1246688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.353%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.94% above the average, while the minimum instance value is 9.86% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.248%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.98% above the average, while the minimum instance value is 14.84% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.353%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.94% above the average, while the minimum instance value is 9.86% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       790.27
    Elapsed Cycles                cycle         5911
    Memory Throughput                 %        30.59
    DRAM Throughput                   %        30.59
    Duration                         us         7.42
    L1/TEX Cache Throughput           %        17.23
    L2 Cache Throughput               %        17.28
    SM Active Cycles              cycle      3279.34
    Compute (SM) Throughput           %        10.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13992
    Total DRAM Elapsed Cycles        cycle       274432
    Average L1 Active Cycles         cycle      3279.34
    Total L1 Elapsed Cycles          cycle       318240
    Average L2 Active Cycles         cycle      2557.17
    Total L2 Elapsed Cycles          cycle       145104
    Average SM Active Cycles         cycle      3279.34
    Total SM Elapsed Cycles          cycle       318240
    Average SMSP Active Cycles       cycle      3054.58
    Total SMSP Elapsed Cycles        cycle      1272960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.278%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.83% above the average, while the minimum instance value is 9.37% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.905%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.61% above the average, while the minimum instance value is 17.30% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.278%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.83% above the average, while the minimum instance value is 9.37% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.95
    Elapsed Cycles                cycle         5830
    Memory Throughput                 %        31.14
    DRAM Throughput                   %        31.14
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.48
    L2 Cache Throughput               %        17.54
    SM Active Cycles              cycle      3231.38
    Compute (SM) Throughput           %        10.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14082.67
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3231.38
    Total L1 Elapsed Cycles          cycle       309766
    Average L2 Active Cycles         cycle      2652.79
    Total L2 Elapsed Cycles          cycle       143112
    Average SM Active Cycles         cycle      3231.38
    Total SM Elapsed Cycles          cycle       309766
    Average SMSP Active Cycles       cycle      3176.08
    Total SMSP Elapsed Cycles        cycle      1239064
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.13
    Elapsed Cycles                cycle      1312388
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292552.78
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203677.33
    Total DRAM Elapsed Cycles        cycle     60383232
    Average L1 Active Cycles         cycle   1292552.78
    Total L1 Elapsed Cycles          cycle     75129906
    Average L2 Active Cycles         cycle      1147229
    Total L2 Elapsed Cycles          cycle     31909296
    Average SM Active Cycles         cycle   1292552.78
    Total SM Elapsed Cycles          cycle     75129906
    Average SMSP Active Cycles       cycle   1292322.66
    Total SMSP Elapsed Cycles        cycle    300519624
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       837.22
    Elapsed Cycles                cycle       409831
    Memory Throughput                 %        87.31
    DRAM Throughput                   %        87.31
    Duration                         us       483.30
    L1/TEX Cache Throughput           %        47.93
    L2 Cache Throughput               %        56.51
    SM Active Cycles              cycle    391076.62
    Compute (SM) Throughput           %        30.11
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.14
    Achieved Active Warps Per SM           warp        46.63
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2634544
    Total DRAM Elapsed Cycles        cycle     18105344
    Average L1 Active Cycles         cycle    391076.62
    Total L1 Elapsed Cycles          cycle     22801690
    Average L2 Active Cycles         cycle    399457.29
    Total L2 Elapsed Cycles          cycle      9791664
    Average SM Active Cycles         cycle    391076.62
    Total SM Elapsed Cycles          cycle     22801690
    Average SMSP Active Cycles       cycle    392529.65
    Total SMSP Elapsed Cycles        cycle     91206760
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.61
    Elapsed Cycles                cycle      1190642
    Memory Throughput                 %        85.35
    DRAM Throughput                   %        23.06
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.29
    L2 Cache Throughput               %        13.93
    SM Active Cycles              cycle   1160296.95
    Compute (SM) Throughput           %        85.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.86
    Achieved Active Warps Per SM           warp        36.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148538.67
    Total DRAM Elapsed Cycles        cycle     55891968
    Average L1 Active Cycles         cycle   1160296.95
    Total L1 Elapsed Cycles          cycle     68829822
    Average L2 Active Cycles         cycle   1213343.92
    Total L2 Elapsed Cycles          cycle     29536200
    Average SM Active Cycles         cycle   1160296.95
    Total SM Elapsed Cycles          cycle     68829822
    Average SMSP Active Cycles       cycle   1157523.40
    Total SMSP Elapsed Cycles        cycle    275319288
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.32
    Elapsed Cycles                cycle         5408
    Memory Throughput                 %        28.80
    DRAM Throughput                   %        28.80
    Duration                         us         6.82
    L1/TEX Cache Throughput           %        19.86
    L2 Cache Throughput               %        16.05
    SM Active Cycles              cycle      2845.28
    Compute (SM) Throughput           %        11.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.83
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12093.33
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2845.28
    Total L1 Elapsed Cycles          cycle       295084
    Average L2 Active Cycles         cycle      2382.88
    Total L2 Elapsed Cycles          cycle       132840
    Average SM Active Cycles         cycle      2845.28
    Total SM Elapsed Cycles          cycle       295084
    Average SMSP Active Cycles       cycle      2777.58
    Total SMSP Elapsed Cycles        cycle      1180336
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.361%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.78% above the average, while the minimum instance value is 4.02% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.06
    Elapsed Cycles                cycle         5787
    Memory Throughput                 %        31.27
    DRAM Throughput                   %        31.27
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.62
    L2 Cache Throughput               %        17.59
    SM Active Cycles              cycle      3206.12
    Compute (SM) Throughput           %        10.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.87
    Achieved Active Warps Per SM           warp        34.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14090.67
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3206.12
    Total L1 Elapsed Cycles          cycle       310978
    Average L2 Active Cycles         cycle      2556.71
    Total L2 Elapsed Cycles          cycle       142464
    Average SM Active Cycles         cycle      3206.12
    Total SM Elapsed Cycles          cycle       310978
    Average SMSP Active Cycles       cycle      3065.22
    Total SMSP Elapsed Cycles        cycle      1243912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.169%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.79% above the average, while the minimum instance value is 14.04% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.94
    Elapsed Cycles                cycle         5714
    Memory Throughput                 %        31.54
    DRAM Throughput                   %        31.54
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.34
    L2 Cache Throughput               %        17.91
    SM Active Cycles              cycle      3257.72
    Compute (SM) Throughput           %        10.41
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.67
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14050.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3257.72
    Total L1 Elapsed Cycles          cycle       314876
    Average L2 Active Cycles         cycle      2562.46
    Total L2 Elapsed Cycles          cycle       140304
    Average SM Active Cycles         cycle      3257.72
    Total SM Elapsed Cycles          cycle       314876
    Average SMSP Active Cycles       cycle      3143.88
    Total SMSP Elapsed Cycles        cycle      1259504
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       789.40
    Elapsed Cycles                cycle         5856
    Memory Throughput                 %        30.90
    DRAM Throughput                   %        30.90
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.97
    L2 Cache Throughput               %        17.46
    SM Active Cycles              cycle      3143.95
    Compute (SM) Throughput           %        10.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.96
    Achieved Active Warps Per SM           warp        36.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14082.67
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3143.95
    Total L1 Elapsed Cycles          cycle       309776
    Average L2 Active Cycles         cycle      2563.08
    Total L2 Elapsed Cycles          cycle       143712
    Average SM Active Cycles         cycle      3143.95
    Total SM Elapsed Cycles          cycle       309776
    Average SMSP Active Cycles       cycle      3091.02
    Total SMSP Elapsed Cycles        cycle      1239104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.128%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.71% above the average, while the minimum instance value is 9.00% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.067%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.48% above the average, while the minimum instance value is 13.26% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.128%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.71% above the average, while the minimum instance value is 9.00% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.75
    Elapsed Cycles                cycle      1313446
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        12.44
    Duration                         ms         1.55
    L1/TEX Cache Throughput           %        82.51
    L2 Cache Throughput               %        14.92
    SM Active Cycles              cycle   1292724.50
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.53
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203317.33
    Total DRAM Elapsed Cycles        cycle     58045440
    Average L1 Active Cycles         cycle   1292724.50
    Total L1 Elapsed Cycles          cycle     75146612
    Average L2 Active Cycles         cycle      1146771
    Total L2 Elapsed Cycles          cycle     31283856
    Average SM Active Cycles         cycle   1292724.50
    Total SM Elapsed Cycles          cycle     75146612
    Average SMSP Active Cycles       cycle   1292497.97
    Total SMSP Elapsed Cycles        cycle    300586448
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       832.57
    Elapsed Cycles                cycle       406791
    Memory Throughput                 %        87.37
    DRAM Throughput                   %        87.37
    Duration                         us       482.40
    L1/TEX Cache Throughput           %        47.56
    L2 Cache Throughput               %        56.85
    SM Active Cycles              cycle    391469.88
    Compute (SM) Throughput           %        29.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.47
    Achieved Active Warps Per SM           warp        46.30
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2631504
    Total DRAM Elapsed Cycles        cycle     18070528
    Average L1 Active Cycles         cycle    391469.88
    Total L1 Elapsed Cycles          cycle     22976796
    Average L2 Active Cycles         cycle    396275.50
    Total L2 Elapsed Cycles          cycle      9725808
    Average SM Active Cycles         cycle    391469.88
    Total SM Elapsed Cycles          cycle     22976796
    Average SMSP Active Cycles       cycle    389069.41
    Total SMSP Elapsed Cycles        cycle     91907184
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       821.89
    Elapsed Cycles                cycle      1219946
    Memory Throughput                 %        83.99
    DRAM Throughput                   %        23.51
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        14.13
    SM Active Cycles              cycle   1158093.33
    Compute (SM) Throughput           %        83.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2149741.33
    Total DRAM Elapsed Cycles        cycle     54875136
    Average L1 Active Cycles         cycle   1158093.33
    Total L1 Elapsed Cycles          cycle     69945712
    Average L2 Active Cycles         cycle   1183980.50
    Total L2 Elapsed Cycles          cycle     29082048
    Average SM Active Cycles         cycle   1158093.33
    Total SM Elapsed Cycles          cycle     69945712
    Average SMSP Active Cycles       cycle   1157042.78
    Total SMSP Elapsed Cycles        cycle    279782848
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.19
    Elapsed Cycles                cycle         5585
    Memory Throughput                 %        28.88
    DRAM Throughput                   %        28.88
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        19.21
    L2 Cache Throughput               %        15.92
    SM Active Cycles              cycle      2941.31
    Compute (SM) Throughput           %        11.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.48
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12272
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2941.31
    Total L1 Elapsed Cycles          cycle       294386
    Average L2 Active Cycles         cycle      2437.38
    Total L2 Elapsed Cycles          cycle       134088
    Average SM Active Cycles         cycle      2941.31
    Total SM Elapsed Cycles          cycle       294386
    Average SMSP Active Cycles       cycle      2836.89
    Total SMSP Elapsed Cycles        cycle      1177544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.169%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.25% above the average, while the minimum instance value is 23.54% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.95
    Elapsed Cycles                cycle         5729
    Memory Throughput                 %        31.26
    DRAM Throughput                   %        31.26
    Duration                         us         7.33
    L1/TEX Cache Throughput           %        17.65
    L2 Cache Throughput               %        17.56
    SM Active Cycles              cycle      3201.36
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.66
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14085.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3201.36
    Total L1 Elapsed Cycles          cycle       310126
    Average L2 Active Cycles         cycle      2551.29
    Total L2 Elapsed Cycles          cycle       142608
    Average SM Active Cycles         cycle      3201.36
    Total SM Elapsed Cycles          cycle       310126
    Average SMSP Active Cycles       cycle      3022.54
    Total SMSP Elapsed Cycles        cycle      1240504
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       783.53
    Elapsed Cycles                cycle         5923
    Memory Throughput                 %        30.40
    DRAM Throughput                   %        30.40
    Duration                         us         7.55
    L1/TEX Cache Throughput           %        16.88
    L2 Cache Throughput               %        17.02
    SM Active Cycles              cycle      3347.14
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.31
    Achieved Active Warps Per SM           warp        33.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14058.67
    Total DRAM Elapsed Cycles        cycle       277504
    Average L1 Active Cycles         cycle      3347.14
    Total L1 Elapsed Cycles          cycle       320076
    Average L2 Active Cycles         cycle      2608.42
    Total L2 Elapsed Cycles          cycle       147456
    Average SM Active Cycles         cycle      3347.14
    Total SM Elapsed Cycles          cycle       320076
    Average SMSP Active Cycles       cycle      3124.77
    Total SMSP Elapsed Cycles        cycle      1280304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.245%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.65% above the average, while the minimum instance value is 10.67% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.751%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.16% above the average, while the minimum instance value is 11.29% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.245%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.65% above the average, while the minimum instance value is 10.67% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       794.57
    Elapsed Cycles                cycle         5716
    Memory Throughput                 %        32.13
    DRAM Throughput                   %        32.13
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.49
    L2 Cache Throughput               %        18.15
    SM Active Cycles              cycle      3229.79
    Compute (SM) Throughput           %        10.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.23
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3229.79
    Total L1 Elapsed Cycles          cycle       321170
    Average L2 Active Cycles         cycle      2640.50
    Total L2 Elapsed Cycles          cycle       138336
    Average SM Active Cycles         cycle      3229.79
    Total SM Elapsed Cycles          cycle       321170
    Average SMSP Active Cycles       cycle      3200.65
    Total SMSP Elapsed Cycles        cycle      1284680
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.57
    Elapsed Cycles                cycle      1299660
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.84
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.47
    SM Active Cycles              cycle   1292431.91
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203781.33
    Total DRAM Elapsed Cycles        cycle     61016064
    Average L1 Active Cycles         cycle   1292431.91
    Total L1 Elapsed Cycles          cycle     75142744
    Average L2 Active Cycles         cycle   1171915.50
    Total L2 Elapsed Cycles          cycle     32244120
    Average SM Active Cycles         cycle   1292431.91
    Total SM Elapsed Cycles          cycle     75142744
    Average SMSP Active Cycles       cycle   1292511.60
    Total SMSP Elapsed Cycles        cycle    300570976
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.29
    Elapsed Cycles                cycle       384061
    Memory Throughput                 %        87.45
    DRAM Throughput                   %        87.45
    Duration                         us       480.77
    L1/TEX Cache Throughput           %        49.33
    L2 Cache Throughput               %        58.13
    SM Active Cycles              cycle    379313.98
    Compute (SM) Throughput           %        30.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        94.99
    Achieved Active Warps Per SM           warp        45.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2624752
    Total DRAM Elapsed Cycles        cycle     18009088
    Average L1 Active Cycles         cycle    379313.98
    Total L1 Elapsed Cycles          cycle     22163836
    Average L2 Active Cycles         cycle    392229.67
    Total L2 Elapsed Cycles          cycle      9518232
    Average SM Active Cycles         cycle    379313.98
    Total SM Elapsed Cycles          cycle     22163836
    Average SMSP Active Cycles       cycle    379025.10
    Total SMSP Elapsed Cycles        cycle     88655344
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.56
    Elapsed Cycles                cycle      1210116
    Memory Throughput                 %        84.60
    DRAM Throughput                   %        23.19
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.43
    L2 Cache Throughput               %        13.99
    SM Active Cycles              cycle   1158501.79
    Compute (SM) Throughput           %        84.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.96
    Achieved Active Warps Per SM           warp        36.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2150448
    Total DRAM Elapsed Cycles        cycle     55645184
    Average L1 Active Cycles         cycle   1158501.79
    Total L1 Elapsed Cycles          cycle     69439728
    Average L2 Active Cycles         cycle   1204370.33
    Total L2 Elapsed Cycles          cycle     29404848
    Average SM Active Cycles         cycle   1158501.79
    Total SM Elapsed Cycles          cycle     69439728
    Average SMSP Active Cycles       cycle   1157661.83
    Total SMSP Elapsed Cycles        cycle    277758912
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.88
    Elapsed Cycles                cycle         5454
    Memory Throughput                 %        28.48
    DRAM Throughput                   %        28.48
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.68
    L2 Cache Throughput               %        15.90
    SM Active Cycles              cycle      2871.05
    Compute (SM) Throughput           %        10.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.08
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12104
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2871.05
    Total L1 Elapsed Cycles          cycle       298232
    Average L2 Active Cycles         cycle      2419.79
    Total L2 Elapsed Cycles          cycle       134112
    Average SM Active Cycles         cycle      2871.05
    Total SM Elapsed Cycles          cycle       298232
    Average SMSP Active Cycles       cycle      2765.87
    Total SMSP Elapsed Cycles        cycle      1192928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.455%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 17.22% above the average, while the minimum instance value is 5.03% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       783.90
    Elapsed Cycles                cycle         5705
    Memory Throughput                 %        31.71
    DRAM Throughput                   %        31.71
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.26
    L2 Cache Throughput               %        17.85
    SM Active Cycles              cycle      3273.64
    Compute (SM) Throughput           %        10.42
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.16
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3273.64
    Total L1 Elapsed Cycles          cycle       314562
    Average L2 Active Cycles         cycle      2614.33
    Total L2 Elapsed Cycles          cycle       140568
    Average SM Active Cycles         cycle      3273.64
    Total SM Elapsed Cycles          cycle       314562
    Average SMSP Active Cycles       cycle      3278.46
    Total SMSP Elapsed Cycles        cycle      1258248
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.015%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.97% above the average, while the minimum instance value is 12.45% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.015%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.97% above the average, while the minimum instance value is 12.45% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       794.13
    Elapsed Cycles                cycle         5942
    Memory Throughput                 %        30.90
    DRAM Throughput                   %        30.90
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.25
    L2 Cache Throughput               %        17.42
    SM Active Cycles              cycle      3275.86
    Compute (SM) Throughput           %        10.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.62
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13973.33
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3275.86
    Total L1 Elapsed Cycles          cycle       308602
    Average L2 Active Cycles         cycle         2625
    Total L2 Elapsed Cycles          cycle       143976
    Average SM Active Cycles         cycle      3275.86
    Total SM Elapsed Cycles          cycle       308602
    Average SMSP Active Cycles       cycle      3156.48
    Total SMSP Elapsed Cycles        cycle      1234408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.531%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.01% above the average, while the minimum instance value is 11.67% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.91
    Elapsed Cycles                cycle         5661
    Memory Throughput                 %        31.60
    DRAM Throughput                   %        31.60
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.68
    L2 Cache Throughput               %        17.80
    SM Active Cycles              cycle      3194.88
    Compute (SM) Throughput           %        10.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.16
    Achieved Active Warps Per SM           warp        35.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14074.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3194.88
    Total L1 Elapsed Cycles          cycle       311344
    Average L2 Active Cycles         cycle      2617.92
    Total L2 Elapsed Cycles          cycle       140928
    Average SM Active Cycles         cycle      3194.88
    Total SM Elapsed Cycles          cycle       311344
    Average SMSP Active Cycles       cycle      3079.18
    Total SMSP Elapsed Cycles        cycle      1245376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.55%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.68% above the average, while the minimum instance value is 13.94% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.683%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 17.23% above the average, while the minimum instance value is 7.06% below the       
          average.                                                                                                      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.54
    Elapsed Cycles                cycle      1299270
    Memory Throughput                 %        82.37
    DRAM Throughput                   %        11.84
    Duration                         ms         1.63
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.51
    SM Active Cycles              cycle   1292578.86
    Compute (SM) Throughput           %        82.37
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203962.67
    Total DRAM Elapsed Cycles        cycle     61016064
    Average L1 Active Cycles         cycle   1292578.86
    Total L1 Elapsed Cycles          cycle     75108642
    Average L2 Active Cycles         cycle   1172741.33
    Total L2 Elapsed Cycles          cycle     32244336
    Average SM Active Cycles         cycle   1292578.86
    Total SM Elapsed Cycles          cycle     75108642
    Average SMSP Active Cycles       cycle   1292457.86
    Total SMSP Elapsed Cycles        cycle    300434568
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.47
    Elapsed Cycles                cycle       384893
    Memory Throughput                 %        87.27
    DRAM Throughput                   %        87.27
    Duration                         us       481.41
    L1/TEX Cache Throughput           %        49.35
    L2 Cache Throughput               %        58.06
    SM Active Cycles              cycle    377505.45
    Compute (SM) Throughput           %        30.99
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.46
    Achieved Active Warps Per SM           warp        45.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2623024
    Total DRAM Elapsed Cycles        cycle     18033664
    Average L1 Active Cycles         cycle    377505.45
    Total L1 Elapsed Cycles          cycle     22150102
    Average L2 Active Cycles         cycle    392810.21
    Total L2 Elapsed Cycles          cycle      9530928
    Average SM Active Cycles         cycle    377505.45
    Total SM Elapsed Cycles          cycle     22150102
    Average SMSP Active Cycles       cycle    379612.77
    Total SMSP Elapsed Cycles        cycle     88600408
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.58
    Elapsed Cycles                cycle      1210038
    Memory Throughput                 %        84.77
    DRAM Throughput                   %        23.17
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.53
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1157126.72
    Compute (SM) Throughput           %        84.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.08
    Achieved Active Warps Per SM           warp        37.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148608
    Total DRAM Elapsed Cycles        cycle     55651328
    Average L1 Active Cycles         cycle   1157126.72
    Total L1 Elapsed Cycles          cycle     69301700
    Average L2 Active Cycles         cycle   1202884.04
    Total L2 Elapsed Cycles          cycle     29408040
    Average SM Active Cycles         cycle   1157126.72
    Total SM Elapsed Cycles          cycle     69301700
    Average SMSP Active Cycles       cycle   1156553.93
    Total SMSP Elapsed Cycles        cycle    277206800
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.14
    Elapsed Cycles                cycle         5463
    Memory Throughput                 %        29.01
    DRAM Throughput                   %        29.01
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        18.70
    L2 Cache Throughput               %        15.89
    SM Active Cycles              cycle      3021.66
    Compute (SM) Throughput           %        11.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 30.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.38
    Achieved Active Warps Per SM           warp        33.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12280
    Total DRAM Elapsed Cycles        cycle       253952
    Average L1 Active Cycles         cycle      3021.66
    Total L1 Elapsed Cycles          cycle       294712
    Average L2 Active Cycles         cycle      2474.67
    Total L2 Elapsed Cycles          cycle       134160
    Average SM Active Cycles         cycle      3021.66
    Total SM Elapsed Cycles          cycle       294712
    Average SMSP Active Cycles       cycle      2861.83
    Total SMSP Elapsed Cycles        cycle      1178848
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       783.81
    Elapsed Cycles                cycle         5989
    Memory Throughput                 %        30.23
    DRAM Throughput                   %        30.23
    Duration                         us         7.58
    L1/TEX Cache Throughput           %        17.29
    L2 Cache Throughput               %        17.04
    SM Active Cycles              cycle      3266.74
    Compute (SM) Throughput           %        10.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.11
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13981.33
    Total DRAM Elapsed Cycles        cycle       277504
    Average L1 Active Cycles         cycle      3266.74
    Total L1 Elapsed Cycles          cycle       321236
    Average L2 Active Cycles         cycle      2547.04
    Total L2 Elapsed Cycles          cycle       147264
    Average SM Active Cycles         cycle      3266.74
    Total SM Elapsed Cycles          cycle       321236
    Average SMSP Active Cycles       cycle      3032.81
    Total SMSP Elapsed Cycles        cycle      1284944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.723%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 11.40% above the average, while the minimum instance value is 12.51% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.256%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.43% above the average, while the minimum instance value is 14.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.723%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 11.40% above the average, while the minimum instance value is 12.51% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.38
    Elapsed Cycles                cycle         5696
    Memory Throughput                 %        31.48
    DRAM Throughput                   %        31.48
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        18.23
    L2 Cache Throughput               %        17.89
    SM Active Cycles              cycle      3099.88
    Compute (SM) Throughput           %        10.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.34
    Achieved Active Warps Per SM           warp        36.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14024
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3099.88
    Total L1 Elapsed Cycles          cycle       311680
    Average L2 Active Cycles         cycle      2611.38
    Total L2 Elapsed Cycles          cycle       140160
    Average SM Active Cycles         cycle      3099.88
    Total SM Elapsed Cycles          cycle       311680
    Average SMSP Active Cycles       cycle      3085.04
    Total SMSP Elapsed Cycles        cycle      1246720
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.608%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.77% above the average, while the minimum instance value is 15.66% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.56
    Elapsed Cycles                cycle         5700
    Memory Throughput                 %        31.50
    DRAM Throughput                   %        31.50
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.63
    L2 Cache Throughput               %        17.93
    SM Active Cycles              cycle      3204.98
    Compute (SM) Throughput           %        10.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3204.98
    Total L1 Elapsed Cycles          cycle       307386
    Average L2 Active Cycles         cycle      2675.75
    Total L2 Elapsed Cycles          cycle       140040
    Average SM Active Cycles         cycle      3204.98
    Total SM Elapsed Cycles          cycle       307386
    Average SMSP Active Cycles       cycle      3180.12
    Total SMSP Elapsed Cycles        cycle      1229544
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.21
    Elapsed Cycles                cycle      1312845
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.97
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292493.64
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1204517.33
    Total DRAM Elapsed Cycles        cycle     60389376
    Average L1 Active Cycles         cycle   1292493.64
    Total L1 Elapsed Cycles          cycle     75129962
    Average L2 Active Cycles         cycle   1164199.67
    Total L2 Elapsed Cycles          cycle     31913472
    Average SM Active Cycles         cycle   1292493.64
    Total SM Elapsed Cycles          cycle     75129962
    Average SMSP Active Cycles       cycle   1292400.22
    Total SMSP Elapsed Cycles        cycle    300519848
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.03
    Elapsed Cycles                cycle       392003
    Memory Throughput                 %        87.47
    DRAM Throughput                   %        87.47
    Duration                         us       482.43
    L1/TEX Cache Throughput           %        48.96
    L2 Cache Throughput               %        57.98
    SM Active Cycles              cycle    375391.81
    Compute (SM) Throughput           %        30.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.13
    Achieved Active Warps Per SM           warp        46.62
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2634802.67
    Total DRAM Elapsed Cycles        cycle     18072576
    Average L1 Active Cycles         cycle    375391.81
    Total L1 Elapsed Cycles          cycle     22335064
    Average L2 Active Cycles         cycle       392823
    Total L2 Elapsed Cycles          cycle      9552408
    Average SM Active Cycles         cycle    375391.81
    Total SM Elapsed Cycles          cycle     22335064
    Average SMSP Active Cycles       cycle    379705.72
    Total SMSP Elapsed Cycles        cycle     89340256
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.61
    Elapsed Cycles                cycle      1216084
    Memory Throughput                 %        84.61
    DRAM Throughput                   %        23.04
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158137.86
    Compute (SM) Throughput           %        84.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2147312
    Total DRAM Elapsed Cycles        cycle     55927808
    Average L1 Active Cycles         cycle   1158137.86
    Total L1 Elapsed Cycles          cycle     69429636
    Average L2 Active Cycles         cycle   1202525.21
    Total L2 Elapsed Cycles          cycle     29554896
    Average SM Active Cycles         cycle   1158137.86
    Total SM Elapsed Cycles          cycle     69429636
    Average SMSP Active Cycles       cycle   1157869.22
    Total SMSP Elapsed Cycles        cycle    277718544
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.41
    Elapsed Cycles                cycle         5446
    Memory Throughput                 %        28.51
    DRAM Throughput                   %        28.51
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        19.76
    L2 Cache Throughput               %        15.93
    SM Active Cycles              cycle      2858.47
    Compute (SM) Throughput           %        11.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.64
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12114.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2858.47
    Total L1 Elapsed Cycles          cycle       294992
    Average L2 Active Cycles         cycle      2409.83
    Total L2 Elapsed Cycles          cycle       133752
    Average SM Active Cycles         cycle      2858.47
    Total SM Elapsed Cycles          cycle       294992
    Average SMSP Active Cycles       cycle      2760.75
    Total SMSP Elapsed Cycles        cycle      1179968
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.507%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.80% above the average, while the minimum instance value is 19.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.049%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 11.14% above the average, while the minimum instance value is 20.96% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.507%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.80% above the average, while the minimum instance value is 19.19% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.092%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.09% above the average, while the minimum instance value is 4.77% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.29
    Elapsed Cycles                cycle         5950
    Memory Throughput                 %        30.20
    DRAM Throughput                   %        30.20
    Duration                         us         7.52
    L1/TEX Cache Throughput           %        17.97
    L2 Cache Throughput               %        17.16
    SM Active Cycles              cycle      3144.66
    Compute (SM) Throughput           %        10.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.32
    Achieved Active Warps Per SM           warp        36.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14018.67
    Total DRAM Elapsed Cycles        cycle       278528
    Average L1 Active Cycles         cycle      3144.66
    Total L1 Elapsed Cycles          cycle       312274
    Average L2 Active Cycles         cycle      2653.25
    Total L2 Elapsed Cycles          cycle       146160
    Average SM Active Cycles         cycle      3144.66
    Total SM Elapsed Cycles          cycle       312274
    Average SMSP Active Cycles       cycle      3154.25
    Total SMSP Elapsed Cycles        cycle      1249096
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       788.34
    Elapsed Cycles                cycle         5847
    Memory Throughput                 %        30.63
    DRAM Throughput                   %        30.63
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.89
    L2 Cache Throughput               %        17.47
    SM Active Cycles              cycle      3157.83
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.82
    Achieved Active Warps Per SM           warp        36.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13957.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3157.83
    Total L1 Elapsed Cycles          cycle       306228
    Average L2 Active Cycles         cycle      2559.58
    Total L2 Elapsed Cycles          cycle       143544
    Average SM Active Cycles         cycle      3157.83
    Total SM Elapsed Cycles          cycle       306228
    Average SMSP Active Cycles       cycle      3053.65
    Total SMSP Elapsed Cycles        cycle      1224912
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.553%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.60% above the average, while the minimum instance value is 9.45% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       787.25
    Elapsed Cycles                cycle         5759
    Memory Throughput                 %        31.67
    DRAM Throughput                   %        31.67
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.58
    L2 Cache Throughput               %        17.74
    SM Active Cycles              cycle      3213.52
    Compute (SM) Throughput           %        10.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.74
    Achieved Active Warps Per SM           warp        35.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14106.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3213.52
    Total L1 Elapsed Cycles          cycle       306326
    Average L2 Active Cycles         cycle      2657.75
    Total L2 Elapsed Cycles          cycle       141600
    Average SM Active Cycles         cycle      3213.52
    Total SM Elapsed Cycles          cycle       306326
    Average SMSP Active Cycles       cycle      3184.12
    Total SMSP Elapsed Cycles        cycle      1225304
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.18
    Elapsed Cycles                cycle      1313013
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292498.79
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203296
    Total DRAM Elapsed Cycles        cycle     60390400
    Average L1 Active Cycles         cycle   1292498.79
    Total L1 Elapsed Cycles          cycle     75141908
    Average L2 Active Cycles         cycle   1164090.75
    Total L2 Elapsed Cycles          cycle     31912992
    Average SM Active Cycles         cycle   1292498.79
    Total SM Elapsed Cycles          cycle     75141908
    Average SMSP Active Cycles       cycle   1292552.69
    Total SMSP Elapsed Cycles        cycle    300567632
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.80
    Elapsed Cycles                cycle       390451
    Memory Throughput                 %        87.41
    DRAM Throughput                   %        87.41
    Duration                         us       480.70
    L1/TEX Cache Throughput           %        49.06
    L2 Cache Throughput               %        58.13
    SM Active Cycles              cycle    379951.81
    Compute (SM) Throughput           %        30.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.39
    Achieved Active Warps Per SM           warp        45.79
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2623480
    Total DRAM Elapsed Cycles        cycle     18008064
    Average L1 Active Cycles         cycle    379951.81
    Total L1 Elapsed Cycles          cycle     22281876
    Average L2 Active Cycles         cycle    393062.46
    Total L2 Elapsed Cycles          cycle      9518568
    Average SM Active Cycles         cycle    379951.81
    Total SM Elapsed Cycles          cycle     22281876
    Average SMSP Active Cycles       cycle    379945.04
    Total SMSP Elapsed Cycles        cycle     89127504
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.58
    Elapsed Cycles                cycle      1210601
    Memory Throughput                 %        84.51
    DRAM Throughput                   %        23.17
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.54
    L2 Cache Throughput               %        13.98
    SM Active Cycles              cycle   1157048.81
    Compute (SM) Throughput           %        84.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.08
    Achieved Active Warps Per SM           warp        37.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148909.33
    Total DRAM Elapsed Cycles        cycle     55655424
    Average L1 Active Cycles         cycle   1157048.81
    Total L1 Elapsed Cycles          cycle     69509172
    Average L2 Active Cycles         cycle   1203959.58
    Total L2 Elapsed Cycles          cycle     29411016
    Average SM Active Cycles         cycle   1157048.81
    Total SM Elapsed Cycles          cycle     69509172
    Average SMSP Active Cycles       cycle   1157634.66
    Total SMSP Elapsed Cycles        cycle    278036688
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       785.71
    Elapsed Cycles                cycle         5504
    Memory Throughput                 %        28.89
    DRAM Throughput                   %        28.89
    Duration                         us         6.94
    L1/TEX Cache Throughput           %        19.01
    L2 Cache Throughput               %        15.81
    SM Active Cycles              cycle      2972.19
    Compute (SM) Throughput           %        11.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.69
    Achieved Active Warps Per SM           warp        33.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12277.33
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2972.19
    Total L1 Elapsed Cycles          cycle       294432
    Average L2 Active Cycles         cycle      2501.75
    Total L2 Elapsed Cycles          cycle       134976
    Average SM Active Cycles         cycle      2972.19
    Total SM Elapsed Cycles          cycle       294432
    Average SMSP Active Cycles       cycle      2899.91
    Total SMSP Elapsed Cycles        cycle      1177728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       787.31
    Elapsed Cycles                cycle         5844
    Memory Throughput                 %        31.07
    DRAM Throughput                   %        31.07
    Duration                         us         7.36
    L1/TEX Cache Throughput           %        17.92
    L2 Cache Throughput               %        17.50
    SM Active Cycles              cycle      3152.10
    Compute (SM) Throughput           %         9.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.54
    Achieved Active Warps Per SM           warp        35.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14053.33
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3152.10
    Total L1 Elapsed Cycles          cycle       327852
    Average L2 Active Cycles         cycle      2571.04
    Total L2 Elapsed Cycles          cycle       143160
    Average SM Active Cycles         cycle      3152.10
    Total SM Elapsed Cycles          cycle       327852
    Average SMSP Active Cycles       cycle      3076.71
    Total SMSP Elapsed Cycles        cycle      1311408
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.743%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 10.30% above the average, while the minimum instance value is 12.72% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.936%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 12.74% above the average, while the minimum instance value is 11.07% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.743%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 10.30% above the average, while the minimum instance value is 12.72% below the      
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.16
    Elapsed Cycles                cycle         5621
    Memory Throughput                 %        32.17
    DRAM Throughput                   %        32.17
    Duration                         us         7.10
    L1/TEX Cache Throughput           %        17.55
    L2 Cache Throughput               %        18.17
    SM Active Cycles              cycle      3219.10
    Compute (SM) Throughput           %        10.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.75
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14056
    Total DRAM Elapsed Cycles        cycle       262144
    Average L1 Active Cycles         cycle      3219.10
    Total L1 Elapsed Cycles          cycle       302344
    Average L2 Active Cycles         cycle      2552.42
    Total L2 Elapsed Cycles          cycle       138072
    Average SM Active Cycles         cycle      3219.10
    Total SM Elapsed Cycles          cycle       302344
    Average SMSP Active Cycles       cycle      3019.12
    Total SMSP Elapsed Cycles        cycle      1209376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.01%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.65% above the average, while the minimum instance value is 18.16% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.264%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.86% above the average, while the minimum instance value is 5.27% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.99
    Elapsed Cycles                cycle         5666
    Memory Throughput                 %        31.98
    DRAM Throughput                   %        31.98
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.76
    L2 Cache Throughput               %        18.00
    SM Active Cycles              cycle      3181.10
    Compute (SM) Throughput           %        10.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.97
    Achieved Active Warps Per SM           warp        35.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14082.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3181.10
    Total L1 Elapsed Cycles          cycle       310914
    Average L2 Active Cycles         cycle      2572.12
    Total L2 Elapsed Cycles          cycle       139464
    Average SM Active Cycles         cycle      3181.10
    Total SM Elapsed Cycles          cycle       310914
    Average SMSP Active Cycles       cycle      3049.08
    Total SMSP Elapsed Cycles        cycle      1243656
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.17
    Elapsed Cycles                cycle      1312708
    Memory Throughput                 %        82.33
    DRAM Throughput                   %        11.97
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292575.69
    Compute (SM) Throughput           %        82.33
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1205090.67
    Total DRAM Elapsed Cycles        cycle     60386304
    Average L1 Active Cycles         cycle   1292575.69
    Total L1 Elapsed Cycles          cycle     75142136
    Average L2 Active Cycles         cycle   1147420.67
    Total L2 Elapsed Cycles          cycle     31911792
    Average SM Active Cycles         cycle   1292575.69
    Total SM Elapsed Cycles          cycle     75142136
    Average SMSP Active Cycles       cycle   1292565.10
    Total SMSP Elapsed Cycles        cycle    300568544
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.13
    Elapsed Cycles                cycle       402963
    Memory Throughput                 %        87.50
    DRAM Throughput                   %        87.50
    Duration                         us       480.58
    L1/TEX Cache Throughput           %        47.55
    L2 Cache Throughput               %        57.32
    SM Active Cycles              cycle    393570.79
    Compute (SM) Throughput           %        29.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.08
    Achieved Active Warps Per SM           warp        45.64
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2625162.67
    Total DRAM Elapsed Cycles        cycle     18001920
    Average L1 Active Cycles         cycle    393570.79
    Total L1 Elapsed Cycles          cycle     22984136
    Average L2 Active Cycles         cycle    398756.21
    Total L2 Elapsed Cycles          cycle      9645744
    Average SM Active Cycles         cycle    393570.79
    Total SM Elapsed Cycles          cycle     22984136
    Average SMSP Active Cycles       cycle    391885.09
    Total SMSP Elapsed Cycles        cycle     91936544
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.82
    Elapsed Cycles                cycle      1215670
    Memory Throughput                 %        84.56
    DRAM Throughput                   %        23.06
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        13.92
    SM Active Cycles              cycle   1158052.33
    Compute (SM) Throughput           %        84.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2147490.67
    Total DRAM Elapsed Cycles        cycle     55882752
    Average L1 Active Cycles         cycle   1158052.33
    Total L1 Elapsed Cycles          cycle     69470556
    Average L2 Active Cycles         cycle   1205184.04
    Total L2 Elapsed Cycles          cycle     29531472
    Average SM Active Cycles         cycle   1158052.33
    Total SM Elapsed Cycles          cycle     69470556
    Average SMSP Active Cycles       cycle   1157656.78
    Total SMSP Elapsed Cycles        cycle    277882224
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       785.43
    Elapsed Cycles                cycle         5415
    Memory Throughput                 %        28.72
    DRAM Throughput                   %        28.72
    Duration                         us         6.85
    L1/TEX Cache Throughput           %        19.94
    L2 Cache Throughput               %        16.02
    SM Active Cycles              cycle      2833.21
    Compute (SM) Throughput           %        11.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12058.67
    Total DRAM Elapsed Cycles        cycle       251904
    Average L1 Active Cycles         cycle      2833.21
    Total L1 Elapsed Cycles          cycle       294850
    Average L2 Active Cycles         cycle      2392.96
    Total L2 Elapsed Cycles          cycle       133272
    Average SM Active Cycles         cycle      2833.21
    Total SM Elapsed Cycles          cycle       294850
    Average SMSP Active Cycles       cycle      2802.00
    Total SMSP Elapsed Cycles        cycle      1179400
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.397%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 9.68% above the average, while the minimum instance value is 17.62% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.602%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.16% above the average, while the minimum instance value is 20.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.397%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 9.68% above the average, while the minimum instance value is 17.62% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.498%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 17.40% above the average, while the minimum instance value is 4.68% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.37
    Elapsed Cycles                cycle         5946
    Memory Throughput                 %        30.28
    DRAM Throughput                   %        30.28
    Duration                         us         7.49
    L1/TEX Cache Throughput           %        17.36
    L2 Cache Throughput               %        17.20
    SM Active Cycles              cycle      3253.83
    Compute (SM) Throughput           %        10.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.59
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14002.67
    Total DRAM Elapsed Cycles        cycle       277504
    Average L1 Active Cycles         cycle      3253.83
    Total L1 Elapsed Cycles          cycle       319996
    Average L2 Active Cycles         cycle      2578.17
    Total L2 Elapsed Cycles          cycle       145968
    Average SM Active Cycles         cycle      3253.83
    Total SM Elapsed Cycles          cycle       319996
    Average SMSP Active Cycles       cycle      3062.05
    Total SMSP Elapsed Cycles        cycle      1279984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.972%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 12.56% above the average, while the minimum instance value is 18.36% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.54
    Elapsed Cycles                cycle         5724
    Memory Throughput                 %        31.45
    DRAM Throughput                   %        31.45
    Duration                         us         7.23
    L1/TEX Cache Throughput           %        17.50
    L2 Cache Throughput               %        17.85
    SM Active Cycles              cycle      3228.76
    Compute (SM) Throughput           %        10.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.34
    Achieved Active Warps Per SM           warp        35.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14010.67
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3228.76
    Total L1 Elapsed Cycles          cycle       305790
    Average L2 Active Cycles         cycle      2649.21
    Total L2 Elapsed Cycles          cycle       140592
    Average SM Active Cycles         cycle      3228.76
    Total SM Elapsed Cycles          cycle       305790
    Average SMSP Active Cycles       cycle      3151.17
    Total SMSP Elapsed Cycles        cycle      1223160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.082%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.50% above the average, while the minimum instance value is 12.45% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.50
    Elapsed Cycles                cycle         5691
    Memory Throughput                 %        31.72
    DRAM Throughput                   %        31.72
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        17.26
    L2 Cache Throughput               %        17.95
    SM Active Cycles              cycle      3272.88
    Compute (SM) Throughput           %        10.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.97
    Achieved Active Warps Per SM           warp        34.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14074.67
    Total DRAM Elapsed Cycles        cycle       266240
    Average L1 Active Cycles         cycle      3272.88
    Total L1 Elapsed Cycles          cycle       302494
    Average L2 Active Cycles         cycle      2640.29
    Total L2 Elapsed Cycles          cycle       139896
    Average SM Active Cycles         cycle      3272.88
    Total SM Elapsed Cycles          cycle       302494
    Average SMSP Active Cycles       cycle      3201.01
    Total SMSP Elapsed Cycles        cycle      1209976
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.17
    Elapsed Cycles                cycle      1313865
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        12.26
    Duration                         ms         1.57
    L1/TEX Cache Throughput           %        82.51
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292691.12
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203925.33
    Total DRAM Elapsed Cycles        cycle     58942464
    Average L1 Active Cycles         cycle   1292691.12
    Total L1 Elapsed Cycles          cycle     75123124
    Average L2 Active Cycles         cycle   1147361.42
    Total L2 Elapsed Cycles          cycle     31307112
    Average SM Active Cycles         cycle   1292691.12
    Total SM Elapsed Cycles          cycle     75123124
    Average SMSP Active Cycles       cycle   1292718.03
    Total SMSP Elapsed Cycles        cycle    300492496
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.53
    Elapsed Cycles                cycle       387633
    Memory Throughput                 %        87.11
    DRAM Throughput                   %        87.11
    Duration                         us       484.83
    L1/TEX Cache Throughput           %        49.52
    L2 Cache Throughput               %        57.65
    SM Active Cycles              cycle    375528.79
    Compute (SM) Throughput           %        31.09
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.66
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2636648
    Total DRAM Elapsed Cycles        cycle     18161664
    Average L1 Active Cycles         cycle    375528.79
    Total L1 Elapsed Cycles          cycle     22077522
    Average L2 Active Cycles         cycle    393909.04
    Total L2 Elapsed Cycles          cycle      9598296
    Average SM Active Cycles         cycle    375528.79
    Total SM Elapsed Cycles          cycle     22077522
    Average SMSP Active Cycles       cycle    376783.38
    Total SMSP Elapsed Cycles        cycle     88310088
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       821.90
    Elapsed Cycles                cycle      1212863
    Memory Throughput                 %        84.56
    DRAM Throughput                   %        23.64
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.44
    L2 Cache Throughput               %        14.20
    SM Active Cycles              cycle   1158306.71
    Compute (SM) Throughput           %        84.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.01
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2149629.33
    Total DRAM Elapsed Cycles        cycle     54552576
    Average L1 Active Cycles         cycle   1158306.71
    Total L1 Elapsed Cycles          cycle     69472916
    Average L2 Active Cycles         cycle   1182043.38
    Total L2 Elapsed Cycles          cycle     28917456
    Average SM Active Cycles         cycle   1158306.71
    Total SM Elapsed Cycles          cycle     69472916
    Average SMSP Active Cycles       cycle   1158185.78
    Total SMSP Elapsed Cycles        cycle    277891664
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       798.06
    Elapsed Cycles                cycle         5589
    Memory Throughput                 %        29.02
    DRAM Throughput                   %        29.02
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        18.83
    L2 Cache Throughput               %        15.87
    SM Active Cycles              cycle      2999.60
    Compute (SM) Throughput           %        11.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.07
    Achieved Active Warps Per SM           warp        34.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12282.67
    Total DRAM Elapsed Cycles        cycle       253952
    Average L1 Active Cycles         cycle      2999.60
    Total L1 Elapsed Cycles          cycle       293284
    Average L2 Active Cycles         cycle      2469.29
    Total L2 Elapsed Cycles          cycle       134352
    Average SM Active Cycles         cycle      2999.60
    Total SM Elapsed Cycles          cycle       293284
    Average SMSP Active Cycles       cycle      2916.90
    Total SMSP Elapsed Cycles        cycle      1173136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.96%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.33% above the average, while the minimum instance value is 20.26% below the average.     

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.10
    SM Frequency                    Mhz       779.98
    Elapsed Cycles                cycle         5776
    Memory Throughput                 %        31.01
    DRAM Throughput                   %        31.01
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.88
    L2 Cache Throughput               %        17.47
    SM Active Cycles              cycle      3159.57
    Compute (SM) Throughput           %        10.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.33
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13973.33
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3159.57
    Total L1 Elapsed Cycles          cycle       319326
    Average L2 Active Cycles         cycle      2565.12
    Total L2 Elapsed Cycles          cycle       143640
    Average SM Active Cycles         cycle      3159.57
    Total SM Elapsed Cycles          cycle       319326
    Average SMSP Active Cycles       cycle      3049.29
    Total SMSP Elapsed Cycles        cycle      1277304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.523%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 9.62% above the average, while the minimum instance value is 9.07% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.449%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.84% above the average, while the minimum instance value is 12.34% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.523%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 9.62% above the average, while the minimum instance value is 9.07% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       785.35
    Elapsed Cycles                cycle         5751
    Memory Throughput                 %        31.59
    DRAM Throughput                   %        31.59
    Duration                         us         7.26
    L1/TEX Cache Throughput           %        17.85
    L2 Cache Throughput               %        17.80
    SM Active Cycles              cycle      3164.72
    Compute (SM) Throughput           %        10.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.92
    Achieved Active Warps Per SM           warp        35.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14072
    Total DRAM Elapsed Cycles        cycle       267264
    Average L1 Active Cycles         cycle      3164.72
    Total L1 Elapsed Cycles          cycle       315218
    Average L2 Active Cycles         cycle      2627.04
    Total L2 Elapsed Cycles          cycle       141144
    Average SM Active Cycles         cycle      3164.72
    Total SM Elapsed Cycles          cycle       315218
    Average SMSP Active Cycles       cycle      3224.95
    Total SMSP Elapsed Cycles        cycle      1260872
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       786.75
    Elapsed Cycles                cycle         5711
    Memory Throughput                 %        31.78
    DRAM Throughput                   %        31.78
    Duration                         us         7.20
    L1/TEX Cache Throughput           %        18.12
    L2 Cache Throughput               %        17.91
    SM Active Cycles              cycle      3117.74
    Compute (SM) Throughput           %        10.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.67
    Achieved Active Warps Per SM           warp        36.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     13994.67
    Total DRAM Elapsed Cycles        cycle       264192
    Average L1 Active Cycles         cycle      3117.74
    Total L1 Elapsed Cycles          cycle       307602
    Average L2 Active Cycles         cycle      2659.25
    Total L2 Elapsed Cycles          cycle       140232
    Average SM Active Cycles         cycle      3117.74
    Total SM Elapsed Cycles          cycle       307602
    Average SMSP Active Cycles       cycle      3181.69
    Total SMSP Elapsed Cycles        cycle      1230408
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.37
    Elapsed Cycles                cycle      1313074
    Memory Throughput                 %        82.36
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.62
    SM Active Cycles              cycle   1292577.05
    Compute (SM) Throughput           %        82.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.56
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203488
    Total DRAM Elapsed Cycles        cycle     60392448
    Average L1 Active Cycles         cycle   1292577.05
    Total L1 Elapsed Cycles          cycle     75112754
    Average L2 Active Cycles         cycle   1164698.04
    Total L2 Elapsed Cycles          cycle     31914792
    Average SM Active Cycles         cycle   1292577.05
    Total SM Elapsed Cycles          cycle     75112754
    Average SMSP Active Cycles       cycle   1292386.79
    Total SMSP Elapsed Cycles        cycle    300451016
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       835.58
    Elapsed Cycles                cycle       405760
    Memory Throughput                 %        87.52
    DRAM Throughput                   %        87.52
    Duration                         us       479.65
    L1/TEX Cache Throughput           %        47.22
    L2 Cache Throughput               %        57.00
    SM Active Cycles              cycle    390575.88
    Compute (SM) Throughput           %        29.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.55
    Achieved Active Warps Per SM           warp        46.34
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2620840
    Total DRAM Elapsed Cycles        cycle     17968128
    Average L1 Active Cycles         cycle    390575.88
    Total L1 Elapsed Cycles          cycle     23141760
    Average L2 Active Cycles         cycle    399202.62
    Total L2 Elapsed Cycles          cycle      9702528
    Average SM Active Cycles         cycle    390575.88
    Total SM Elapsed Cycles          cycle     23141760
    Average SMSP Active Cycles       cycle    392260.29
    Total SMSP Elapsed Cycles        cycle     92567040
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.87
    Elapsed Cycles                cycle      1217653
    Memory Throughput                 %        84.66
    DRAM Throughput                   %        23.63
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.46
    L2 Cache Throughput               %        14.15
    SM Active Cycles              cycle   1158090.67
    Compute (SM) Throughput           %        84.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.03
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2150005.33
    Total DRAM Elapsed Cycles        cycle     54597632
    Average L1 Active Cycles         cycle   1158090.67
    Total L1 Elapsed Cycles          cycle     69387108
    Average L2 Active Cycles         cycle   1183678.96
    Total L2 Elapsed Cycles          cycle     29034240
    Average SM Active Cycles         cycle   1158090.67
    Total SM Elapsed Cycles          cycle     69387108
    Average SMSP Active Cycles       cycle   1158372.55
    Total SMSP Elapsed Cycles        cycle    277548432
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.07
    Elapsed Cycles                cycle         5457
    Memory Throughput                 %        28.49
    DRAM Throughput                   %        28.49
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.44
    L2 Cache Throughput               %        15.91
    SM Active Cycles              cycle      2906.62
    Compute (SM) Throughput           %        11.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.32
    Achieved Active Warps Per SM           warp        34.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12106.67
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2906.62
    Total L1 Elapsed Cycles          cycle       296982
    Average L2 Active Cycles         cycle      2388.21
    Total L2 Elapsed Cycles          cycle       133992
    Average SM Active Cycles         cycle      2906.62
    Total SM Elapsed Cycles          cycle       296982
    Average SMSP Active Cycles       cycle      2755.13
    Total SMSP Elapsed Cycles        cycle      1187928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.029%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 14.09% above the average, while the minimum instance value is 4.91% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       787.45
    Elapsed Cycles                cycle         5874
    Memory Throughput                 %        30.92
    DRAM Throughput                   %        30.92
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.69
    L2 Cache Throughput               %        17.42
    SM Active Cycles              cycle      3193.21
    Compute (SM) Throughput           %        10.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.78
    Achieved Active Warps Per SM           warp        35.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14037.33
    Total DRAM Elapsed Cycles        cycle       272384
    Average L1 Active Cycles         cycle      3193.21
    Total L1 Elapsed Cycles          cycle       317990
    Average L2 Active Cycles         cycle      2642.79
    Total L2 Elapsed Cycles          cycle       143904
    Average SM Active Cycles         cycle      3193.21
    Total SM Elapsed Cycles          cycle       317990
    Average SMSP Active Cycles       cycle      3139.85
    Total SMSP Elapsed Cycles        cycle      1271960
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.025%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 10.52% above the average, while the minimum instance value is 12.64% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.157%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.70% above the average, while the minimum instance value is 7.11% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       788.51
    Elapsed Cycles                cycle         5673
    Memory Throughput                 %        31.99
    DRAM Throughput                   %        31.99
    Duration                         us         7.14
    L1/TEX Cache Throughput           %        18.18
    L2 Cache Throughput               %        18.03
    SM Active Cycles              cycle      3107.60
    Compute (SM) Throughput           %        10.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.03
    Achieved Active Warps Per SM           warp        36.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14029.33
    Total DRAM Elapsed Cycles        cycle       263168
    Average L1 Active Cycles         cycle      3107.60
    Total L1 Elapsed Cycles          cycle       303956
    Average L2 Active Cycles         cycle      2623.33
    Total L2 Elapsed Cycles          cycle       139248
    Average SM Active Cycles         cycle      3107.60
    Total SM Elapsed Cycles          cycle       303956
    Average SMSP Active Cycles       cycle      3133.93
    Total SMSP Elapsed Cycles        cycle      1215824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       787.55
    Elapsed Cycles                cycle         5872
    Memory Throughput                 %        30.87
    DRAM Throughput                   %        30.87
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        17.43
    SM Active Cycles              cycle      3139.40
    Compute (SM) Throughput           %        10.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        13960
    Total DRAM Elapsed Cycles        cycle       271360
    Average L1 Active Cycles         cycle      3139.40
    Total L1 Elapsed Cycles          cycle       301380
    Average L2 Active Cycles         cycle      2639.46
    Total L2 Elapsed Cycles          cycle       143928
    Average SM Active Cycles         cycle      3139.40
    Total SM Elapsed Cycles          cycle       301380
    Average SMSP Active Cycles       cycle      3133.24
    Total SMSP Elapsed Cycles        cycle      1205520
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.20
    Elapsed Cycles                cycle      1313481
    Memory Throughput                 %        82.32
    DRAM Throughput                   %        11.95
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.53
    L2 Cache Throughput               %        14.63
    SM Active Cycles              cycle   1292406.78
    Compute (SM) Throughput           %        82.32
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203272
    Total DRAM Elapsed Cycles        cycle     60401664
    Average L1 Active Cycles         cycle   1292406.78
    Total L1 Elapsed Cycles          cycle     75148848
    Average L2 Active Cycles         cycle   1163290.54
    Total L2 Elapsed Cycles          cycle     31918176
    Average SM Active Cycles         cycle   1292406.78
    Total SM Elapsed Cycles          cycle     75148848
    Average SMSP Active Cycles       cycle   1292482.38
    Total SMSP Elapsed Cycles        cycle    300595392
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.01
    Elapsed Cycles                cycle       391202
    Memory Throughput                 %        87.57
    DRAM Throughput                   %        87.57
    Duration                         us       481.47
    L1/TEX Cache Throughput           %        49.21
    L2 Cache Throughput               %        58.13
    SM Active Cycles              cycle    377295.31
    Compute (SM) Throughput           %        30.90
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.43
    Achieved Active Warps Per SM           warp        46.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2632600
    Total DRAM Elapsed Cycles        cycle     18037760
    Average L1 Active Cycles         cycle    377295.31
    Total L1 Elapsed Cycles          cycle     22214760
    Average L2 Active Cycles         cycle    394640.33
    Total L2 Elapsed Cycles          cycle      9533856
    Average SM Active Cycles         cycle    377295.31
    Total SM Elapsed Cycles          cycle     22214760
    Average SMSP Active Cycles       cycle    380328.14
    Total SMSP Elapsed Cycles        cycle     88859040
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.87
    Elapsed Cycles                cycle      1212278
    Memory Throughput                 %        84.74
    DRAM Throughput                   %        23.13
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.48
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle   1157766.17
    Compute (SM) Throughput           %        84.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.02
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2148274.67
    Total DRAM Elapsed Cycles        cycle     55718912
    Average L1 Active Cycles         cycle   1157766.17
    Total L1 Elapsed Cycles          cycle     69324424
    Average L2 Active Cycles         cycle   1200460.54
    Total L2 Elapsed Cycles          cycle     29444424
    Average SM Active Cycles         cycle   1157766.17
    Total SM Elapsed Cycles          cycle     69324424
    Average SMSP Active Cycles       cycle   1157729.78
    Total SMSP Elapsed Cycles        cycle    277297696
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       784.83
    Elapsed Cycles                cycle         5513
    Memory Throughput                 %        28.86
    DRAM Throughput                   %        28.86
    Duration                         us         6.98
    L1/TEX Cache Throughput           %        19.29
    L2 Cache Throughput               %        15.73
    SM Active Cycles              cycle      2929.14
    Compute (SM) Throughput           %        11.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.66
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12312
    Total DRAM Elapsed Cycles        cycle       256000
    Average L1 Active Cycles         cycle      2929.14
    Total L1 Elapsed Cycles          cycle       291404
    Average L2 Active Cycles         cycle      2500.38
    Total L2 Elapsed Cycles          cycle       135552
    Average SM Active Cycles         cycle      2929.14
    Total SM Elapsed Cycles          cycle       291404
    Average SMSP Active Cycles       cycle      2908.06
    Total SMSP Elapsed Cycles        cycle      1165616
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.526%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 9.55% above the average, while the minimum instance value is 26.17% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.69
    Elapsed Cycles                cycle         5770
    Memory Throughput                 %        31.18
    DRAM Throughput                   %        31.18
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.25
    L2 Cache Throughput               %        17.66
    SM Active Cycles              cycle      3275.55
    Compute (SM) Throughput           %        10.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.13
    Achieved Active Warps Per SM           warp        34.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14048
    Total DRAM Elapsed Cycles        cycle       270336
    Average L1 Active Cycles         cycle      3275.55
    Total L1 Elapsed Cycles          cycle       323588
    Average L2 Active Cycles         cycle      2562.21
    Total L2 Elapsed Cycles          cycle       142056
    Average SM Active Cycles         cycle      3275.55
    Total SM Elapsed Cycles          cycle       323588
    Average SMSP Active Cycles       cycle      3061.08
    Total SMSP Elapsed Cycles        cycle      1294352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.023%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 10.26% above the average, while the minimum instance value is 11.59% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.494%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 11.84% above the average, while the minimum instance value is 12.68% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.023%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 10.26% above the average, while the minimum instance value is 11.59% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       801.34
    Elapsed Cycles                cycle         5818
    Memory Throughput                 %        31.67
    DRAM Throughput                   %        31.67
    Duration                         us         7.17
    L1/TEX Cache Throughput           %        17.35
    L2 Cache Throughput               %        17.94
    SM Active Cycles              cycle      3255.74
    Compute (SM) Throughput           %        10.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.77
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14000
    Total DRAM Elapsed Cycles        cycle       265216
    Average L1 Active Cycles         cycle      3255.74
    Total L1 Elapsed Cycles          cycle       312882
    Average L2 Active Cycles         cycle      2619.75
    Total L2 Elapsed Cycles          cycle       139872
    Average SM Active Cycles         cycle      3255.74
    Total SM Elapsed Cycles          cycle       312882
    Average SMSP Active Cycles       cycle      3165.08
    Total SMSP Elapsed Cycles        cycle      1251528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.11
    SM Frequency                    Mhz       807.64
    Elapsed Cycles                cycle         5762
    Memory Throughput                 %        32.59
    DRAM Throughput                   %        32.59
    Duration                         us         7.04
    L1/TEX Cache Throughput           %        17.32
    L2 Cache Throughput               %        18.23
    SM Active Cycles              cycle      3262.59
    Compute (SM) Throughput           %        10.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.38
    Achieved Active Warps Per SM           warp        34.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14016
    Total DRAM Elapsed Cycles        cycle       258048
    Average L1 Active Cycles         cycle      3262.59
    Total L1 Elapsed Cycles          cycle       312396
    Average L2 Active Cycles         cycle      2540.04
    Total L2 Elapsed Cycles          cycle       137448
    Average SM Active Cycles         cycle      3262.59
    Total SM Elapsed Cycles          cycle       312396
    Average SMSP Active Cycles       cycle      3092.94
    Total SMSP Elapsed Cycles        cycle      1249584
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.50
    Elapsed Cycles                cycle      1313610
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        12.33
    Duration                         ms         1.56
    L1/TEX Cache Throughput           %        82.52
    L2 Cache Throughput               %        14.91
    SM Active Cycles              cycle   1292556.81
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.57
    Achieved Active Warps Per SM           warp        46.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1203400
    Total DRAM Elapsed Cycles        cycle     58564608
    Average L1 Active Cycles         cycle   1292556.81
    Total L1 Elapsed Cycles          cycle     75126594
    Average L2 Active Cycles         cycle   1147249.79
    Total L2 Elapsed Cycles          cycle     31296528
    Average SM Active Cycles         cycle   1292556.81
    Total SM Elapsed Cycles          cycle     75126594
    Average SMSP Active Cycles       cycle   1292596.50
    Total SMSP Elapsed Cycles        cycle    300506376
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.34
    Elapsed Cycles                cycle       384803
    Memory Throughput                 %        87.38
    DRAM Throughput                   %        87.38
    Duration                         us       481.44
    L1/TEX Cache Throughput           %        49.58
    L2 Cache Throughput               %        58.10
    SM Active Cycles              cycle    374595.53
    Compute (SM) Throughput           %        31.13
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.31
    Achieved Active Warps Per SM           warp        46.23
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2626277.33
    Total DRAM Elapsed Cycles        cycle     18033664
    Average L1 Active Cycles         cycle    374595.53
    Total L1 Elapsed Cycles          cycle     22050946
    Average L2 Active Cycles         cycle    391795.54
    Total L2 Elapsed Cycles          cycle      9530616
    Average SM Active Cycles         cycle    374595.53
    Total SM Elapsed Cycles          cycle     22050946
    Average SMSP Active Cycles       cycle    375014.09
    Total SMSP Elapsed Cycles        cycle     88203784
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.90
    Elapsed Cycles                cycle      1212517
    Memory Throughput                 %        84.27
    DRAM Throughput                   %        23.14
    Duration                         ms         1.49
    L1/TEX Cache Throughput           %        87.52
    L2 Cache Throughput               %        13.96
    SM Active Cycles              cycle   1157277.78
    Compute (SM) Throughput           %        84.27
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.06
    Achieved Active Warps Per SM           warp        36.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2149333.33
    Total DRAM Elapsed Cycles        cycle     55738368
    Average L1 Active Cycles         cycle   1157277.78
    Total L1 Elapsed Cycles          cycle     69712352
    Average L2 Active Cycles         cycle   1204515.96
    Total L2 Elapsed Cycles          cycle     29455320
    Average SM Active Cycles         cycle   1157277.78
    Total SM Elapsed Cycles          cycle     69712352
    Average SMSP Active Cycles       cycle   1157647.09
    Total SMSP Elapsed Cycles        cycle    278849408
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.72
    Elapsed Cycles                cycle         5436
    Memory Throughput                 %        28.67
    DRAM Throughput                   %        28.67
    Duration                         us         6.88
    L1/TEX Cache Throughput           %        19.39
    L2 Cache Throughput               %        16.01
    SM Active Cycles              cycle      2914.12
    Compute (SM) Throughput           %        10.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.89
    Achieved Active Warps Per SM           warp        34.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12085.33
    Total DRAM Elapsed Cycles        cycle       252928
    Average L1 Active Cycles         cycle      2914.12
    Total L1 Elapsed Cycles          cycle       300120
    Average L2 Active Cycles         cycle      2451.29
    Total L2 Elapsed Cycles          cycle       133248
    Average SM Active Cycles         cycle      2914.12
    Total SM Elapsed Cycles          cycle       300120
    Average SMSP Active Cycles       cycle      2819.27
    Total SMSP Elapsed Cycles        cycle      1200480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.976%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 13.53% above the average, while the minimum instance value is 4.50% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.80
    Elapsed Cycles                cycle         5864
    Memory Throughput                 %        30.88
    DRAM Throughput                   %        30.88
    Duration                         us         7.39
    L1/TEX Cache Throughput           %        17.08
    L2 Cache Throughput               %        17.42
    SM Active Cycles              cycle      3307.09
    Compute (SM) Throughput           %        10.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.69
    Achieved Active Warps Per SM           warp        33.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14069.33
    Total DRAM Elapsed Cycles        cycle       273408
    Average L1 Active Cycles         cycle      3307.09
    Total L1 Elapsed Cycles          cycle       324422
    Average L2 Active Cycles         cycle      2589.54
    Total L2 Elapsed Cycles          cycle       143928
    Average SM Active Cycles         cycle      3307.09
    Total SM Elapsed Cycles          cycle       324422
    Average SMSP Active Cycles       cycle      3101.96
    Total SMSP Elapsed Cycles        cycle      1297688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.435%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 10.88% above the average, while the minimum instance value is 10.95% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.04%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 12.69% above the average, while the minimum instance value is 10.57% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.435%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 10.88% above the average, while the minimum instance value is 10.95% below  
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       788.16
    Elapsed Cycles                cycle         5802
    Memory Throughput                 %        31.51
    DRAM Throughput                   %        31.51
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        17.59
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3211.62
    Compute (SM) Throughput           %        10.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14088
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3211.62
    Total L1 Elapsed Cycles          cycle       310010
    Average L2 Active Cycles         cycle      2536.54
    Total L2 Elapsed Cycles          cycle       142176
    Average SM Active Cycles         cycle      3211.62
    Total SM Elapsed Cycles          cycle       310010
    Average SMSP Active Cycles       cycle      3048.44
    Total SMSP Elapsed Cycles        cycle      1240040
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       786.62
    Elapsed Cycles                cycle         5783
    Memory Throughput                 %        31.38
    DRAM Throughput                   %        31.38
    Duration                         us         7.30
    L1/TEX Cache Throughput           %        18.00
    L2 Cache Throughput               %        17.68
    SM Active Cycles              cycle      3138.69
    Compute (SM) Throughput           %        10.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.44
    Achieved Active Warps Per SM           warp        36.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        14032
    Total DRAM Elapsed Cycles        cycle       268288
    Average L1 Active Cycles         cycle      3138.69
    Total L1 Elapsed Cycles          cycle       305536
    Average L2 Active Cycles         cycle      2611.62
    Total L2 Elapsed Cycles          cycle       141960
    Average SM Active Cycles         cycle      3138.69
    Total SM Elapsed Cycles          cycle       305536
    Average SMSP Active Cycles       cycle      3138.42
    Total SMSP Elapsed Cycles        cycle      1222144
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (256, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.32
    Elapsed Cycles                cycle      1312641
    Memory Throughput                 %        82.35
    DRAM Throughput                   %        11.96
    Duration                         ms         1.61
    L1/TEX Cache Throughput           %        82.51
    L2 Cache Throughput               %        14.64
    SM Active Cycles              cycle   1292731.60
    Compute (SM) Throughput           %        82.35
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                  65536
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        16777216
    Uses Green Context                                             0
    Waves Per SM                                              188.32
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.53
    Achieved Active Warps Per SM           warp        46.33
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   1203466.67
    Total DRAM Elapsed Cycles        cycle     60374016
    Average L1 Active Cycles         cycle   1292731.60
    Total L1 Elapsed Cycles          cycle     75121812
    Average L2 Active Cycles         cycle   1164058.79
    Total L2 Elapsed Cycles          cycle     31904520
    Average SM Active Cycles         cycle   1292731.60
    Total SM Elapsed Cycles          cycle     75121812
    Average SMSP Active Cycles       cycle   1292595.45
    Total SMSP Elapsed Cycles        cycle    300487248
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (4096, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.07
    Elapsed Cycles                cycle       388583
    Memory Throughput                 %        87.66
    DRAM Throughput                   %        87.66
    Duration                         us       477.98
    L1/TEX Cache Throughput           %        47.50
    L2 Cache Throughput               %        58.42
    SM Active Cycles              cycle    390357.38
    Compute (SM) Throughput           %        29.84
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   4096
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         1048576
    Uses Green Context                                             0
    Waves Per SM                                               11.77
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        92.40
    Achieved Active Warps Per SM           warp        44.35
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   2615930.67
    Total DRAM Elapsed Cycles        cycle     17905664
    Average L1 Active Cycles         cycle    390357.38
    Total L1 Elapsed Cycles          cycle     23008196
    Average L2 Active Cycles         cycle    398988.92
    Total L2 Elapsed Cycles          cycle      9464784
    Average SM Active Cycles         cycle    390357.38
    Total SM Elapsed Cycles          cycle     23008196
    Average SMSP Active Cycles       cycle    391987.84
    Total SMSP Elapsed Cycles        cycle     92032784
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.22
    Elapsed Cycles                cycle      1219187
    Memory Throughput                 %        84.40
    DRAM Throughput                   %        23.57
    Duration                         ms         1.46
    L1/TEX Cache Throughput           %        87.45
    L2 Cache Throughput               %        14.14
    SM Active Cycles              cycle   1158223.09
    Compute (SM) Throughput           %        84.40
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 23.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.03
    Achieved Active Warps Per SM           warp        36.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      2148544
    Total DRAM Elapsed Cycles        cycle     54682624
    Average L1 Active Cycles         cycle   1158223.09
    Total L1 Elapsed Cycles          cycle     69603382
    Average L2 Active Cycles         cycle   1184293.38
    Total L2 Elapsed Cycles          cycle     29054040
    Average SM Active Cycles         cycle   1158223.09
    Total SM Elapsed Cycles          cycle     69603382
    Average SMSP Active Cycles       cycle   1157919.67
    Total SMSP Elapsed Cycles        cycle    278413528
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 256, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.77
    Elapsed Cycles                cycle         5410
    Memory Throughput                 %        28.92
    DRAM Throughput                   %        28.92
    Duration                         us         6.91
    L1/TEX Cache Throughput           %        19.23
    L2 Cache Throughput               %        15.85
    SM Active Cycles              cycle      2938.36
    Compute (SM) Throughput           %        11.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          131072
    Uses Green Context                                             0
    Waves Per SM                                                1.47
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 164 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.44
    Achieved Active Warps Per SM           warp        34.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        12288
    Total DRAM Elapsed Cycles        cycle       254976
    Average L1 Active Cycles         cycle      2938.36
    Total L1 Elapsed Cycles          cycle       292760
    Average L2 Active Cycles         cycle      2443.62
    Total L2 Elapsed Cycles          cycle       134664
    Average SM Active Cycles         cycle      2938.36
    Total SM Elapsed Cycles          cycle       292760
    Average SMSP Active Cycles       cycle      2890.57
    Total SMSP Elapsed Cycles        cycle      1171040
    -------------------------- ----------- ------------

