==PROF== Connected to process 26091 (/teamspace/studios/this_studio/QuantizedMHA/bin/profile_unfused)
==PROF== Profiling "extract_mat" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 240: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 241: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 242: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 243: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 244: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 245: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 246: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 247: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 248: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 249: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 250: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 251: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 252: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 253: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 254: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 255: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 256: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 257: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 258: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 259: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 260: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 261: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 262: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 263: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 264: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 265: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 266: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 267: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 268: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 269: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 270: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 271: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 272: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 273: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 274: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 275: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 276: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 277: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 278: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 279: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 280: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 281: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 282: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 283: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 284: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 285: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 286: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 287: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 288: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 289: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 290: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 291: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 292: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 293: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 294: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 295: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 296: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 297: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 298: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 299: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 300: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 301: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 302: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 303: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 304: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 305: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 306: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 307: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 308: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 309: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 310: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 311: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 312: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 313: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 314: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 315: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 316: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 317: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 318: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 319: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 320: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 321: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 322: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 323: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 324: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 325: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 326: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 327: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 328: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 329: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 330: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 331: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 332: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 333: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 334: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 335: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 336: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 337: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 338: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 339: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 340: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 341: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 342: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 343: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 344: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 345: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 346: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 347: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 348: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 349: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 350: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 351: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 352: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 353: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 354: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 355: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 356: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 357: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 358: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 359: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 360: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 361: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 362: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 363: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 364: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 365: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 366: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 367: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 368: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 369: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 370: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 371: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 372: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 373: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 374: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 375: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 376: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 377: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 378: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 379: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 380: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 381: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 382: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 383: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 384: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 385: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 386: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 387: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 388: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 389: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 390: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 391: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 392: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 393: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 394: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 395: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 396: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 397: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 398: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 399: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 400: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 401: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 402: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 403: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 404: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 405: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 406: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 407: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 408: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 409: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 410: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 411: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 412: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 413: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 414: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 415: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 416: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 417: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 418: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 419: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 420: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 421: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 422: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 423: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 424: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 425: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 426: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 427: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 428: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 429: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 430: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 431: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 432: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 433: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 434: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 435: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 436: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 437: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 438: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 439: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 440: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 441: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 442: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 443: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_Bt" - 444: 0%....50%....100% - 8 passes
==PROF== Profiling "softmax" - 445: 0%....50%....100% - 8 passes
==PROF== Profiling "mma_A_B" - 446: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 447: 0%....50%....100% - 8 passes
Initializing host data (constant values for correctness check)...
Running correctness check 
Loaded reference output from .cache/ref_N8192_d1024.bin
Correctness check PASSED.
Loaded input matrices from .cache/input_random_N8192_d1024.bin
Running 0 warmup iterations...
Running 1 profiling iterations...
Profiling complete.
==PROF== Disconnected from process 26091
[26091] profile_unfused@127.0.0.1
  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       813.31
    Elapsed Cycles                cycle         7244
    Memory Throughput                 %        50.86
    DRAM Throughput                   %        50.86
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        23.08
    L2 Cache Throughput               %        28.31
    SM Active Cycles              cycle      5164.98
    Compute (SM) Throughput           %        15.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.92
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      5164.98
    Total L1 Elapsed Cycles          cycle       425728
    Average L2 Active Cycles         cycle      4443.25
    Total L2 Elapsed Cycles          cycle       175176
    Average SM Active Cycles         cycle      5164.98
    Total SM Elapsed Cycles          cycle       425728
    Average SMSP Active Cycles       cycle      4965.52
    Total SMSP Elapsed Cycles        cycle      1702912
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       789.07
    Elapsed Cycles                cycle         7540
    Memory Throughput                 %        47.12
    DRAM Throughput                   %        47.12
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.94
    L2 Cache Throughput               %        26.55
    SM Active Cycles              cycle      4863.17
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.76
    Achieved Active Warps Per SM           warp        36.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       354304
    Average L1 Active Cycles         cycle      4863.17
    Total L1 Elapsed Cycles          cycle       410170
    Average L2 Active Cycles         cycle      4447.04
    Total L2 Elapsed Cycles          cycle       186816
    Average SM Active Cycles         cycle      4863.17
    Total SM Elapsed Cycles          cycle       410170
    Average SMSP Active Cycles       cycle      4878.98
    Total SMSP Elapsed Cycles        cycle      1640680
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       817.24
    Elapsed Cycles                cycle         7618
    Memory Throughput                 %        48.62
    DRAM Throughput                   %        48.62
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      5024.41
    Compute (SM) Throughput           %        15.79
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.14
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27712
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5024.41
    Total L1 Elapsed Cycles          cycle       415092
    Average L2 Active Cycles         cycle      4510.71
    Total L2 Elapsed Cycles          cycle       184152
    Average SM Active Cycles         cycle      5024.41
    Total SM Elapsed Cycles          cycle       415092
    Average SMSP Active Cycles       cycle      5002.79
    Total SMSP Elapsed Cycles        cycle      1660368
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.39
    Elapsed Cycles                cycle      5185780
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.41
    Duration                         ms         6.15
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160461.55
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301490.67
    Total DRAM Elapsed Cycles        cycle    230438912
    Average L1 Active Cycles         cycle   5160461.55
    Total L1 Elapsed Cycles          cycle    299476896
    Average L2 Active Cycles         cycle      4610261
    Total L2 Elapsed Cycles          cycle    125458848
    Average SM Active Cycles         cycle   5160461.55
    Total SM Elapsed Cycles          cycle    299476896
    Average SMSP Active Cycles       cycle   5160644.02
    Total SMSP Elapsed Cycles        cycle   1197907584
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       840.98
    Elapsed Cycles                cycle      1852153
    Memory Throughput                 %        89.97
    DRAM Throughput                   %        89.97
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.67
    L2 Cache Throughput               %        49.57
    SM Active Cycles              cycle   1730983.17
    Compute (SM) Throughput           %        23.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       102.95
    Achieved Active Warps Per SM           warp        49.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12316253.33
    Total DRAM Elapsed Cycles        cycle     82131968
    Average L1 Active Cycles         cycle   1730983.17
    Total L1 Elapsed Cycles          cycle    101203210
    Average L2 Active Cycles         cycle   1797844.33
    Total L2 Elapsed Cycles          cycle     44844456
    Average SM Active Cycles         cycle   1730983.17
    Total SM Elapsed Cycles          cycle    101203210
    Average SMSP Active Cycles       cycle   1732328.44
    Total SMSP Elapsed Cycles        cycle    404812840
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       823.93
    Elapsed Cycles                cycle      4543880
    Memory Throughput                 %        89.64
    DRAM Throughput                   %        25.03
    Duration                         ms         5.49
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4414870.41
    Compute (SM) Throughput           %        89.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8579682.67
    Total DRAM Elapsed Cycles        cycle    205625344
    Average L1 Active Cycles         cycle   4414870.41
    Total L1 Elapsed Cycles          cycle    262082530
    Average L2 Active Cycles         cycle   4518783.67
    Total L2 Elapsed Cycles          cycle    109961520
    Average SM Active Cycles         cycle   4414870.41
    Total SM Elapsed Cycles          cycle    262082530
    Average SMSP Active Cycles       cycle   4413506.01
    Total SMSP Elapsed Cycles        cycle   1048330120
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       802.31
    Elapsed Cycles                cycle         7043
    Memory Throughput                 %        44.35
    DRAM Throughput                   %        44.35
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        25.08
    L2 Cache Throughput               %        24.46
    SM Active Cycles              cycle      4505.50
    Compute (SM) Throughput           %        17.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.48
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23842.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4505.50
    Total L1 Elapsed Cycles          cycle       382880
    Average L2 Active Cycles         cycle      4011.79
    Total L2 Elapsed Cycles          cycle       171360
    Average SM Active Cycles         cycle      4505.50
    Total SM Elapsed Cycles          cycle       382880
    Average SMSP Active Cycles       cycle      4477.46
    Total SMSP Elapsed Cycles        cycle      1531520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       797.76
    Elapsed Cycles                cycle         7440
    Memory Throughput                 %        48.38
    DRAM Throughput                   %        48.38
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.32
    L2 Cache Throughput               %        27.34
    SM Active Cycles              cycle      4976.67
    Compute (SM) Throughput           %        15.56
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4976.67
    Total L1 Elapsed Cycles          cycle       421136
    Average L2 Active Cycles         cycle      4475.12
    Total L2 Elapsed Cycles          cycle       181536
    Average SM Active Cycles         cycle      4976.67
    Total SM Elapsed Cycles          cycle       421136
    Average SMSP Active Cycles       cycle      4958.88
    Total SMSP Elapsed Cycles        cycle      1684544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       796.61
    Elapsed Cycles                cycle         7556
    Memory Throughput                 %        47.87
    DRAM Throughput                   %        47.87
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      5241.83
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.11
    Achieved Active Warps Per SM           warp        33.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5241.83
    Total L1 Elapsed Cycles          cycle       412878
    Average L2 Active Cycles         cycle      4361.21
    Total L2 Elapsed Cycles          cycle       184608
    Average SM Active Cycles         cycle      5241.83
    Total SM Elapsed Cycles          cycle       412878
    Average SMSP Active Cycles       cycle      4821.64
    Total SMSP Elapsed Cycles        cycle      1651512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.74
    Elapsed Cycles                cycle         7507
    Memory Throughput                 %        48.30
    DRAM Throughput                   %        48.30
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        24.05
    L2 Cache Throughput               %        27.14
    SM Active Cycles              cycle      5024.16
    Compute (SM) Throughput           %        16.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.92
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5024.16
    Total L1 Elapsed Cycles          cycle       408476
    Average L2 Active Cycles         cycle      4521.92
    Total L2 Elapsed Cycles          cycle       182736
    Average SM Active Cycles         cycle      5024.16
    Total SM Elapsed Cycles          cycle       408476
    Average SMSP Active Cycles       cycle      5001.44
    Total SMSP Elapsed Cycles        cycle      1633904
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.54
    Elapsed Cycles                cycle      5186847
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        15.72
    Duration                         ms         6.42
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.20
    SM Active Cycles              cycle   5160249.31
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6307960
    Total DRAM Elapsed Cycles        cycle    240703488
    Average L1 Active Cycles         cycle   5160249.31
    Total L1 Elapsed Cycles          cycle    299470784
    Average L2 Active Cycles         cycle   4658002.83
    Total L2 Elapsed Cycles          cycle    127197984
    Average SM Active Cycles         cycle   5160249.31
    Total SM Elapsed Cycles          cycle    299470784
    Average SMSP Active Cycles       cycle   5159884.95
    Total SMSP Elapsed Cycles        cycle   1197883136
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.88
    Elapsed Cycles                cycle      1769554
    Memory Throughput                 %        90.16
    DRAM Throughput                   %        90.16
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.40
    L2 Cache Throughput               %        51.23
    SM Active Cycles              cycle   1765314.09
    Compute (SM) Throughput           %        22.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.48
    Achieved Active Warps Per SM           warp        46.31
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12338458.67
    Total DRAM Elapsed Cycles        cycle     82111488
    Average L1 Active Cycles         cycle   1765314.09
    Total L1 Elapsed Cycles          cycle    106830588
    Average L2 Active Cycles         cycle   1800881.54
    Total L2 Elapsed Cycles          cycle     43410168
    Average SM Active Cycles         cycle   1765314.09
    Total SM Elapsed Cycles          cycle    106830588
    Average SMSP Active Cycles       cycle   1758569.04
    Total SMSP Elapsed Cycles        cycle    427322352
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       823.85
    Elapsed Cycles                cycle      4540103
    Memory Throughput                 %        89.93
    DRAM Throughput                   %        25.04
    Duration                         ms         5.49
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4413523.02
    Compute (SM) Throughput           %        89.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8577490.67
    Total DRAM Elapsed Cycles        cycle    205516800
    Average L1 Active Cycles         cycle   4413523.02
    Total L1 Elapsed Cycles          cycle    261227544
    Average L2 Active Cycles         cycle   4526622.17
    Total L2 Elapsed Cycles          cycle    109887888
    Average SM Active Cycles         cycle   4413523.02
    Total SM Elapsed Cycles          cycle    261227544
    Average SMSP Active Cycles       cycle   4412373.36
    Total SMSP Elapsed Cycles        cycle   1044910176
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.42
    Elapsed Cycles                cycle         6991
    Memory Throughput                 %        44.08
    DRAM Throughput                   %        44.08
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        25.05
    L2 Cache Throughput               %        24.21
    SM Active Cycles              cycle      4510.05
    Compute (SM) Throughput           %        17.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.01
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24072
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4510.05
    Total L1 Elapsed Cycles          cycle       372074
    Average L2 Active Cycles         cycle      4110.38
    Total L2 Elapsed Cycles          cycle       172896
    Average SM Active Cycles         cycle      4510.05
    Total SM Elapsed Cycles          cycle       372074
    Average SMSP Active Cycles       cycle      4466.03
    Total SMSP Elapsed Cycles        cycle      1488296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.54
    Elapsed Cycles                cycle         7414
    Memory Throughput                 %        47.60
    DRAM Throughput                   %        47.60
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.64
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      4981.43
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.67
    Achieved Active Warps Per SM           warp        34.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4981.43
    Total L1 Elapsed Cycles          cycle       414546
    Average L2 Active Cycles         cycle      4385.92
    Total L2 Elapsed Cycles          cycle       183672
    Average SM Active Cycles         cycle      4981.43
    Total SM Elapsed Cycles          cycle       414546
    Average SMSP Active Cycles       cycle      4802.70
    Total SMSP Elapsed Cycles        cycle      1658184
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       789.46
    Elapsed Cycles                cycle         7447
    Memory Throughput                 %        47.84
    DRAM Throughput                   %        47.84
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      4981.59
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.86
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      4981.59
    Total L1 Elapsed Cycles          cycle       407684
    Average L2 Active Cycles         cycle      4559.50
    Total L2 Elapsed Cycles          cycle       184296
    Average SM Active Cycles         cycle      4981.59
    Total SM Elapsed Cycles          cycle       407684
    Average SMSP Active Cycles       cycle      4967.91
    Total SMSP Elapsed Cycles        cycle      1630736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       788.03
    Elapsed Cycles                cycle         7505
    Memory Throughput                 %        47.44
    DRAM Throughput                   %        47.44
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        26.63
    SM Active Cycles              cycle      4996.93
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4996.93
    Total L1 Elapsed Cycles          cycle       405176
    Average L2 Active Cycles         cycle      4479.79
    Total L2 Elapsed Cycles          cycle       185856
    Average SM Active Cycles         cycle      4996.93
    Total SM Elapsed Cycles          cycle       405176
    Average SMSP Active Cycles       cycle      4896.90
    Total SMSP Elapsed Cycles        cycle      1620704
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       840.24
    Elapsed Cycles                cycle      5184842
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.43
    Duration                         ms         6.14
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160314.81
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302394.67
    Total DRAM Elapsed Cycles        cycle    230197248
    Average L1 Active Cycles         cycle   5160314.81
    Total L1 Elapsed Cycles          cycle    299472012
    Average L2 Active Cycles         cycle   4610150.12
    Total L2 Elapsed Cycles          cycle    125439384
    Average SM Active Cycles         cycle   5160314.81
    Total SM Elapsed Cycles          cycle    299472012
    Average SMSP Active Cycles       cycle   5160380.53
    Total SMSP Elapsed Cycles        cycle   1197888048
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.13
    Elapsed Cycles                cycle      1850647
    Memory Throughput                 %        90.70
    DRAM Throughput                   %        90.70
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        41.38
    L2 Cache Throughput               %        49.64
    SM Active Cycles              cycle   1835888.40
    Compute (SM) Throughput           %        22.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.84
    Achieved Active Warps Per SM           warp        46.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12364328
    Total DRAM Elapsed Cycles        cycle     81790976
    Average L1 Active Cycles         cycle   1835888.40
    Total L1 Elapsed Cycles          cycle    106811544
    Average L2 Active Cycles         cycle   1850264.88
    Total L2 Elapsed Cycles          cycle     44782512
    Average SM Active Cycles         cycle   1835888.40
    Total SM Elapsed Cycles          cycle    106811544
    Average SMSP Active Cycles       cycle   1828920.19
    Total SMSP Elapsed Cycles        cycle    427246176
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       823.61
    Elapsed Cycles                cycle      4537010
    Memory Throughput                 %        89.65
    DRAM Throughput                   %        25.06
    Duration                         ms         5.48
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4414870.19
    Compute (SM) Throughput           %        89.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8582714.67
    Total DRAM Elapsed Cycles        cycle    205494272
    Average L1 Active Cycles         cycle   4414870.19
    Total L1 Elapsed Cycles          cycle    262057462
    Average L2 Active Cycles         cycle   4579602.33
    Total L2 Elapsed Cycles          cycle    109831752
    Average SM Active Cycles         cycle   4414870.19
    Total SM Elapsed Cycles          cycle    262057462
    Average SMSP Active Cycles       cycle   4414866.72
    Total SMSP Elapsed Cycles        cycle   1048229848
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       800.38
    Elapsed Cycles                cycle         7153
    Memory Throughput                 %        43.42
    DRAM Throughput                   %        43.42
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        24.80
    L2 Cache Throughput               %        24.11
    SM Active Cycles              cycle      4556.84
    Compute (SM) Throughput           %        17.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23786.67
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4556.84
    Total L1 Elapsed Cycles          cycle       385346
    Average L2 Active Cycles         cycle      3899.62
    Total L2 Elapsed Cycles          cycle       174048
    Average SM Active Cycles         cycle      4556.84
    Total SM Elapsed Cycles          cycle       385346
    Average SMSP Active Cycles       cycle      4390.25
    Total SMSP Elapsed Cycles        cycle      1541384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       805.51
    Elapsed Cycles                cycle         7639
    Memory Throughput                 %        47.78
    DRAM Throughput                   %        47.78
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        26.67
    SM Active Cycles              cycle      5025.95
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.59
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27885.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5025.95
    Total L1 Elapsed Cycles          cycle       413338
    Average L2 Active Cycles         cycle      4601.79
    Total L2 Elapsed Cycles          cycle       185592
    Average SM Active Cycles         cycle      5025.95
    Total SM Elapsed Cycles          cycle       413338
    Average SMSP Active Cycles       cycle      5132.83
    Total SMSP Elapsed Cycles        cycle      1653352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       790.03
    Elapsed Cycles                cycle         7374
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        24.40
    L2 Cache Throughput               %        27.13
    SM Active Cycles              cycle      4985.57
    Compute (SM) Throughput           %        16.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.61
    Achieved Active Warps Per SM           warp        34.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4985.57
    Total L1 Elapsed Cycles          cycle       401974
    Average L2 Active Cycles         cycle      4499.25
    Total L2 Elapsed Cycles          cycle       182424
    Average SM Active Cycles         cycle      4985.57
    Total SM Elapsed Cycles          cycle       401974
    Average SMSP Active Cycles       cycle      4911.53
    Total SMSP Elapsed Cycles        cycle      1607896
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       787.50
    Elapsed Cycles                cycle         7397
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        27.03
    SM Active Cycles              cycle      4904.07
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4904.07
    Total L1 Elapsed Cycles          cycle       407160
    Average L2 Active Cycles         cycle      4481.08
    Total L2 Elapsed Cycles          cycle       183240
    Average SM Active Cycles         cycle      4904.07
    Total SM Elapsed Cycles          cycle       407160
    Average SMSP Active Cycles       cycle      4906.49
    Total SMSP Elapsed Cycles        cycle      1628640
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.62
    Elapsed Cycles                cycle      5185774
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.37
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160428.29
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6309338.67
    Total DRAM Elapsed Cycles        cycle    231196672
    Average L1 Active Cycles         cycle   5160428.29
    Total L1 Elapsed Cycles          cycle    299459036
    Average L2 Active Cycles         cycle   4611327.38
    Total L2 Elapsed Cycles          cycle    125410104
    Average SM Active Cycles         cycle   5160428.29
    Total SM Elapsed Cycles          cycle    299459036
    Average SMSP Active Cycles       cycle   5160144.76
    Total SMSP Elapsed Cycles        cycle   1197836144
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.99
    Elapsed Cycles                cycle      1758791
    Memory Throughput                 %        89.59
    DRAM Throughput                   %        89.59
    Duration                         ms         2.21
    L1/TEX Cache Throughput           %        41.27
    L2 Cache Throughput               %        50.91
    SM Active Cycles              cycle   1741958.71
    Compute (SM) Throughput           %        22.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.41
    Achieved Active Warps Per SM           warp        46.76
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12346882.67
    Total DRAM Elapsed Cycles        cycle     82692096
    Average L1 Active Cycles         cycle   1741958.71
    Total L1 Elapsed Cycles          cycle    107075494
    Average L2 Active Cycles         cycle   1809275.25
    Total L2 Elapsed Cycles          cycle     43698864
    Average SM Active Cycles         cycle   1741958.71
    Total SM Elapsed Cycles          cycle    107075494
    Average SMSP Active Cycles       cycle   1742994.09
    Total SMSP Elapsed Cycles        cycle    428301976
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.82
    Elapsed Cycles                cycle      4542553
    Memory Throughput                 %        89.47
    DRAM Throughput                   %        25.22
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        91.74
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4415109.38
    Compute (SM) Throughput           %        89.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8578709.33
    Total DRAM Elapsed Cycles        cycle    204115968
    Average L1 Active Cycles         cycle   4415109.38
    Total L1 Elapsed Cycles          cycle    262590040
    Average L2 Active Cycles         cycle   4525370.54
    Total L2 Elapsed Cycles          cycle    109902600
    Average SM Active Cycles         cycle   4415109.38
    Total SM Elapsed Cycles          cycle    262590040
    Average SMSP Active Cycles       cycle   4413219.78
    Total SMSP Elapsed Cycles        cycle   1050360160
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       827.20
    Elapsed Cycles                cycle         7101
    Memory Throughput                 %        45.57
    DRAM Throughput                   %        45.57
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.20
    L2 Cache Throughput               %        24.43
    SM Active Cycles              cycle      4483.45
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.82
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4483.45
    Total L1 Elapsed Cycles          cycle       386046
    Average L2 Active Cycles         cycle      3964.33
    Total L2 Elapsed Cycles          cycle       171576
    Average SM Active Cycles         cycle      4483.45
    Total SM Elapsed Cycles          cycle       386046
    Average SMSP Active Cycles       cycle      4477.25
    Total SMSP Elapsed Cycles        cycle      1544184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.638%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.17% above the average, while the minimum instance value is 3.01% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       804.27
    Elapsed Cycles                cycle         7838
    Memory Throughput                 %        46.11
    DRAM Throughput                   %        46.11
    Duration                         us         9.70
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        26.03
    SM Active Cycles              cycle      4876.40
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 22.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.09
    Achieved Active Warps Per SM           warp        37.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27624
    Total DRAM Elapsed Cycles        cycle       359424
    Average L1 Active Cycles         cycle      4876.40
    Total L1 Elapsed Cycles          cycle       412528
    Average L2 Active Cycles         cycle      4470.54
    Total L2 Elapsed Cycles          cycle       190200
    Average SM Active Cycles         cycle      4876.40
    Total SM Elapsed Cycles          cycle       412528
    Average SMSP Active Cycles       cycle      4867.47
    Total SMSP Elapsed Cycles        cycle      1650112
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.36
    Elapsed Cycles                cycle         7443
    Memory Throughput                 %        47.86
    DRAM Throughput                   %        47.86
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        24.13
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      4998.86
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.68
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      4998.86
    Total L1 Elapsed Cycles          cycle       406370
    Average L2 Active Cycles         cycle      4549.17
    Total L2 Elapsed Cycles          cycle       184152
    Average SM Active Cycles         cycle      4998.86
    Total SM Elapsed Cycles          cycle       406370
    Average SMSP Active Cycles       cycle      4957.93
    Total SMSP Elapsed Cycles        cycle      1625480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       825.78
    Elapsed Cycles                cycle         7645
    Memory Throughput                 %        48.74
    DRAM Throughput                   %        48.74
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        26.80
    SM Active Cycles              cycle      5036.66
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.26
    Achieved Active Warps Per SM           warp        36.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27698.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5036.66
    Total L1 Elapsed Cycles          cycle       417534
    Average L2 Active Cycles         cycle      4439.54
    Total L2 Elapsed Cycles          cycle       184872
    Average SM Active Cycles         cycle      5036.66
    Total SM Elapsed Cycles          cycle       417534
    Average SMSP Active Cycles       cycle      4929.06
    Total SMSP Elapsed Cycles        cycle      1670136
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.78
    Elapsed Cycles                cycle      5185840
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.36
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.67
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160776.09
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302701.33
    Total DRAM Elapsed Cycles        cycle    231164928
    Average L1 Active Cycles         cycle   5160776.09
    Total L1 Elapsed Cycles          cycle    299466058
    Average L2 Active Cycles         cycle      4610507
    Total L2 Elapsed Cycles          cycle    125470488
    Average SM Active Cycles         cycle   5160776.09
    Total SM Elapsed Cycles          cycle    299466058
    Average SMSP Active Cycles       cycle      5160598
    Total SMSP Elapsed Cycles        cycle   1197864232
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.94
    Elapsed Cycles                cycle      1756132
    Memory Throughput                 %        89.45
    DRAM Throughput                   %        89.45
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.37
    L2 Cache Throughput               %        50.92
    SM Active Cycles              cycle   1760698.72
    Compute (SM) Throughput           %        22.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.46
    Achieved Active Warps Per SM           warp        46.30
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12310352
    Total DRAM Elapsed Cycles        cycle     82575360
    Average L1 Active Cycles         cycle   1760698.72
    Total L1 Elapsed Cycles          cycle    106828652
    Average L2 Active Cycles         cycle      1858114
    Total L2 Elapsed Cycles          cycle     43638336
    Average SM Active Cycles         cycle   1760698.72
    Total SM Elapsed Cycles          cycle    106828652
    Average SMSP Active Cycles       cycle   1835621.04
    Total SMSP Elapsed Cycles        cycle    427314608
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.97
    Elapsed Cycles                cycle      4540234
    Memory Throughput                 %        89.60
    DRAM Throughput                   %        25.09
    Duration                         ms         5.48
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.20
    SM Active Cycles              cycle   4414540.05
    Compute (SM) Throughput           %        89.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8583146.67
    Total DRAM Elapsed Cycles        cycle    205232128
    Average L1 Active Cycles         cycle   4414540.05
    Total L1 Elapsed Cycles          cycle    262199368
    Average L2 Active Cycles         cycle   4521718.88
    Total L2 Elapsed Cycles          cycle    109845528
    Average SM Active Cycles         cycle   4414540.05
    Total SM Elapsed Cycles          cycle    262199368
    Average SMSP Active Cycles       cycle   4415370.48
    Total SMSP Elapsed Cycles        cycle   1048797472
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.64
    Elapsed Cycles                cycle         6954
    Memory Throughput                 %        43.61
    DRAM Throughput                   %        43.61
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        25.11
    L2 Cache Throughput               %        24.21
    SM Active Cycles              cycle      4500.57
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.40
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23818.67
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4500.57
    Total L1 Elapsed Cycles          cycle       383996
    Average L2 Active Cycles         cycle      4022.04
    Total L2 Elapsed Cycles          cycle       173088
    Average SM Active Cycles         cycle      4500.57
    Total SM Elapsed Cycles          cycle       383996
    Average SMSP Active Cycles       cycle      4384.91
    Total SMSP Elapsed Cycles        cycle      1535984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.29
    Elapsed Cycles                cycle         7381
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      5047.19
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.60
    Achieved Active Warps Per SM           warp        34.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5047.19
    Total L1 Elapsed Cycles          cycle       410246
    Average L2 Active Cycles         cycle      4502.04
    Total L2 Elapsed Cycles          cycle       183816
    Average SM Active Cycles         cycle      5047.19
    Total SM Elapsed Cycles          cycle       410246
    Average SMSP Active Cycles       cycle      4974.82
    Total SMSP Elapsed Cycles        cycle      1640984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       802.95
    Elapsed Cycles                cycle         7545
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        27.02
    SM Active Cycles              cycle      5134.45
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.13
    Achieved Active Warps Per SM           warp        33.66
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5134.45
    Total L1 Elapsed Cycles          cycle       413852
    Average L2 Active Cycles         cycle      4475.96
    Total L2 Elapsed Cycles          cycle       183288
    Average SM Active Cycles         cycle      5134.45
    Total SM Elapsed Cycles          cycle       413852
    Average SMSP Active Cycles       cycle      4930.31
    Total SMSP Elapsed Cycles        cycle      1655408
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       812.29
    Elapsed Cycles                cycle         7654
    Memory Throughput                 %        47.93
    DRAM Throughput                   %        47.93
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        26.76
    SM Active Cycles              cycle      5020.76
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.15
    Achieved Active Warps Per SM           warp        35.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5020.76
    Total L1 Elapsed Cycles          cycle       410594
    Average L2 Active Cycles         cycle      4417.96
    Total L2 Elapsed Cycles          cycle       184920
    Average SM Active Cycles         cycle      5020.76
    Total SM Elapsed Cycles          cycle       410594
    Average SMSP Active Cycles       cycle      4910.56
    Total SMSP Elapsed Cycles        cycle      1642376
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       837.42
    Elapsed Cycles                cycle      5184768
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.37
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.41
    SM Active Cycles              cycle   5160306.19
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301786.67
    Total DRAM Elapsed Cycles        cycle    230978560
    Average L1 Active Cycles         cycle   5160306.19
    Total L1 Elapsed Cycles          cycle    299468186
    Average L2 Active Cycles         cycle   4609318.21
    Total L2 Elapsed Cycles          cycle    125397600
    Average SM Active Cycles         cycle   5160306.19
    Total SM Elapsed Cycles          cycle    299468186
    Average SMSP Active Cycles       cycle   5160268.55
    Total SMSP Elapsed Cycles        cycle   1197872744
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.67
    Elapsed Cycles                cycle      1778230
    Memory Throughput                 %        89.80
    DRAM Throughput                   %        89.80
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        42.93
    L2 Cache Throughput               %        51.01
    SM Active Cycles              cycle   1761275.03
    Compute (SM) Throughput           %        23.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.13
    Achieved Active Warps Per SM           warp        46.62
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12343194.67
    Total DRAM Elapsed Cycles        cycle     82470912
    Average L1 Active Cycles         cycle   1761275.03
    Total L1 Elapsed Cycles          cycle    103039902
    Average L2 Active Cycles         cycle   1805386.25
    Total L2 Elapsed Cycles          cycle     43600608
    Average SM Active Cycles         cycle   1761275.03
    Total SM Elapsed Cycles          cycle    103039902
    Average SMSP Active Cycles       cycle   1765752.62
    Total SMSP Elapsed Cycles        cycle    412159608
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       804.43
    Elapsed Cycles                cycle      4528220
    Memory Throughput                 %        89.76
    DRAM Throughput                   %        24.52
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.74
    L2 Cache Throughput               %        15.06
    SM Active Cycles              cycle   4415456.43
    Compute (SM) Throughput           %        89.76
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.04
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577168
    Total DRAM Elapsed Cycles        cycle    209900544
    Average L1 Active Cycles         cycle   4415456.43
    Total L1 Elapsed Cycles          cycle    261726300
    Average L2 Active Cycles         cycle   4516123.21
    Total L2 Elapsed Cycles          cycle    110920560
    Average SM Active Cycles         cycle   4415456.43
    Total SM Elapsed Cycles          cycle    261726300
    Average SMSP Active Cycles       cycle   4412662.52
    Total SMSP Elapsed Cycles        cycle   1046905200
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.51
    Elapsed Cycles                cycle         6939
    Memory Throughput                 %        45.01
    DRAM Throughput                   %        45.01
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.71
    L2 Cache Throughput               %        24.82
    SM Active Cycles              cycle      4573.53
    Compute (SM) Throughput           %        17.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.10
    Achieved Active Warps Per SM           warp        33.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24042.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4573.53
    Total L1 Elapsed Cycles          cycle       384052
    Average L2 Active Cycles         cycle      4094.12
    Total L2 Elapsed Cycles          cycle       168912
    Average SM Active Cycles         cycle      4573.53
    Total SM Elapsed Cycles          cycle       384052
    Average SMSP Active Cycles       cycle      4493.90
    Total SMSP Elapsed Cycles        cycle      1536208
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.493%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.44% above the average, while the minimum instance value is 2.96% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.90
    Elapsed Cycles                cycle         7677
    Memory Throughput                 %        47.06
    DRAM Throughput                   %        47.06
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        26.49
    SM Active Cycles              cycle      5046.53
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.55
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27706.67
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5046.53
    Total L1 Elapsed Cycles          cycle       412394
    Average L2 Active Cycles         cycle      4495.33
    Total L2 Elapsed Cycles          cycle       186888
    Average SM Active Cycles         cycle      5046.53
    Total SM Elapsed Cycles          cycle       412394
    Average SMSP Active Cycles       cycle      4962.60
    Total SMSP Elapsed Cycles        cycle      1649576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.499%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.88% above the average, while the minimum instance value is 8.27% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       803.11
    Elapsed Cycles                cycle         7616
    Memory Throughput                 %        47.94
    DRAM Throughput                   %        47.94
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        22.67
    L2 Cache Throughput               %        26.79
    SM Active Cycles              cycle      4984.48
    Compute (SM) Throughput           %        15.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.12
    Achieved Active Warps Per SM           warp        36.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      4984.48
    Total L1 Elapsed Cycles          cycle       433580
    Average L2 Active Cycles         cycle      4424.29
    Total L2 Elapsed Cycles          cycle       184944
    Average SM Active Cycles         cycle      4984.48
    Total SM Elapsed Cycles          cycle       433580
    Average SMSP Active Cycles       cycle      4916.11
    Total SMSP Elapsed Cycles        cycle      1734320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.14
    Elapsed Cycles                cycle         7468
    Memory Throughput                 %        47.51
    DRAM Throughput                   %        47.51
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        26.65
    SM Active Cycles              cycle      5046.76
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.04
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5046.76
    Total L1 Elapsed Cycles          cycle       410770
    Average L2 Active Cycles         cycle      4497.33
    Total L2 Elapsed Cycles          cycle       185904
    Average SM Active Cycles         cycle      5046.76
    Total SM Elapsed Cycles          cycle       410770
    Average SMSP Active Cycles       cycle      4924.84
    Total SMSP Elapsed Cycles        cycle      1643080
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.47
    Elapsed Cycles                cycle      5185141
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.39
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160580.55
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6303232
    Total DRAM Elapsed Cycles        cycle    230689792
    Average L1 Active Cycles         cycle   5160580.55
    Total L1 Elapsed Cycles          cycle    299462084
    Average L2 Active Cycles         cycle   4609646.46
    Total L2 Elapsed Cycles          cycle    125472672
    Average SM Active Cycles         cycle   5160580.55
    Total SM Elapsed Cycles          cycle    299462084
    Average SMSP Active Cycles       cycle   5160019.34
    Total SMSP Elapsed Cycles        cycle   1197848336
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.34
    Elapsed Cycles                cycle      1846889
    Memory Throughput                 %        90.84
    DRAM Throughput                   %        90.84
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        41.46
    L2 Cache Throughput               %        49.70
    SM Active Cycles              cycle   1822604.74
    Compute (SM) Throughput           %        22.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.25
    Achieved Active Warps Per SM           warp        46.68
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12356354.67
    Total DRAM Elapsed Cycles        cycle     81610752
    Average L1 Active Cycles         cycle   1822604.74
    Total L1 Elapsed Cycles          cycle    106614486
    Average L2 Active Cycles         cycle   1847974.17
    Total L2 Elapsed Cycles          cycle     44703696
    Average SM Active Cycles         cycle   1822604.74
    Total SM Elapsed Cycles          cycle    106614486
    Average SMSP Active Cycles       cycle   1826936.88
    Total SMSP Elapsed Cycles        cycle    426457944
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       817.93
    Elapsed Cycles                cycle      4529260
    Memory Throughput                 %        89.51
    DRAM Throughput                   %        24.95
    Duration                         ms         5.51
    L1/TEX Cache Throughput           %        91.70
    L2 Cache Throughput               %        15.20
    SM Active Cycles              cycle   4417111.31
    Compute (SM) Throughput           %        89.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.01
    Achieved Active Warps Per SM           warp        42.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8586930.67
    Total DRAM Elapsed Cycles        cycle    206484480
    Average L1 Active Cycles         cycle   4417111.31
    Total L1 Elapsed Cycles          cycle    262463162
    Average L2 Active Cycles         cycle   4576419.08
    Total L2 Elapsed Cycles          cycle    109898664
    Average SM Active Cycles         cycle   4417111.31
    Total SM Elapsed Cycles          cycle    262463162
    Average SMSP Active Cycles       cycle   4414235.50
    Total SMSP Elapsed Cycles        cycle   1049852648
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       800.41
    Elapsed Cycles                cycle         7050
    Memory Throughput                 %        44.32
    DRAM Throughput                   %        44.32
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        24.43
    SM Active Cycles              cycle      4659.90
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.06
    Achieved Active Warps Per SM           warp        33.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23826.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4659.90
    Total L1 Elapsed Cycles          cycle       386044
    Average L2 Active Cycles         cycle      3939.46
    Total L2 Elapsed Cycles          cycle       171600
    Average SM Active Cycles         cycle      4659.90
    Total SM Elapsed Cycles          cycle       386044
    Average SMSP Active Cycles       cycle      4406.80
    Total SMSP Elapsed Cycles        cycle      1544176
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       832.66
    Elapsed Cycles                cycle         7734
    Memory Throughput                 %        48.33
    DRAM Throughput                   %        48.33
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        26.50
    SM Active Cycles              cycle      5003.48
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 23.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.43
    Achieved Active Warps Per SM           warp        36.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27714.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5003.48
    Total L1 Elapsed Cycles          cycle       413814
    Average L2 Active Cycles         cycle      4480.88
    Total L2 Elapsed Cycles          cycle       186888
    Average SM Active Cycles         cycle      5003.48
    Total SM Elapsed Cycles          cycle       413814
    Average SMSP Active Cycles       cycle      4929.02
    Total SMSP Elapsed Cycles        cycle      1655256
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       801.43
    Elapsed Cycles                cycle         7554
    Memory Throughput                 %        47.92
    DRAM Throughput                   %        47.92
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      4932.62
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.62
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27722.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4932.62
    Total L1 Elapsed Cycles          cycle       415780
    Average L2 Active Cycles         cycle      4515.38
    Total L2 Elapsed Cycles          cycle       183888
    Average SM Active Cycles         cycle      4932.62
    Total SM Elapsed Cycles          cycle       415780
    Average SMSP Active Cycles       cycle      5011.15
    Total SMSP Elapsed Cycles        cycle      1663120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.13%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.34% above the average, while the minimum instance value is 12.06% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       801.22
    Elapsed Cycles                cycle         7552
    Memory Throughput                 %        48.01
    DRAM Throughput                   %        48.01
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        26.96
    SM Active Cycles              cycle      5095.50
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.39
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5095.50
    Total L1 Elapsed Cycles          cycle       412716
    Average L2 Active Cycles         cycle      4514.92
    Total L2 Elapsed Cycles          cycle       183720
    Average SM Active Cycles         cycle      5095.50
    Total SM Elapsed Cycles          cycle       412716
    Average SMSP Active Cycles       cycle      5064.51
    Total SMSP Elapsed Cycles        cycle      1650864
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.27
    Elapsed Cycles                cycle      5183655
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.39
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160364.14
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6304432
    Total DRAM Elapsed Cycles        cycle    230739968
    Average L1 Active Cycles         cycle   5160364.14
    Total L1 Elapsed Cycles          cycle    299489212
    Average L2 Active Cycles         cycle   4609254.12
    Total L2 Elapsed Cycles          cycle    125424480
    Average SM Active Cycles         cycle   5160364.14
    Total SM Elapsed Cycles          cycle    299489212
    Average SMSP Active Cycles       cycle   5160795.95
    Total SMSP Elapsed Cycles        cycle   1197956848
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.79
    Elapsed Cycles                cycle      1748787
    Memory Throughput                 %        90.17
    DRAM Throughput                   %        90.17
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.69
    L2 Cache Throughput               %        51.10
    SM Active Cycles              cycle   1736797.78
    Compute (SM) Throughput           %        23.92
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.10
    Achieved Active Warps Per SM           warp        46.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12362717.33
    Total DRAM Elapsed Cycles        cycle     82262016
    Average L1 Active Cycles         cycle   1736797.78
    Total L1 Elapsed Cycles          cycle    101241166
    Average L2 Active Cycles         cycle   1804500.75
    Total L2 Elapsed Cycles          cycle     43470576
    Average SM Active Cycles         cycle   1736797.78
    Total SM Elapsed Cycles          cycle    101241166
    Average SMSP Active Cycles       cycle   1742122.26
    Total SMSP Elapsed Cycles        cycle    404964664
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.17
    Elapsed Cycles                cycle      4521495
    Memory Throughput                 %        89.64
    DRAM Throughput                   %        25.24
    Duration                         ms         5.44
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.27
    SM Active Cycles              cycle   4413915.48
    Compute (SM) Throughput           %        89.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577808
    Total DRAM Elapsed Cycles        cycle    203944960
    Average L1 Active Cycles         cycle   4413915.48
    Total L1 Elapsed Cycles          cycle    262070666
    Average L2 Active Cycles         cycle   4525917.62
    Total L2 Elapsed Cycles          cycle    109383624
    Average SM Active Cycles         cycle   4413915.48
    Total SM Elapsed Cycles          cycle    262070666
    Average SMSP Active Cycles       cycle   4414391.72
    Total SMSP Elapsed Cycles        cycle   1048282664
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       800.14
    Elapsed Cycles                cycle         7075
    Memory Throughput                 %        44.57
    DRAM Throughput                   %        44.57
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.50
    L2 Cache Throughput               %        24.39
    SM Active Cycles              cycle      4611.90
    Compute (SM) Throughput           %        17.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.68
    Achieved Active Warps Per SM           warp        34.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24037.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4611.90
    Total L1 Elapsed Cycles          cycle       382308
    Average L2 Active Cycles         cycle      4138.42
    Total L2 Elapsed Cycles          cycle       171960
    Average SM Active Cycles         cycle      4611.90
    Total SM Elapsed Cycles          cycle       382308
    Average SMSP Active Cycles       cycle      4590.53
    Total SMSP Elapsed Cycles        cycle      1529232
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.648%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.78% above the average, while the minimum instance value is 2.55% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.41
    Elapsed Cycles                cycle         7447
    Memory Throughput                 %        47.16
    DRAM Throughput                   %        47.16
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        26.71
    SM Active Cycles              cycle      5000.19
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.76
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27688
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      5000.19
    Total L1 Elapsed Cycles          cycle       407826
    Average L2 Active Cycles         cycle      4490.79
    Total L2 Elapsed Cycles          cycle       185448
    Average SM Active Cycles         cycle      5000.19
    Total SM Elapsed Cycles          cycle       407826
    Average SMSP Active Cycles       cycle      4895.28
    Total SMSP Elapsed Cycles        cycle      1631304
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       795.74
    Elapsed Cycles                cycle         7553
    Memory Throughput                 %        47.61
    DRAM Throughput                   %        47.61
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        22.70
    L2 Cache Throughput               %        26.90
    SM Active Cycles              cycle      4976.66
    Compute (SM) Throughput           %        15.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.35
    Achieved Active Warps Per SM           warp        35.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4976.66
    Total L1 Elapsed Cycles          cycle       434258
    Average L2 Active Cycles         cycle      4481.12
    Total L2 Elapsed Cycles          cycle       184128
    Average SM Active Cycles         cycle      4976.66
    Total SM Elapsed Cycles          cycle       434258
    Average SMSP Active Cycles       cycle      5081.45
    Total SMSP Elapsed Cycles        cycle      1737032
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       803.05
    Elapsed Cycles                cycle         7616
    Memory Throughput                 %        47.65
    DRAM Throughput                   %        47.65
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        26.80
    SM Active Cycles              cycle      4978.09
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.42
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4978.09
    Total L1 Elapsed Cycles          cycle       410894
    Average L2 Active Cycles         cycle      4494.88
    Total L2 Elapsed Cycles          cycle       184920
    Average SM Active Cycles         cycle      4978.09
    Total SM Elapsed Cycles          cycle       410894
    Average SMSP Active Cycles       cycle      4974.84
    Total SMSP Elapsed Cycles        cycle      1643576
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       833.21
    Elapsed Cycles                cycle      5184912
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.30
    Duration                         ms         6.20
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160032.05
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6306906.67
    Total DRAM Elapsed Cycles        cycle    232157184
    Average L1 Active Cycles         cycle   5160032.05
    Total L1 Elapsed Cycles          cycle    299454836
    Average L2 Active Cycles         cycle   4611177.79
    Total L2 Elapsed Cycles          cycle    125435400
    Average SM Active Cycles         cycle   5160032.05
    Total SM Elapsed Cycles          cycle    299454836
    Average SMSP Active Cycles       cycle   5160551.57
    Total SMSP Elapsed Cycles        cycle   1197819344
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.03
    Elapsed Cycles                cycle      1755967
    Memory Throughput                 %        89.49
    DRAM Throughput                   %        89.49
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.78
    L2 Cache Throughput               %        51.06
    SM Active Cycles              cycle   1735981.66
    Compute (SM) Throughput           %        23.97
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.69
    Achieved Active Warps Per SM           warp        46.89
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12312469.33
    Total DRAM Elapsed Cycles        cycle     82547712
    Average L1 Active Cycles         cycle   1735981.66
    Total L1 Elapsed Cycles          cycle    101029594
    Average L2 Active Cycles         cycle   1798851.71
    Total L2 Elapsed Cycles          cycle     43623264
    Average SM Active Cycles         cycle   1735981.66
    Total SM Elapsed Cycles          cycle    101029594
    Average SMSP Active Cycles       cycle   1737168.43
    Total SMSP Elapsed Cycles        cycle    404118376
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.87
    Elapsed Cycles                cycle      4535878
    Memory Throughput                 %        89.48
    DRAM Throughput                   %        25.10
    Duration                         ms         5.48
    L1/TEX Cache Throughput           %        91.73
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4415571.14
    Compute (SM) Throughput           %        89.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8581786.67
    Total DRAM Elapsed Cycles        cycle    205139968
    Average L1 Active Cycles         cycle   4415571.14
    Total L1 Elapsed Cycles          cycle    262555826
    Average L2 Active Cycles         cycle   4519048.62
    Total L2 Elapsed Cycles          cycle    109750128
    Average SM Active Cycles         cycle   4415571.14
    Total SM Elapsed Cycles          cycle    262555826
    Average SMSP Active Cycles       cycle   4413491.83
    Total SMSP Elapsed Cycles        cycle   1050223304
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       804.48
    Elapsed Cycles                cycle         6932
    Memory Throughput                 %        44.86
    DRAM Throughput                   %        44.86
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.10
    L2 Cache Throughput               %        24.94
    SM Active Cycles              cycle      4500.91
    Compute (SM) Throughput           %        16.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.87
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23808
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4500.91
    Total L1 Elapsed Cycles          cycle       386964
    Average L2 Active Cycles         cycle      4035.75
    Total L2 Elapsed Cycles          cycle       168168
    Average SM Active Cycles         cycle      4500.91
    Total SM Elapsed Cycles          cycle       386964
    Average SMSP Active Cycles       cycle      4427.31
    Total SMSP Elapsed Cycles        cycle      1547856
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       803.56
    Elapsed Cycles                cycle         7568
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        26.92
    SM Active Cycles              cycle      5057.09
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.13
    Achieved Active Warps Per SM           warp        34.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5057.09
    Total L1 Elapsed Cycles          cycle       412450
    Average L2 Active Cycles         cycle      4438.46
    Total L2 Elapsed Cycles          cycle       184008
    Average SM Active Cycles         cycle      5057.09
    Total SM Elapsed Cycles          cycle       412450
    Average SMSP Active Cycles       cycle      4925.11
    Total SMSP Elapsed Cycles        cycle      1649800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.189%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.49% above the average, while the minimum instance value is 9.57% below the average.       

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.47
    Elapsed Cycles                cycle         7464
    Memory Throughput                 %        47.72
    DRAM Throughput                   %        47.72
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        26.79
    SM Active Cycles              cycle      4958.97
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      4958.97
    Total L1 Elapsed Cycles          cycle       409372
    Average L2 Active Cycles         cycle      4442.54
    Total L2 Elapsed Cycles          cycle       184704
    Average SM Active Cycles         cycle      4958.97
    Total SM Elapsed Cycles          cycle       409372
    Average SMSP Active Cycles       cycle      4861.03
    Total SMSP Elapsed Cycles        cycle      1637488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz          790
    Elapsed Cycles                cycle         7474
    Memory Throughput                 %        47.59
    DRAM Throughput                   %        47.59
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        26.80
    SM Active Cycles              cycle      5060.12
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.46
    Achieved Active Warps Per SM           warp        34.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5060.12
    Total L1 Elapsed Cycles          cycle       408774
    Average L2 Active Cycles         cycle      4477.17
    Total L2 Elapsed Cycles          cycle       184776
    Average SM Active Cycles         cycle      5060.12
    Total SM Elapsed Cycles          cycle       408774
    Average SMSP Active Cycles       cycle      4936.97
    Total SMSP Elapsed Cycles        cycle      1635096
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       834.77
    Elapsed Cycles                cycle      5185705
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.32
    Duration                         ms         6.18
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160310.64
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6304200
    Total DRAM Elapsed Cycles        cycle    231728128
    Average L1 Active Cycles         cycle   5160310.64
    Total L1 Elapsed Cycles          cycle    299476194
    Average L2 Active Cycles         cycle   4606469.17
    Total L2 Elapsed Cycles          cycle    125386416
    Average SM Active Cycles         cycle   5160310.64
    Total SM Elapsed Cycles          cycle    299476194
    Average SMSP Active Cycles       cycle   5160509.72
    Total SMSP Elapsed Cycles        cycle   1197904776
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.91
    Elapsed Cycles                cycle      1750157
    Memory Throughput                 %        90.04
    DRAM Throughput                   %        90.04
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.57
    L2 Cache Throughput               %        51.19
    SM Active Cycles              cycle   1741387.38
    Compute (SM) Throughput           %        23.84
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.76
    Achieved Active Warps Per SM           warp        46.44
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12350776
    Total DRAM Elapsed Cycles        cycle     82298880
    Average L1 Active Cycles         cycle   1741387.38
    Total L1 Elapsed Cycles          cycle    101565092
    Average L2 Active Cycles         cycle   1805403.21
    Total L2 Elapsed Cycles          cycle     43490928
    Average SM Active Cycles         cycle   1741387.38
    Total SM Elapsed Cycles          cycle    101565092
    Average SMSP Active Cycles       cycle   1745595.10
    Total SMSP Elapsed Cycles        cycle    406260368
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.15
    Elapsed Cycles                cycle      4543612
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        24.46
    Duration                         ms         5.62
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %        15.02
    SM Active Cycles              cycle   4416170.60
    Compute (SM) Throughput           %        89.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.03
    Achieved Active Warps Per SM           warp        42.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8581165.33
    Total DRAM Elapsed Cycles        cycle    210484224
    Average L1 Active Cycles         cycle   4416170.60
    Total L1 Elapsed Cycles          cycle    261688846
    Average L2 Active Cycles         cycle   4575237.21
    Total L2 Elapsed Cycles          cycle    111229272
    Average SM Active Cycles         cycle   4416170.60
    Total SM Elapsed Cycles          cycle    261688846
    Average SMSP Active Cycles       cycle   4413480.97
    Total SMSP Elapsed Cycles        cycle   1046755384
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       824.46
    Elapsed Cycles                cycle         7026
    Memory Throughput                 %        45.90
    DRAM Throughput                   %        45.90
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        24.41
    L2 Cache Throughput               %        24.70
    SM Active Cycles              cycle      4628.71
    Compute (SM) Throughput           %        17.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.83
    Achieved Active Warps Per SM           warp        34.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23970.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4628.71
    Total L1 Elapsed Cycles          cycle       379900
    Average L2 Active Cycles         cycle      3957.71
    Total L2 Elapsed Cycles          cycle       169632
    Average SM Active Cycles         cycle      4628.71
    Total SM Elapsed Cycles          cycle       379900
    Average SMSP Active Cycles       cycle      4456.89
    Total SMSP Elapsed Cycles        cycle      1519600
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.073%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.06% above the average, while the minimum instance value is 2.85% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       818.17
    Elapsed Cycles                cycle         7627
    Memory Throughput                 %        48.34
    DRAM Throughput                   %        48.34
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.63
    L2 Cache Throughput               %        26.90
    SM Active Cycles              cycle      5288.74
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.32
    Achieved Active Warps Per SM           warp        34.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27720
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5288.74
    Total L1 Elapsed Cycles          cycle       414830
    Average L2 Active Cycles         cycle      4490.96
    Total L2 Elapsed Cycles          cycle       184200
    Average SM Active Cycles         cycle      5288.74
    Total SM Elapsed Cycles          cycle       414830
    Average SMSP Active Cycles       cycle      5043.21
    Total SMSP Elapsed Cycles        cycle      1659320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       788.67
    Elapsed Cycles                cycle         7413
    Memory Throughput                 %        47.95
    DRAM Throughput                   %        47.95
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.02
    SM Active Cycles              cycle      5018.53
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.23
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5018.53
    Total L1 Elapsed Cycles          cycle       410036
    Average L2 Active Cycles         cycle      4561.88
    Total L2 Elapsed Cycles          cycle       183192
    Average SM Active Cycles         cycle      5018.53
    Total SM Elapsed Cycles          cycle       410036
    Average SMSP Active Cycles       cycle      4982.00
    Total SMSP Elapsed Cycles        cycle      1640144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       788.01
    Elapsed Cycles                cycle         7505
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        26.67
    SM Active Cycles              cycle      4929.76
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.87
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4929.76
    Total L1 Elapsed Cycles          cycle       410846
    Average L2 Active Cycles         cycle      4473.12
    Total L2 Elapsed Cycles          cycle       185688
    Average SM Active Cycles         cycle      4929.76
    Total SM Elapsed Cycles          cycle       410846
    Average SMSP Active Cycles       cycle      4881.55
    Total SMSP Elapsed Cycles        cycle      1643384
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.10
    Elapsed Cycles                cycle      5186353
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.76
    Duration                         ms         6.41
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.23
    SM Active Cycles              cycle   5160042.52
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6308477.33
    Total DRAM Elapsed Cycles        cycle    240231424
    Average L1 Active Cycles         cycle   5160042.52
    Total L1 Elapsed Cycles          cycle    299438404
    Average L2 Active Cycles         cycle      4650941
    Total L2 Elapsed Cycles          cycle    126950160
    Average SM Active Cycles         cycle   5160042.52
    Total SM Elapsed Cycles          cycle    299438404
    Average SMSP Active Cycles       cycle   5160508.64
    Total SMSP Elapsed Cycles        cycle   1197753616
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       854.74
    Elapsed Cycles                cycle      1877712
    Memory Throughput                 %        90.43
    DRAM Throughput                   %        90.43
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        40.47
    L2 Cache Throughput               %        48.95
    SM Active Cycles              cycle   1869943.34
    Compute (SM) Throughput           %        22.17
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.62
    Achieved Active Warps Per SM           warp        46.38
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12350808
    Total DRAM Elapsed Cycles        cycle     81945600
    Average L1 Active Cycles         cycle   1869943.34
    Total L1 Elapsed Cycles          cycle    109214962
    Average L2 Active Cycles         cycle   1889153.21
    Total L2 Elapsed Cycles          cycle     45405456
    Average SM Active Cycles         cycle   1869943.34
    Total SM Elapsed Cycles          cycle    109214962
    Average SMSP Active Cycles       cycle   1871642.25
    Total SMSP Elapsed Cycles        cycle    436859848
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.11
    Elapsed Cycles                cycle      4536994
    Memory Throughput                 %        89.81
    DRAM Throughput                   %        24.51
    Duration                         ms         5.61
    L1/TEX Cache Throughput           %        91.73
    L2 Cache Throughput               %        15.04
    SM Active Cycles              cycle   4415828.29
    Compute (SM) Throughput           %        89.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8582000
    Total DRAM Elapsed Cycles        cycle    210094080
    Average L1 Active Cycles         cycle   4415828.29
    Total L1 Elapsed Cycles          cycle    261598686
    Average L2 Active Cycles         cycle   4577662.29
    Total L2 Elapsed Cycles          cycle    111023064
    Average SM Active Cycles         cycle   4415828.29
    Total SM Elapsed Cycles          cycle    261598686
    Average SMSP Active Cycles       cycle   4412911.09
    Total SMSP Elapsed Cycles        cycle   1046394744
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       806.75
    Elapsed Cycles                cycle         7080
    Memory Throughput                 %        43.94
    DRAM Throughput                   %        43.94
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        25.10
    L2 Cache Throughput               %        24.47
    SM Active Cycles              cycle      4502.34
    Compute (SM) Throughput           %        17.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.03
    Achieved Active Warps Per SM           warp        35.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23696
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4502.34
    Total L1 Elapsed Cycles          cycle       380324
    Average L2 Active Cycles         cycle      3957.71
    Total L2 Elapsed Cycles          cycle       171552
    Average SM Active Cycles         cycle      4502.34
    Total SM Elapsed Cycles          cycle       380324
    Average SMSP Active Cycles       cycle      4436.75
    Total SMSP Elapsed Cycles        cycle      1521296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.07
    Elapsed Cycles                cycle         7616
    Memory Throughput                 %        47.01
    DRAM Throughput                   %        47.01
    Duration                         us         9.66
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        26.29
    SM Active Cycles              cycle      5014.95
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.98
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27922.67
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      5014.95
    Total L1 Elapsed Cycles          cycle       414386
    Average L2 Active Cycles         cycle      4491.96
    Total L2 Elapsed Cycles          cycle       188472
    Average SM Active Cycles         cycle      5014.95
    Total SM Elapsed Cycles          cycle       414386
    Average SMSP Active Cycles       cycle      4914.76
    Total SMSP Elapsed Cycles        cycle      1657544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.13
    Elapsed Cycles                cycle         7346
    Memory Throughput                 %        48.31
    DRAM Throughput                   %        48.31
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.99
    L2 Cache Throughput               %        27.22
    SM Active Cycles              cycle      4934.09
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.24
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27701.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4934.09
    Total L1 Elapsed Cycles          cycle       408954
    Average L2 Active Cycles         cycle      4481.79
    Total L2 Elapsed Cycles          cycle       181776
    Average SM Active Cycles         cycle      4934.09
    Total SM Elapsed Cycles          cycle       408954
    Average SMSP Active Cycles       cycle      4947.44
    Total SMSP Elapsed Cycles        cycle      1635816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       822.25
    Elapsed Cycles                cycle         7643
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        22.72
    L2 Cache Throughput               %        26.82
    SM Active Cycles              cycle      5062.69
    Compute (SM) Throughput           %        15.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.79
    Achieved Active Warps Per SM           warp        34.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5062.69
    Total L1 Elapsed Cycles          cycle       431392
    Average L2 Active Cycles         cycle      4579.50
    Total L2 Elapsed Cycles          cycle       184488
    Average SM Active Cycles         cycle      5062.69
    Total SM Elapsed Cycles          cycle       431392
    Average SMSP Active Cycles       cycle      5115.56
    Total SMSP Elapsed Cycles        cycle      1725568
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.37
    Elapsed Cycles                cycle      5187014
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.76
    Duration                         ms         6.41
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.24
    SM Active Cycles              cycle   5160612.98
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6309490.67
    Total DRAM Elapsed Cycles        cycle    240147456
    Average L1 Active Cycles         cycle   5160612.98
    Total L1 Elapsed Cycles          cycle    299450546
    Average L2 Active Cycles         cycle   4648893.38
    Total L2 Elapsed Cycles          cycle    126904800
    Average SM Active Cycles         cycle   5160612.98
    Total SM Elapsed Cycles          cycle    299450546
    Average SMSP Active Cycles       cycle   5160157.93
    Total SMSP Elapsed Cycles        cycle   1197802184
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       843.50
    Elapsed Cycles                cycle      1861363
    Memory Throughput                 %        90.09
    DRAM Throughput                   %        90.09
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.26
    L2 Cache Throughput               %        49.40
    SM Active Cycles              cycle   1845991.31
    Compute (SM) Throughput           %        22.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.89
    Achieved Active Warps Per SM           warp        46.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12361008
    Total DRAM Elapsed Cycles        cycle     82323456
    Average L1 Active Cycles         cycle   1845991.31
    Total L1 Elapsed Cycles          cycle    106982806
    Average L2 Active Cycles         cycle   1867557.62
    Total L2 Elapsed Cycles          cycle     45029112
    Average SM Active Cycles         cycle   1845991.31
    Total SM Elapsed Cycles          cycle    106982806
    Average SMSP Active Cycles       cycle   1843218.88
    Total SMSP Elapsed Cycles        cycle    427931224
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.35
    Elapsed Cycles                cycle      4531829
    Memory Throughput                 %        89.82
    DRAM Throughput                   %        24.53
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.07
    SM Active Cycles              cycle   4413884.21
    Compute (SM) Throughput           %        89.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8579181.33
    Total DRAM Elapsed Cycles        cycle    209829888
    Average L1 Active Cycles         cycle   4413884.21
    Total L1 Elapsed Cycles          cycle    261554888
    Average L2 Active Cycles         cycle   4568462.96
    Total L2 Elapsed Cycles          cycle    110882592
    Average SM Active Cycles         cycle   4413884.21
    Total SM Elapsed Cycles          cycle    261554888
    Average SMSP Active Cycles       cycle   4412753.60
    Total SMSP Elapsed Cycles        cycle   1046219552
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       800.69
    Elapsed Cycles                cycle         6979
    Memory Throughput                 %        44.82
    DRAM Throughput                   %        44.82
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.42
    L2 Cache Throughput               %        24.68
    SM Active Cycles              cycle      4627.93
    Compute (SM) Throughput           %        17.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.53
    Achieved Active Warps Per SM           warp        33.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24021.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4627.93
    Total L1 Elapsed Cycles          cycle       381484
    Average L2 Active Cycles         cycle      4088.29
    Total L2 Elapsed Cycles          cycle       169824
    Average SM Active Cycles         cycle      4627.93
    Total SM Elapsed Cycles          cycle       381484
    Average SMSP Active Cycles       cycle      4510.06
    Total SMSP Elapsed Cycles        cycle      1525936
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.274%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.13% above the average, while the minimum instance value is 3.09% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       800.04
    Elapsed Cycles                cycle         7566
    Memory Throughput                 %        48.10
    DRAM Throughput                   %        48.10
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.46
    L2 Cache Throughput               %        26.92
    SM Active Cycles              cycle      5193.29
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.21
    Achieved Active Warps Per SM           warp        34.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5193.29
    Total L1 Elapsed Cycles          cycle       418010
    Average L2 Active Cycles         cycle      4545.50
    Total L2 Elapsed Cycles          cycle       183888
    Average SM Active Cycles         cycle      5193.29
    Total SM Elapsed Cycles          cycle       418010
    Average SMSP Active Cycles       cycle      5082.75
    Total SMSP Elapsed Cycles        cycle      1672040
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       813.87
    Elapsed Cycles                cycle         7644
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        22.99
    L2 Cache Throughput               %        26.81
    SM Active Cycles              cycle      4966.81
    Compute (SM) Throughput           %        15.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.00
    Achieved Active Warps Per SM           warp        36.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4966.81
    Total L1 Elapsed Cycles          cycle       426576
    Average L2 Active Cycles         cycle      4430.62
    Total L2 Elapsed Cycles          cycle       184632
    Average SM Active Cycles         cycle      4966.81
    Total SM Elapsed Cycles          cycle       426576
    Average SMSP Active Cycles       cycle      4908.39
    Total SMSP Elapsed Cycles        cycle      1706304
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.67
    Elapsed Cycles                cycle         7454
    Memory Throughput                 %        47.61
    DRAM Throughput                   %        47.61
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        26.84
    SM Active Cycles              cycle      5010.52
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.83
    Achieved Active Warps Per SM           warp        34.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5010.52
    Total L1 Elapsed Cycles          cycle       417240
    Average L2 Active Cycles         cycle      4485.92
    Total L2 Elapsed Cycles          cycle       184392
    Average SM Active Cycles         cycle      5010.52
    Total SM Elapsed Cycles          cycle       417240
    Average SMSP Active Cycles       cycle      4918.05
    Total SMSP Elapsed Cycles        cycle      1668960
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.31
    Elapsed Cycles                cycle      5187992
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        15.76
    Duration                         ms         6.41
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.23
    SM Active Cycles              cycle   5160120.90
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6309722.67
    Total DRAM Elapsed Cycles        cycle    240175104
    Average L1 Active Cycles         cycle   5160120.90
    Total L1 Elapsed Cycles          cycle    299468534
    Average L2 Active Cycles         cycle   4650237.54
    Total L2 Elapsed Cycles          cycle    126919320
    Average SM Active Cycles         cycle   5160120.90
    Total SM Elapsed Cycles          cycle    299468534
    Average SMSP Active Cycles       cycle   5160167.14
    Total SMSP Elapsed Cycles        cycle   1197874136
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.29
    Elapsed Cycles                cycle      1773949
    Memory Throughput                 %        90.08
    DRAM Throughput                   %        90.08
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.09
    L2 Cache Throughput               %        51.18
    SM Active Cycles              cycle   1749457.50
    Compute (SM) Throughput           %        23.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.56
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12338616
    Total DRAM Elapsed Cycles        cycle     82182144
    Average L1 Active Cycles         cycle   1749457.50
    Total L1 Elapsed Cycles          cycle    102533612
    Average L2 Active Cycles         cycle   1799012.92
    Total L2 Elapsed Cycles          cycle     43447272
    Average SM Active Cycles         cycle   1749457.50
    Total SM Elapsed Cycles          cycle    102533612
    Average SMSP Active Cycles       cycle   1754073.18
    Total SMSP Elapsed Cycles        cycle    410134448
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.39
    Elapsed Cycles                cycle      4532224
    Memory Throughput                 %        89.50
    DRAM Throughput                   %        24.55
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %        15.06
    SM Active Cycles              cycle   4415941.59
    Compute (SM) Throughput           %        89.50
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.04
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8588018.67
    Total DRAM Elapsed Cycles        cycle    209889280
    Average L1 Active Cycles         cycle   4415941.59
    Total L1 Elapsed Cycles          cycle    262485282
    Average L2 Active Cycles         cycle   4560296.50
    Total L2 Elapsed Cycles          cycle    110914752
    Average SM Active Cycles         cycle   4415941.59
    Total SM Elapsed Cycles          cycle    262485282
    Average SMSP Active Cycles       cycle   4414613.26
    Total SMSP Elapsed Cycles        cycle   1049941128
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       807.92
    Elapsed Cycles                cycle         7119
    Memory Throughput                 %        43.85
    DRAM Throughput                   %        43.85
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        25.22
    L2 Cache Throughput               %        24.32
    SM Active Cycles              cycle      4480.50
    Compute (SM) Throughput           %        17.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.32
    Achieved Active Warps Per SM           warp        35.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23797.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4480.50
    Total L1 Elapsed Cycles          cycle       379596
    Average L2 Active Cycles         cycle      3901.50
    Total L2 Elapsed Cycles          cycle       172416
    Average SM Active Cycles         cycle      4480.50
    Total SM Elapsed Cycles          cycle       379596
    Average SMSP Active Cycles       cycle      4357.82
    Total SMSP Elapsed Cycles        cycle      1518384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.57
    Elapsed Cycles                cycle         7495
    Memory Throughput                 %        47.57
    DRAM Throughput                   %        47.57
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.12
    L2 Cache Throughput               %        26.72
    SM Active Cycles              cycle      4979.60
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4979.60
    Total L1 Elapsed Cycles          cycle       406472
    Average L2 Active Cycles         cycle      4493.25
    Total L2 Elapsed Cycles          cycle       185232
    Average SM Active Cycles         cycle      4979.60
    Total SM Elapsed Cycles          cycle       406472
    Average SMSP Active Cycles       cycle      4887.80
    Total SMSP Elapsed Cycles        cycle      1625888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       800.50
    Elapsed Cycles                cycle         7698
    Memory Throughput                 %        47.30
    DRAM Throughput                   %        47.30
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        26.41
    SM Active Cycles              cycle      5093.09
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27848
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5093.09
    Total L1 Elapsed Cycles          cycle       407910
    Average L2 Active Cycles         cycle      4502.88
    Total L2 Elapsed Cycles          cycle       187440
    Average SM Active Cycles         cycle      5093.09
    Total SM Elapsed Cycles          cycle       407910
    Average SMSP Active Cycles       cycle      5016.43
    Total SMSP Elapsed Cycles        cycle      1631640
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       799.56
    Elapsed Cycles                cycle         7611
    Memory Throughput                 %        47.76
    DRAM Throughput                   %        47.76
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.94
    L2 Cache Throughput               %        26.73
    SM Active Cycles              cycle      5064.41
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5064.41
    Total L1 Elapsed Cycles          cycle       409486
    Average L2 Active Cycles         cycle      4509.38
    Total L2 Elapsed Cycles          cycle       185208
    Average SM Active Cycles         cycle      5064.41
    Total SM Elapsed Cycles          cycle       409486
    Average SMSP Active Cycles       cycle      4975.38
    Total SMSP Elapsed Cycles        cycle      1637944
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.36
    Elapsed Cycles                cycle      5184689
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.36
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160527.40
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6308416
    Total DRAM Elapsed Cycles        cycle    231290880
    Average L1 Active Cycles         cycle   5160527.40
    Total L1 Elapsed Cycles          cycle    299445930
    Average L2 Active Cycles         cycle   4610437.62
    Total L2 Elapsed Cycles          cycle    125447208
    Average SM Active Cycles         cycle   5160527.40
    Total SM Elapsed Cycles          cycle    299445930
    Average SMSP Active Cycles       cycle   5160362.72
    Total SMSP Elapsed Cycles        cycle   1197783720
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       846.30
    Elapsed Cycles                cycle      1857292
    Memory Throughput                 %        90.70
    DRAM Throughput                   %        90.70
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.35
    L2 Cache Throughput               %        49.34
    SM Active Cycles              cycle   1841379.98
    Compute (SM) Throughput           %        22.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.90
    Achieved Active Warps Per SM           warp        46.51
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12381741.33
    Total DRAM Elapsed Cycles        cycle     81906688
    Average L1 Active Cycles         cycle   1841379.98
    Total L1 Elapsed Cycles          cycle    106876452
    Average L2 Active Cycles         cycle   1859861.25
    Total L2 Elapsed Cycles          cycle     44964024
    Average SM Active Cycles         cycle   1841379.98
    Total SM Elapsed Cycles          cycle    106876452
    Average SMSP Active Cycles       cycle   1838829.26
    Total SMSP Elapsed Cycles        cycle    427505808
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.01
    Elapsed Cycles                cycle      4514971
    Memory Throughput                 %        90.00
    DRAM Throughput                   %        24.19
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.67
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4418797.53
    Compute (SM) Throughput           %        90.00
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.99
    Achieved Active Warps Per SM           warp        42.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8579802.67
    Total DRAM Elapsed Cycles        cycle    212771840
    Average L1 Active Cycles         cycle   4418797.53
    Total L1 Elapsed Cycles          cycle    261027462
    Average L2 Active Cycles         cycle   4641678.08
    Total L2 Elapsed Cycles          cycle    112437384
    Average SM Active Cycles         cycle   4418797.53
    Total SM Elapsed Cycles          cycle    261027462
    Average SMSP Active Cycles       cycle   4418413.40
    Total SMSP Elapsed Cycles        cycle   1044109848
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       818.63
    Elapsed Cycles                cycle         7133
    Memory Throughput                 %        45.24
    DRAM Throughput                   %        45.24
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.56
    L2 Cache Throughput               %        24.33
    SM Active Cycles              cycle      4600.22
    Compute (SM) Throughput           %        17.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.17
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24088
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4600.22
    Total L1 Elapsed Cycles          cycle       383554
    Average L2 Active Cycles         cycle      3967.71
    Total L2 Elapsed Cycles          cycle       172296
    Average SM Active Cycles         cycle      4600.22
    Total SM Elapsed Cycles          cycle       383554
    Average SMSP Active Cycles       cycle      4352.02
    Total SMSP Elapsed Cycles        cycle      1534216
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.06
    Elapsed Cycles                cycle         7489
    Memory Throughput                 %        47.64
    DRAM Throughput                   %        47.64
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        26.69
    SM Active Cycles              cycle      5019.67
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.35
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5019.67
    Total L1 Elapsed Cycles          cycle       409780
    Average L2 Active Cycles         cycle      4460.58
    Total L2 Elapsed Cycles          cycle       185376
    Average SM Active Cycles         cycle      5019.67
    Total SM Elapsed Cycles          cycle       409780
    Average SMSP Active Cycles       cycle      4918.28
    Total SMSP Elapsed Cycles        cycle      1639120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.172%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.43% above the average, while the minimum instance value is 8.20% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.49
    Elapsed Cycles                cycle         7517
    Memory Throughput                 %        47.25
    DRAM Throughput                   %        47.25
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        26.56
    SM Active Cycles              cycle      5023.17
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.82
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5023.17
    Total L1 Elapsed Cycles          cycle       410104
    Average L2 Active Cycles         cycle      4469.29
    Total L2 Elapsed Cycles          cycle       186216
    Average SM Active Cycles         cycle      5023.17
    Total SM Elapsed Cycles          cycle       410104
    Average SMSP Active Cycles       cycle      4880.58
    Total SMSP Elapsed Cycles        cycle      1640416
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.96
    Elapsed Cycles                cycle         7557
    Memory Throughput                 %        48.03
    DRAM Throughput                   %        48.03
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.16
    L2 Cache Throughput               %        26.91
    SM Active Cycles              cycle      5141.60
    Compute (SM) Throughput           %        15.48
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.19
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5141.60
    Total L1 Elapsed Cycles          cycle       423376
    Average L2 Active Cycles         cycle         4539
    Total L2 Elapsed Cycles          cycle       183984
    Average SM Active Cycles         cycle      5141.60
    Total SM Elapsed Cycles          cycle       423376
    Average SMSP Active Cycles       cycle      5044.93
    Total SMSP Elapsed Cycles        cycle      1693504
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       837.24
    Elapsed Cycles                cycle      5184630
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.38
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160276.38
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6306064
    Total DRAM Elapsed Cycles        cycle    231036928
    Average L1 Active Cycles         cycle   5160276.38
    Total L1 Elapsed Cycles          cycle    299481190
    Average L2 Active Cycles         cycle   4611048.17
    Total L2 Elapsed Cycles          cycle    125491080
    Average SM Active Cycles         cycle   5160276.38
    Total SM Elapsed Cycles          cycle    299481190
    Average SMSP Active Cycles       cycle   5160528.83
    Total SMSP Elapsed Cycles        cycle   1197924760
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.82
    Elapsed Cycles                cycle      1744374
    Memory Throughput                 %        90.23
    DRAM Throughput                   %        90.23
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.61
    L2 Cache Throughput               %        51.29
    SM Active Cycles              cycle   1733149.98
    Compute (SM) Throughput           %        23.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.01
    Achieved Active Warps Per SM           warp        46.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12336154.67
    Total DRAM Elapsed Cycles        cycle     82028544
    Average L1 Active Cycles         cycle   1733149.98
    Total L1 Elapsed Cycles          cycle    101441270
    Average L2 Active Cycles         cycle   1795161.54
    Total L2 Elapsed Cycles          cycle     43349592
    Average SM Active Cycles         cycle   1733149.98
    Total SM Elapsed Cycles          cycle    101441270
    Average SMSP Active Cycles       cycle   1735027.12
    Total SMSP Elapsed Cycles        cycle    405765080
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.76
    Elapsed Cycles                cycle      4530959
    Memory Throughput                 %        89.71
    DRAM Throughput                   %        25.14
    Duration                         ms         5.47
    L1/TEX Cache Throughput           %        91.69
    L2 Cache Throughput               %        15.23
    SM Active Cycles              cycle   4417419.60
    Compute (SM) Throughput           %        89.71
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.03
    Achieved Active Warps Per SM           warp        42.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8584498.67
    Total DRAM Elapsed Cycles        cycle    204899328
    Average L1 Active Cycles         cycle   4417419.60
    Total L1 Elapsed Cycles          cycle    261867378
    Average L2 Active Cycles         cycle   4519582.46
    Total L2 Elapsed Cycles          cycle    109631136
    Average SM Active Cycles         cycle   4417419.60
    Total SM Elapsed Cycles          cycle    261867378
    Average SMSP Active Cycles       cycle   4413856.90
    Total SMSP Elapsed Cycles        cycle   1047469512
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       787.66
    Elapsed Cycles                cycle         7025
    Memory Throughput                 %        43.45
    DRAM Throughput                   %        43.45
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        25.35
    L2 Cache Throughput               %        24.13
    SM Active Cycles              cycle      4457.64
    Compute (SM) Throughput           %        17.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.19
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23840
    Total DRAM Elapsed Cycles        cycle       329216
    Average L1 Active Cycles         cycle      4457.64
    Total L1 Elapsed Cycles          cycle       374522
    Average L2 Active Cycles         cycle      3964.58
    Total L2 Elapsed Cycles          cycle       173664
    Average SM Active Cycles         cycle      4457.64
    Total SM Elapsed Cycles          cycle       374522
    Average SMSP Active Cycles       cycle      4338.81
    Total SMSP Elapsed Cycles        cycle      1498088
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       789.04
    Elapsed Cycles                cycle         7493
    Memory Throughput                 %        47.48
    DRAM Throughput                   %        47.48
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        26.76
    SM Active Cycles              cycle      4996.14
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4996.14
    Total L1 Elapsed Cycles          cycle       410590
    Average L2 Active Cycles         cycle      4461.42
    Total L2 Elapsed Cycles          cycle       185208
    Average SM Active Cycles         cycle      4996.14
    Total SM Elapsed Cycles          cycle       410590
    Average SMSP Active Cycles       cycle      4893.94
    Total SMSP Elapsed Cycles        cycle      1642360
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.53
    Elapsed Cycles                cycle         7468
    Memory Throughput                 %        47.59
    DRAM Throughput                   %        47.59
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        24.58
    L2 Cache Throughput               %        26.78
    SM Active Cycles              cycle      5027.74
    Compute (SM) Throughput           %        16.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.38
    Achieved Active Warps Per SM           warp        34.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      5027.74
    Total L1 Elapsed Cycles          cycle       398932
    Average L2 Active Cycles         cycle      4557.96
    Total L2 Elapsed Cycles          cycle       184848
    Average SM Active Cycles         cycle      5027.74
    Total SM Elapsed Cycles          cycle       398932
    Average SMSP Active Cycles       cycle      4981.63
    Total SMSP Elapsed Cycles        cycle      1595728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       801.76
    Elapsed Cycles                cycle         7528
    Memory Throughput                 %        47.94
    DRAM Throughput                   %        47.94
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        27.01
    SM Active Cycles              cycle      5021.95
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.63
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5021.95
    Total L1 Elapsed Cycles          cycle       412134
    Average L2 Active Cycles         cycle      4573.17
    Total L2 Elapsed Cycles          cycle       183216
    Average SM Active Cycles         cycle      5021.95
    Total SM Elapsed Cycles          cycle       412134
    Average SMSP Active Cycles       cycle      5112.09
    Total SMSP Elapsed Cycles        cycle      1648536
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      5163977
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        15.54
    Duration                         ms         6.50
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.05
    SM Active Cycles              cycle   5160353.12
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6303232
    Total DRAM Elapsed Cycles        cycle    243375104
    Average L1 Active Cycles         cycle   5160353.12
    Total L1 Elapsed Cycles          cycle    299470400
    Average L2 Active Cycles         cycle   4696236.92
    Total L2 Elapsed Cycles          cycle    128610696
    Average SM Active Cycles         cycle   5160353.12
    Total SM Elapsed Cycles          cycle    299470400
    Average SMSP Active Cycles       cycle   5159976.86
    Total SMSP Elapsed Cycles        cycle   1197881600
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       847.24
    Elapsed Cycles                cycle      1865085
    Memory Throughput                 %        90.30
    DRAM Throughput                   %        90.30
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.32
    L2 Cache Throughput               %        49.24
    SM Active Cycles              cycle   1843348.69
    Compute (SM) Throughput           %        22.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.20
    Achieved Active Warps Per SM           warp        46.66
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12359466.67
    Total DRAM Elapsed Cycles        cycle     82126848
    Average L1 Active Cycles         cycle   1843348.69
    Total L1 Elapsed Cycles          cycle    106965746
    Average L2 Active Cycles         cycle   1872998.04
    Total L2 Elapsed Cycles          cycle     45137232
    Average SM Active Cycles         cycle   1843348.69
    Total SM Elapsed Cycles          cycle    106965746
    Average SMSP Active Cycles       cycle   1851458.83
    Total SMSP Elapsed Cycles        cycle    427862984
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.15
    Elapsed Cycles                cycle      4528970
    Memory Throughput                 %        89.57
    DRAM Throughput                   %        25.16
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.23
    SM Active Cycles              cycle   4414430.62
    Compute (SM) Throughput           %        89.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8577365.33
    Total DRAM Elapsed Cycles        cycle    204524544
    Average L1 Active Cycles         cycle   4414430.62
    Total L1 Elapsed Cycles          cycle    262275432
    Average L2 Active Cycles         cycle   4526592.42
    Total L2 Elapsed Cycles          cycle    109612104
    Average SM Active Cycles         cycle   4414430.62
    Total SM Elapsed Cycles          cycle    262275432
    Average SMSP Active Cycles       cycle   4413365.45
    Total SMSP Elapsed Cycles        cycle   1049101728
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.58
    Elapsed Cycles                cycle         6886
    Memory Throughput                 %        45.55
    DRAM Throughput                   %        45.55
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.85
    L2 Cache Throughput               %        25.02
    SM Active Cycles              cycle      4547.74
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.68
    Achieved Active Warps Per SM           warp        33.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24018.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4547.74
    Total L1 Elapsed Cycles          cycle       382090
    Average L2 Active Cycles         cycle      4037.42
    Total L2 Elapsed Cycles          cycle       167664
    Average SM Active Cycles         cycle      4547.74
    Total SM Elapsed Cycles          cycle       382090
    Average SMSP Active Cycles       cycle      4432.38
    Total SMSP Elapsed Cycles        cycle      1528360
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.405%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.35% above the average, while the minimum instance value is 3.18% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       799.47
    Elapsed Cycles                cycle         7587
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.23
    L2 Cache Throughput               %        26.79
    SM Active Cycles              cycle      5138.38
    Compute (SM) Throughput           %        15.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.46
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5138.38
    Total L1 Elapsed Cycles          cycle       421990
    Average L2 Active Cycles         cycle      4550.25
    Total L2 Elapsed Cycles          cycle       184848
    Average SM Active Cycles         cycle      5138.38
    Total SM Elapsed Cycles          cycle       421990
    Average SMSP Active Cycles       cycle      5023.98
    Total SMSP Elapsed Cycles        cycle      1687960
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       814.54
    Elapsed Cycles                cycle         7698
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.39
    L2 Cache Throughput               %        26.64
    SM Active Cycles              cycle      5052.98
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.58
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5052.98
    Total L1 Elapsed Cycles          cycle       419396
    Average L2 Active Cycles         cycle      4577.54
    Total L2 Elapsed Cycles          cycle       185976
    Average SM Active Cycles         cycle      5052.98
    Total SM Elapsed Cycles          cycle       419396
    Average SMSP Active Cycles       cycle      5136.21
    Total SMSP Elapsed Cycles        cycle      1677584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       798.81
    Elapsed Cycles                cycle         7527
    Memory Throughput                 %        48.35
    DRAM Throughput                   %        48.35
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.31
    L2 Cache Throughput               %        27.02
    SM Active Cycles              cycle      5008.19
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5008.19
    Total L1 Elapsed Cycles          cycle       420738
    Average L2 Active Cycles         cycle      4519.67
    Total L2 Elapsed Cycles          cycle       183240
    Average SM Active Cycles         cycle      5008.19
    Total SM Elapsed Cycles          cycle       420738
    Average SMSP Active Cycles       cycle      4995.53
    Total SMSP Elapsed Cycles        cycle      1682952
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       837.13
    Elapsed Cycles                cycle      5184993
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.37
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160357.98
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6303690.67
    Total DRAM Elapsed Cycles        cycle    231057408
    Average L1 Active Cycles         cycle   5160357.98
    Total L1 Elapsed Cycles          cycle    299463314
    Average L2 Active Cycles         cycle   4607680.29
    Total L2 Elapsed Cycles          cycle    125424120
    Average SM Active Cycles         cycle   5160357.98
    Total SM Elapsed Cycles          cycle    299463314
    Average SMSP Active Cycles       cycle   5159902.28
    Total SMSP Elapsed Cycles        cycle   1197853256
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       841.48
    Elapsed Cycles                cycle      1849835
    Memory Throughput                 %        90.47
    DRAM Throughput                   %        90.47
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.31
    L2 Cache Throughput               %        49.63
    SM Active Cycles              cycle   1833653.93
    Compute (SM) Throughput           %        22.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.92
    Achieved Active Warps Per SM           warp        46.52
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12363509.33
    Total DRAM Elapsed Cycles        cycle     81995776
    Average L1 Active Cycles         cycle   1833653.93
    Total L1 Elapsed Cycles          cycle    107008002
    Average L2 Active Cycles         cycle   1851608.62
    Total L2 Elapsed Cycles          cycle     44773656
    Average SM Active Cycles         cycle   1833653.93
    Total SM Elapsed Cycles          cycle    107008002
    Average SMSP Active Cycles       cycle   1828421.76
    Total SMSP Elapsed Cycles        cycle    428032008
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       825.63
    Elapsed Cycles                cycle      4544836
    Memory Throughput                 %        89.85
    DRAM Throughput                   %        25.08
    Duration                         ms         5.48
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4416221.64
    Compute (SM) Throughput           %        89.85
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8582498.67
    Total DRAM Elapsed Cycles        cycle    205323264
    Average L1 Active Cycles         cycle   4416221.64
    Total L1 Elapsed Cycles          cycle    261457614
    Average L2 Active Cycles         cycle   4569469.92
    Total L2 Elapsed Cycles          cycle    109937736
    Average SM Active Cycles         cycle   4416221.64
    Total SM Elapsed Cycles          cycle    261457614
    Average SMSP Active Cycles       cycle   4411888.90
    Total SMSP Elapsed Cycles        cycle   1045830456
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       803.12
    Elapsed Cycles                cycle         7072
    Memory Throughput                 %        44.16
    DRAM Throughput                   %        44.16
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        25.59
    L2 Cache Throughput               %        24.35
    SM Active Cycles              cycle      4415.22
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.70
    Achieved Active Warps Per SM           warp        35.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23816
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4415.22
    Total L1 Elapsed Cycles          cycle       385976
    Average L2 Active Cycles         cycle      3978.96
    Total L2 Elapsed Cycles          cycle       171960
    Average SM Active Cycles         cycle      4415.22
    Total SM Elapsed Cycles          cycle       385976
    Average SMSP Active Cycles       cycle      4439.35
    Total SMSP Elapsed Cycles        cycle      1543904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       825.30
    Elapsed Cycles                cycle         7668
    Memory Throughput                 %        48.49
    DRAM Throughput                   %        48.49
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        26.76
    SM Active Cycles              cycle      5034.55
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.39
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27722.67
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5034.55
    Total L1 Elapsed Cycles          cycle       410568
    Average L2 Active Cycles         cycle      4703.12
    Total L2 Elapsed Cycles          cycle       185112
    Average SM Active Cycles         cycle      5034.55
    Total SM Elapsed Cycles          cycle       410568
    Average SMSP Active Cycles       cycle      5149.64
    Total SMSP Elapsed Cycles        cycle      1642272
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.93
    Elapsed Cycles                cycle         7505
    Memory Throughput                 %        47.32
    DRAM Throughput                   %        47.32
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        26.51
    SM Active Cycles              cycle      4984.47
    Compute (SM) Throughput           %        16.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.94
    Achieved Active Warps Per SM           warp        35.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4984.47
    Total L1 Elapsed Cycles          cycle       408382
    Average L2 Active Cycles         cycle      4540.79
    Total L2 Elapsed Cycles          cycle       186816
    Average SM Active Cycles         cycle      4984.47
    Total SM Elapsed Cycles          cycle       408382
    Average SMSP Active Cycles       cycle      4905.05
    Total SMSP Elapsed Cycles        cycle      1633528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       801.68
    Elapsed Cycles                cycle         7501
    Memory Throughput                 %        48.42
    DRAM Throughput                   %        48.42
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.54
    L2 Cache Throughput               %        27.13
    SM Active Cycles              cycle      4943.69
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.64
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4943.69
    Total L1 Elapsed Cycles          cycle       416476
    Average L2 Active Cycles         cycle      4451.38
    Total L2 Elapsed Cycles          cycle       182496
    Average SM Active Cycles         cycle      4943.69
    Total SM Elapsed Cycles          cycle       416476
    Average SMSP Active Cycles       cycle      4990.79
    Total SMSP Elapsed Cycles        cycle      1665904
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       840.26
    Elapsed Cycles                cycle      5185484
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.44
    Duration                         ms         6.14
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.41
    SM Active Cycles              cycle   5160329.36
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6308026.67
    Total DRAM Elapsed Cycles        cycle    230197248
    Average L1 Active Cycles         cycle   5160329.36
    Total L1 Elapsed Cycles          cycle    299454636
    Average L2 Active Cycles         cycle   4606973.88
    Total L2 Elapsed Cycles          cycle    125357112
    Average SM Active Cycles         cycle   5160329.36
    Total SM Elapsed Cycles          cycle    299454636
    Average SMSP Active Cycles       cycle   5160538.22
    Total SMSP Elapsed Cycles        cycle   1197818544
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.76
    Elapsed Cycles                cycle      1862728
    Memory Throughput                 %        90.26
    DRAM Throughput                   %        90.26
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.44
    L2 Cache Throughput               %        49.32
    SM Active Cycles              cycle   1851436.33
    Compute (SM) Throughput           %        23.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.65
    Achieved Active Warps Per SM           warp        46.39
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12363424
    Total DRAM Elapsed Cycles        cycle     82186240
    Average L1 Active Cycles         cycle   1851436.33
    Total L1 Elapsed Cycles          cycle    101737698
    Average L2 Active Cycles         cycle   1868508.83
    Total L2 Elapsed Cycles          cycle     45078528
    Average SM Active Cycles         cycle   1851436.33
    Total SM Elapsed Cycles          cycle    101737698
    Average SMSP Active Cycles       cycle   1846285.21
    Total SMSP Elapsed Cycles        cycle    406950792
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.01
    Elapsed Cycles                cycle      4512385
    Memory Throughput                 %        89.62
    DRAM Throughput                   %        24.20
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.79
    L2 Cache Throughput               %        14.87
    SM Active Cycles              cycle   4413014.71
    Compute (SM) Throughput           %        89.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8576210.67
    Total DRAM Elapsed Cycles        cycle    212646912
    Average L1 Active Cycles         cycle   4413014.71
    Total L1 Elapsed Cycles          cycle    262150424
    Average L2 Active Cycles         cycle   4513328.88
    Total L2 Elapsed Cycles          cycle    112371456
    Average SM Active Cycles         cycle   4413014.71
    Total SM Elapsed Cycles          cycle    262150424
    Average SMSP Active Cycles       cycle   4411827.53
    Total SMSP Elapsed Cycles        cycle   1048601696
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       808.15
    Elapsed Cycles                cycle         7093
    Memory Throughput                 %        44.76
    DRAM Throughput                   %        44.76
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.39
    L2 Cache Throughput               %        24.44
    SM Active Cycles              cycle      4632.29
    Compute (SM) Throughput           %        17.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.36
    Achieved Active Warps Per SM           warp        34.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24064
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4632.29
    Total L1 Elapsed Cycles          cycle       383186
    Average L2 Active Cycles         cycle      3939.42
    Total L2 Elapsed Cycles          cycle       171600
    Average SM Active Cycles         cycle      4632.29
    Total SM Elapsed Cycles          cycle       383186
    Average SMSP Active Cycles       cycle      4383.32
    Total SMSP Elapsed Cycles        cycle      1532744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       803.43
    Elapsed Cycles                cycle         7675
    Memory Throughput                 %        47.32
    DRAM Throughput                   %        47.32
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        26.56
    SM Active Cycles              cycle      4951.29
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.44
    Achieved Active Warps Per SM           warp        35.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27701.33
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4951.29
    Total L1 Elapsed Cycles          cycle       415224
    Average L2 Active Cycles         cycle      4464.71
    Total L2 Elapsed Cycles          cycle       186456
    Average SM Active Cycles         cycle      4951.29
    Total SM Elapsed Cycles          cycle       415224
    Average SMSP Active Cycles       cycle      4894.17
    Total SMSP Elapsed Cycles        cycle      1660896
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.85
    Elapsed Cycles                cycle         7470
    Memory Throughput                 %        47.49
    DRAM Throughput                   %        47.49
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        24.45
    L2 Cache Throughput               %        26.77
    SM Active Cycles              cycle      4985.79
    Compute (SM) Throughput           %        16.33
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.68
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4985.79
    Total L1 Elapsed Cycles          cycle       401212
    Average L2 Active Cycles         cycle      4388.04
    Total L2 Elapsed Cycles          cycle       184944
    Average SM Active Cycles         cycle      4985.79
    Total SM Elapsed Cycles          cycle       401212
    Average SMSP Active Cycles       cycle      4839.85
    Total SMSP Elapsed Cycles        cycle      1604848
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.64
    Elapsed Cycles                cycle         7493
    Memory Throughput                 %        47.65
    DRAM Throughput                   %        47.65
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        26.68
    SM Active Cycles              cycle      4989.78
    Compute (SM) Throughput           %        16.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.74
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4989.78
    Total L1 Elapsed Cycles          cycle       409126
    Average L2 Active Cycles         cycle      4501.04
    Total L2 Elapsed Cycles          cycle       185544
    Average SM Active Cycles         cycle      4989.78
    Total SM Elapsed Cycles          cycle       409126
    Average SMSP Active Cycles       cycle      4895.31
    Total SMSP Elapsed Cycles        cycle      1636504
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.34
    Elapsed Cycles                cycle      5185247
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.39
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.41
    SM Active Cycles              cycle   5160260.33
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6303709.33
    Total DRAM Elapsed Cycles        cycle    230719488
    Average L1 Active Cycles         cycle   5160260.33
    Total L1 Elapsed Cycles          cycle    299461866
    Average L2 Active Cycles         cycle   4610102.75
    Total L2 Elapsed Cycles          cycle    125404104
    Average SM Active Cycles         cycle   5160260.33
    Total SM Elapsed Cycles          cycle    299461866
    Average SMSP Active Cycles       cycle   5160222.36
    Total SMSP Elapsed Cycles        cycle   1197847464
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.42
    Elapsed Cycles                cycle      1874145
    Memory Throughput                 %        89.80
    DRAM Throughput                   %        89.80
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.15
    L2 Cache Throughput               %        48.99
    SM Active Cycles              cycle   1842893.12
    Compute (SM) Throughput           %        23.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.92
    Achieved Active Warps Per SM           warp        47.00
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12318954.67
    Total DRAM Elapsed Cycles        cycle     82308096
    Average L1 Active Cycles         cycle   1842893.12
    Total L1 Elapsed Cycles          cycle    102358156
    Average L2 Active Cycles         cycle   1848261.83
    Total L2 Elapsed Cycles          cycle     45334992
    Average SM Active Cycles         cycle   1842893.12
    Total SM Elapsed Cycles          cycle    102358156
    Average SMSP Active Cycles       cycle   1826576.19
    Total SMSP Elapsed Cycles        cycle    409432624
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       805.85
    Elapsed Cycles                cycle      4533729
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        24.56
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.07
    SM Active Cycles              cycle   4414401.16
    Compute (SM) Throughput           %        89.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8586514.67
    Total DRAM Elapsed Cycles        cycle    209762304
    Average L1 Active Cycles         cycle   4414401.16
    Total L1 Elapsed Cycles          cycle    261709122
    Average L2 Active Cycles         cycle   4571990.04
    Total L2 Elapsed Cycles          cycle    110848272
    Average SM Active Cycles         cycle   4414401.16
    Total SM Elapsed Cycles          cycle    261709122
    Average SMSP Active Cycles       cycle   4414477.26
    Total SMSP Elapsed Cycles        cycle   1046836488
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.61
    Elapsed Cycles                cycle         7008
    Memory Throughput                 %        43.44
    DRAM Throughput                   %        43.44
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        25.27
    L2 Cache Throughput               %        24.18
    SM Active Cycles              cycle      4470.76
    Compute (SM) Throughput           %        17.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.26
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23800
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4470.76
    Total L1 Elapsed Cycles          cycle       381330
    Average L2 Active Cycles         cycle      4019.29
    Total L2 Elapsed Cycles          cycle       173400
    Average SM Active Cycles         cycle      4470.76
    Total SM Elapsed Cycles          cycle       381330
    Average SMSP Active Cycles       cycle      4425.66
    Total SMSP Elapsed Cycles        cycle      1525320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.47
    Elapsed Cycles                cycle         7493
    Memory Throughput                 %        47.26
    DRAM Throughput                   %        47.26
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.67
    SM Active Cycles              cycle      4996.19
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4996.19
    Total L1 Elapsed Cycles          cycle       413302
    Average L2 Active Cycles         cycle      4391.83
    Total L2 Elapsed Cycles          cycle       185520
    Average SM Active Cycles         cycle      4996.19
    Total SM Elapsed Cycles          cycle       413302
    Average SMSP Active Cycles       cycle      4818.09
    Total SMSP Elapsed Cycles        cycle      1653208
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       790.62
    Elapsed Cycles                cycle         7460
    Memory Throughput                 %        47.65
    DRAM Throughput                   %        47.65
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        26.86
    SM Active Cycles              cycle      4974.26
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.01
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4974.26
    Total L1 Elapsed Cycles          cycle       407860
    Average L2 Active Cycles         cycle      4452.08
    Total L2 Elapsed Cycles          cycle       184440
    Average SM Active Cycles         cycle      4974.26
    Total SM Elapsed Cycles          cycle       407860
    Average SMSP Active Cycles       cycle      4897.61
    Total SMSP Elapsed Cycles        cycle      1631440
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       788.91
    Elapsed Cycles                cycle         7491
    Memory Throughput                 %        47.74
    DRAM Throughput                   %        47.74
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        24.29
    L2 Cache Throughput               %        26.70
    SM Active Cycles              cycle      4916.97
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.87
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4916.97
    Total L1 Elapsed Cycles          cycle       403702
    Average L2 Active Cycles         cycle      4448.96
    Total L2 Elapsed Cycles          cycle       185472
    Average SM Active Cycles         cycle      4916.97
    Total SM Elapsed Cycles          cycle       403702
    Average SMSP Active Cycles       cycle      4881.70
    Total SMSP Elapsed Cycles        cycle      1614808
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.00
    Elapsed Cycles                cycle      5186952
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.76
    Duration                         ms         6.41
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.24
    SM Active Cycles              cycle   5160552.29
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302773.33
    Total DRAM Elapsed Cycles        cycle    239990784
    Average L1 Active Cycles         cycle   5160552.29
    Total L1 Elapsed Cycles          cycle    299445002
    Average L2 Active Cycles         cycle   4647397.67
    Total L2 Elapsed Cycles          cycle    126822840
    Average SM Active Cycles         cycle   5160552.29
    Total SM Elapsed Cycles          cycle    299445002
    Average SMSP Active Cycles       cycle   5160144.36
    Total SMSP Elapsed Cycles        cycle   1197780008
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.13
    Elapsed Cycles                cycle      1882450
    Memory Throughput                 %        89.42
    DRAM Throughput                   %        89.42
    Duration                         ms         2.21
    L1/TEX Cache Throughput           %        43.42
    L2 Cache Throughput               %        48.74
    SM Active Cycles              cycle   1847759.91
    Compute (SM) Throughput           %        23.79
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.05
    Achieved Active Warps Per SM           warp        47.06
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12325205.33
    Total DRAM Elapsed Cycles        cycle     82697216
    Average L1 Active Cycles         cycle   1847759.91
    Total L1 Elapsed Cycles          cycle    101795250
    Average L2 Active Cycles         cycle   1804858.38
    Total L2 Elapsed Cycles          cycle     45532296
    Average SM Active Cycles         cycle   1847759.91
    Total SM Elapsed Cycles          cycle    101795250
    Average SMSP Active Cycles       cycle   1739589.93
    Total SMSP Elapsed Cycles        cycle    407181000
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.02
    Elapsed Cycles                cycle      4515292
    Memory Throughput                 %        89.65
    DRAM Throughput                   %        24.20
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.79
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4412739.03
    Compute (SM) Throughput           %        89.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.11
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8580570.67
    Total DRAM Elapsed Cycles        cycle    212768768
    Average L1 Active Cycles         cycle   4412739.03
    Total L1 Elapsed Cycles          cycle    262065578
    Average L2 Active Cycles         cycle   4524327.75
    Total L2 Elapsed Cycles          cycle    112437000
    Average SM Active Cycles         cycle   4412739.03
    Total SM Elapsed Cycles          cycle    262065578
    Average SMSP Active Cycles       cycle   4413217.88
    Total SMSP Elapsed Cycles        cycle   1048262312
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.05
    Elapsed Cycles                cycle         6829
    Memory Throughput                 %        44.99
    DRAM Throughput                   %        44.99
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.51
    L2 Cache Throughput               %        24.62
    SM Active Cycles              cycle      4610.17
    Compute (SM) Throughput           %        17.29
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 31.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.61
    Achieved Active Warps Per SM           warp        32.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24034.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4610.17
    Total L1 Elapsed Cycles          cycle       379124
    Average L2 Active Cycles         cycle      4071.17
    Total L2 Elapsed Cycles          cycle       170112
    Average SM Active Cycles         cycle      4610.17
    Total SM Elapsed Cycles          cycle       379124
    Average SMSP Active Cycles       cycle      4407.89
    Total SMSP Elapsed Cycles        cycle      1516496
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.13
    Elapsed Cycles                cycle         7681
    Memory Throughput                 %        46.10
    DRAM Throughput                   %        46.10
    Duration                         us         9.79
    L1/TEX Cache Throughput           %        23.00
    L2 Cache Throughput               %        25.89
    SM Active Cycles              cycle      5225.84
    Compute (SM) Throughput           %        15.37
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.14
    Achieved Active Warps Per SM           warp        33.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       362496
    Average L1 Active Cycles         cycle      5225.84
    Total L1 Elapsed Cycles          cycle       426280
    Average L2 Active Cycles         cycle      4545.58
    Total L2 Elapsed Cycles          cycle       191280
    Average SM Active Cycles         cycle      5225.84
    Total SM Elapsed Cycles          cycle       426280
    Average SMSP Active Cycles       cycle      5106.25
    Total SMSP Elapsed Cycles        cycle      1705120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.079%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.31% above the average, while the minimum instance value is 9.70% below the average.       

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       817.89
    Elapsed Cycles                cycle         7630
    Memory Throughput                 %        48.36
    DRAM Throughput                   %        48.36
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.21
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      5389.05
    Compute (SM) Throughput           %        15.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.12
    Achieved Active Warps Per SM           warp        33.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5389.05
    Total L1 Elapsed Cycles          cycle       422428
    Average L2 Active Cycles         cycle      4506.33
    Total L2 Elapsed Cycles          cycle       184176
    Average SM Active Cycles         cycle      5389.05
    Total SM Elapsed Cycles          cycle       422428
    Average SMSP Active Cycles       cycle      5053.59
    Total SMSP Elapsed Cycles        cycle      1689712
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       788.36
    Elapsed Cycles                cycle         7562
    Memory Throughput                 %        46.87
    DRAM Throughput                   %        46.87
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        26.49
    SM Active Cycles              cycle      4968.74
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.93
    Achieved Active Warps Per SM           warp        35.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       355328
    Average L1 Active Cycles         cycle      4968.74
    Total L1 Elapsed Cycles          cycle       411416
    Average L2 Active Cycles         cycle      4472.04
    Total L2 Elapsed Cycles          cycle       187008
    Average SM Active Cycles         cycle      4968.74
    Total SM Elapsed Cycles          cycle       411416
    Average SMSP Active Cycles       cycle      4905.02
    Total SMSP Elapsed Cycles        cycle      1645664
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.05
    Elapsed Cycles                cycle      5187329
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        15.77
    Duration                         ms         6.40
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.25
    SM Active Cycles              cycle   5160077.64
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6305981.33
    Total DRAM Elapsed Cycles        cycle    239963136
    Average L1 Active Cycles         cycle   5160077.64
    Total L1 Elapsed Cycles          cycle    299477468
    Average L2 Active Cycles         cycle   4647912.79
    Total L2 Elapsed Cycles          cycle    126808464
    Average SM Active Cycles         cycle   5160077.64
    Total SM Elapsed Cycles          cycle    299477468
    Average SMSP Active Cycles       cycle   5160099.53
    Total SMSP Elapsed Cycles        cycle   1197909872
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.56
    Elapsed Cycles                cycle      1856568
    Memory Throughput                 %        90.49
    DRAM Throughput                   %        90.49
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.68
    L2 Cache Throughput               %        49.54
    SM Active Cycles              cycle   1848398.28
    Compute (SM) Throughput           %        23.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.49
    Achieved Active Warps Per SM           warp        46.32
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12352408
    Total DRAM Elapsed Cycles        cycle     81905664
    Average L1 Active Cycles         cycle   1848398.28
    Total L1 Elapsed Cycles          cycle    101176522
    Average L2 Active Cycles         cycle      1868265
    Total L2 Elapsed Cycles          cycle     44902512
    Average SM Active Cycles         cycle   1848398.28
    Total SM Elapsed Cycles          cycle    101176522
    Average SMSP Active Cycles       cycle   1847255.31
    Total SMSP Elapsed Cycles        cycle    404706088
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.04
    Elapsed Cycles                cycle      4518558
    Memory Throughput                 %        89.45
    DRAM Throughput                   %        24.20
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.71
    L2 Cache Throughput               %        14.85
    SM Active Cycles              cycle   4416701.29
    Compute (SM) Throughput           %        89.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.04
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8586941.33
    Total DRAM Elapsed Cycles        cycle    212911104
    Average L1 Active Cycles         cycle   4416701.29
    Total L1 Elapsed Cycles          cycle    262624276
    Average L2 Active Cycles         cycle   4517383.96
    Total L2 Elapsed Cycles          cycle    112512144
    Average SM Active Cycles         cycle   4416701.29
    Total SM Elapsed Cycles          cycle    262624276
    Average SMSP Active Cycles       cycle   4413620.03
    Total SMSP Elapsed Cycles        cycle   1050497104
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       800.25
    Elapsed Cycles                cycle         7154
    Memory Throughput                 %        43.31
    DRAM Throughput                   %        43.31
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        25.05
    L2 Cache Throughput               %        24.06
    SM Active Cycles              cycle      4510.62
    Compute (SM) Throughput           %        17.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.89
    Achieved Active Warps Per SM           warp        35.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23725.33
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4510.62
    Total L1 Elapsed Cycles          cycle       383204
    Average L2 Active Cycles         cycle      4008.54
    Total L2 Elapsed Cycles          cycle       174216
    Average SM Active Cycles         cycle      4510.62
    Total SM Elapsed Cycles          cycle       383204
    Average SMSP Active Cycles       cycle      4473.69
    Total SMSP Elapsed Cycles        cycle      1532816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       804.08
    Elapsed Cycles                cycle         7809
    Memory Throughput                 %        46.42
    DRAM Throughput                   %        46.42
    Duration                         us         9.66
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        26.14
    SM Active Cycles              cycle      4981.03
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.81
    Achieved Active Warps Per SM           warp        35.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       359424
    Average L1 Active Cycles         cycle      4981.03
    Total L1 Elapsed Cycles          cycle       412992
    Average L2 Active Cycles         cycle      4513.54
    Total L2 Elapsed Cycles          cycle       189480
    Average SM Active Cycles         cycle      4981.03
    Total SM Elapsed Cycles          cycle       412992
    Average SMSP Active Cycles       cycle      4997.93
    Total SMSP Elapsed Cycles        cycle      1651968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       798.13
    Elapsed Cycles                cycle         7626
    Memory Throughput                 %        47.38
    DRAM Throughput                   %        47.38
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.30
    L2 Cache Throughput               %        26.65
    SM Active Cycles              cycle      5003.31
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.88
    Achieved Active Warps Per SM           warp        34.98
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      5003.31
    Total L1 Elapsed Cycles          cycle       420606
    Average L2 Active Cycles         cycle      4523.92
    Total L2 Elapsed Cycles          cycle       185784
    Average SM Active Cycles         cycle      5003.31
    Total SM Elapsed Cycles          cycle       420606
    Average SMSP Active Cycles       cycle      5052.45
    Total SMSP Elapsed Cycles        cycle      1682424
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       802.14
    Elapsed Cycles                cycle         7535
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        27.01
    SM Active Cycles              cycle      5133.64
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5133.64
    Total L1 Elapsed Cycles          cycle       418270
    Average L2 Active Cycles         cycle      4417.25
    Total L2 Elapsed Cycles          cycle       183264
    Average SM Active Cycles         cycle      5133.64
    Total SM Elapsed Cycles          cycle       418270
    Average SMSP Active Cycles       cycle      4904.25
    Total SMSP Elapsed Cycles        cycle      1673080
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       835.48
    Elapsed Cycles                cycle      5185451
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.34
    Duration                         ms         6.18
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160137.47
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6303128
    Total DRAM Elapsed Cycles        cycle    231518208
    Average L1 Active Cycles         cycle   5160137.47
    Total L1 Elapsed Cycles          cycle    299450694
    Average L2 Active Cycles         cycle   4608882.79
    Total L2 Elapsed Cycles          cycle    125426304
    Average SM Active Cycles         cycle   5160137.47
    Total SM Elapsed Cycles          cycle    299450694
    Average SMSP Active Cycles       cycle      5160455
    Total SMSP Elapsed Cycles        cycle   1197802776
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.32
    Elapsed Cycles                cycle      1753112
    Memory Throughput                 %        90.33
    DRAM Throughput                   %        90.33
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.17
    L2 Cache Throughput               %        51.13
    SM Active Cycles              cycle   1835547.43
    Compute (SM) Throughput           %        22.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.89
    Achieved Active Warps Per SM           warp        44.11
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12397298.67
    Total DRAM Elapsed Cycles        cycle     82347008
    Average L1 Active Cycles         cycle   1835547.43
    Total L1 Elapsed Cycles          cycle    107247054
    Average L2 Active Cycles         cycle   1863349.29
    Total L2 Elapsed Cycles          cycle     43516248
    Average SM Active Cycles         cycle   1835547.43
    Total SM Elapsed Cycles          cycle    107247054
    Average SMSP Active Cycles       cycle   1842563.72
    Total SMSP Elapsed Cycles        cycle    428988216
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.78
    Elapsed Cycles                cycle      4530674
    Memory Throughput                 %        89.75
    DRAM Throughput                   %        25.17
    Duration                         ms         5.45
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.23
    SM Active Cycles              cycle   4414061.66
    Compute (SM) Throughput           %        89.75
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8574168
    Total DRAM Elapsed Cycles        cycle    204372992
    Average L1 Active Cycles         cycle   4414061.66
    Total L1 Elapsed Cycles          cycle    261766162
    Average L2 Active Cycles         cycle   4523554.21
    Total L2 Elapsed Cycles          cycle    109625832
    Average SM Active Cycles         cycle   4414061.66
    Total SM Elapsed Cycles          cycle    261766162
    Average SMSP Active Cycles       cycle   4414177.86
    Total SMSP Elapsed Cycles        cycle   1047064648
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       800.28
    Elapsed Cycles                cycle         6981
    Memory Throughput                 %        44.67
    DRAM Throughput                   %        44.67
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.84
    L2 Cache Throughput               %        24.64
    SM Active Cycles              cycle      4547.95
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.04
    Achieved Active Warps Per SM           warp        34.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24013.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4547.95
    Total L1 Elapsed Cycles          cycle       383742
    Average L2 Active Cycles         cycle      4105.54
    Total L2 Elapsed Cycles          cycle       169968
    Average SM Active Cycles         cycle      4547.95
    Total SM Elapsed Cycles          cycle       383742
    Average SMSP Active Cycles       cycle      4450.75
    Total SMSP Elapsed Cycles        cycle      1534968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.57
    Elapsed Cycles                cycle         7600
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        22.72
    L2 Cache Throughput               %        26.76
    SM Active Cycles              cycle      5122.50
    Compute (SM) Throughput           %        15.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.23
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5122.50
    Total L1 Elapsed Cycles          cycle       431740
    Average L2 Active Cycles         cycle      4476.62
    Total L2 Elapsed Cycles          cycle       185064
    Average SM Active Cycles         cycle      5122.50
    Total SM Elapsed Cycles          cycle       431740
    Average SMSP Active Cycles       cycle      4965.54
    Total SMSP Elapsed Cycles        cycle      1726960
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.02
    Elapsed Cycles                cycle         7536
    Memory Throughput                 %        48.08
    DRAM Throughput                   %        48.08
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.31
    L2 Cache Throughput               %        26.96
    SM Active Cycles              cycle      4925.98
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4925.98
    Total L1 Elapsed Cycles          cycle       420620
    Average L2 Active Cycles         cycle      4397.21
    Total L2 Elapsed Cycles          cycle       183576
    Average SM Active Cycles         cycle      4925.98
    Total SM Elapsed Cycles          cycle       420620
    Average SMSP Active Cycles       cycle      4858.16
    Total SMSP Elapsed Cycles        cycle      1682480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       832.18
    Elapsed Cycles                cycle         7758
    Memory Throughput                 %        48.39
    DRAM Throughput                   %        48.39
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        26.42
    SM Active Cycles              cycle      5063.34
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.18
    Achieved Active Warps Per SM           warp        35.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5063.34
    Total L1 Elapsed Cycles          cycle       416742
    Average L2 Active Cycles         cycle      4495.79
    Total L2 Elapsed Cycles          cycle       187512
    Average SM Active Cycles         cycle      5063.34
    Total SM Elapsed Cycles          cycle       416742
    Average SMSP Active Cycles       cycle      5075.31
    Total SMSP Elapsed Cycles        cycle      1666968
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.36
    Elapsed Cycles                cycle      5186330
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.40
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160461.28
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6305117.33
    Total DRAM Elapsed Cycles        cycle    230704128
    Average L1 Active Cycles         cycle   5160461.28
    Total L1 Elapsed Cycles          cycle    299455578
    Average L2 Active Cycles         cycle   4607971.04
    Total L2 Elapsed Cycles          cycle    125426352
    Average SM Active Cycles         cycle   5160461.28
    Total SM Elapsed Cycles          cycle    299455578
    Average SMSP Active Cycles       cycle   5160440.31
    Total SMSP Elapsed Cycles        cycle   1197822312
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.26
    Elapsed Cycles                cycle      1858843
    Memory Throughput                 %        90.25
    DRAM Throughput                   %        90.25
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.05
    L2 Cache Throughput               %        49.50
    SM Active Cycles              cycle   1755525.45
    Compute (SM) Throughput           %        23.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       101.74
    Achieved Active Warps Per SM           warp        48.84
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12337885.33
    Total DRAM Elapsed Cycles        cycle     82022400
    Average L1 Active Cycles         cycle   1755525.45
    Total L1 Elapsed Cycles          cycle    102608942
    Average L2 Active Cycles         cycle   1795930.33
    Total L2 Elapsed Cycles          cycle     44946312
    Average SM Active Cycles         cycle   1755525.45
    Total SM Elapsed Cycles          cycle    102608942
    Average SMSP Active Cycles       cycle   1752401.79
    Total SMSP Elapsed Cycles        cycle    410435768
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.35
    Elapsed Cycles                cycle      4528189
    Memory Throughput                 %        89.82
    DRAM Throughput                   %        24.60
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        91.73
    L2 Cache Throughput               %        15.10
    SM Active Cycles              cycle   4415532.17
    Compute (SM) Throughput           %        89.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8584493.33
    Total DRAM Elapsed Cycles        cycle    209384448
    Average L1 Active Cycles         cycle   4415532.17
    Total L1 Elapsed Cycles          cycle    261558698
    Average L2 Active Cycles         cycle      4562733
    Total L2 Elapsed Cycles          cycle    110648352
    Average SM Active Cycles         cycle   4415532.17
    Total SM Elapsed Cycles          cycle    261558698
    Average SMSP Active Cycles       cycle   4415109.62
    Total SMSP Elapsed Cycles        cycle   1046234792
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       799.73
    Elapsed Cycles                cycle         7076
    Memory Throughput                 %        43.76
    DRAM Throughput                   %        43.76
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.72
    L2 Cache Throughput               %        24.32
    SM Active Cycles              cycle      4570.41
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23752
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4570.41
    Total L1 Elapsed Cycles          cycle       382184
    Average L2 Active Cycles         cycle      4055.92
    Total L2 Elapsed Cycles          cycle       172272
    Average SM Active Cycles         cycle      4570.41
    Total SM Elapsed Cycles          cycle       382184
    Average SMSP Active Cycles       cycle      4518.32
    Total SMSP Elapsed Cycles        cycle      1528736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       813.01
    Elapsed Cycles                cycle         7635
    Memory Throughput                 %        48.00
    DRAM Throughput                   %        48.00
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        26.83
    SM Active Cycles              cycle      5053.81
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.10
    Achieved Active Warps Per SM           warp        35.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5053.81
    Total L1 Elapsed Cycles          cycle       409978
    Average L2 Active Cycles         cycle      4375.67
    Total L2 Elapsed Cycles          cycle       184584
    Average SM Active Cycles         cycle      5053.81
    Total SM Elapsed Cycles          cycle       409978
    Average SMSP Active Cycles       cycle      4864.22
    Total SMSP Elapsed Cycles        cycle      1639912
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       800.66
    Elapsed Cycles                cycle         7602
    Memory Throughput                 %        47.95
    DRAM Throughput                   %        47.95
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.78
    SM Active Cycles              cycle      5095.29
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.74
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5095.29
    Total L1 Elapsed Cycles          cycle       413142
    Average L2 Active Cycles         cycle      4523.67
    Total L2 Elapsed Cycles          cycle       184920
    Average SM Active Cycles         cycle      5095.29
    Total SM Elapsed Cycles          cycle       413142
    Average SMSP Active Cycles       cycle      5007.92
    Total SMSP Elapsed Cycles        cycle      1652568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.53
    Elapsed Cycles                cycle         7569
    Memory Throughput                 %        47.96
    DRAM Throughput                   %        47.96
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        22.97
    L2 Cache Throughput               %        26.92
    SM Active Cycles              cycle      4959.78
    Compute (SM) Throughput           %        15.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.81
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      4959.78
    Total L1 Elapsed Cycles          cycle       426590
    Average L2 Active Cycles         cycle      4499.29
    Total L2 Elapsed Cycles          cycle       183984
    Average SM Active Cycles         cycle      4959.78
    Total SM Elapsed Cycles          cycle       426590
    Average SMSP Active Cycles       cycle      4976.23
    Total SMSP Elapsed Cycles        cycle      1706360
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.02
    Elapsed Cycles                cycle      5185893
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.42
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160712.05
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6314154.67
    Total DRAM Elapsed Cycles        cycle    230793216
    Average L1 Active Cycles         cycle   5160712.05
    Total L1 Elapsed Cycles          cycle    299480814
    Average L2 Active Cycles         cycle   4609916.54
    Total L2 Elapsed Cycles          cycle    125429280
    Average SM Active Cycles         cycle   5160712.05
    Total SM Elapsed Cycles          cycle    299480814
    Average SMSP Active Cycles       cycle   5160209.76
    Total SMSP Elapsed Cycles        cycle   1197923256
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       846.58
    Elapsed Cycles                cycle      1869240
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        89.77
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.64
    L2 Cache Throughput               %        49.19
    SM Active Cycles              cycle   1747358.38
    Compute (SM) Throughput           %        23.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       103.11
    Achieved Active Warps Per SM           warp        49.49
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12327466.67
    Total DRAM Elapsed Cycles        cycle     82397184
    Average L1 Active Cycles         cycle   1747358.38
    Total L1 Elapsed Cycles          cycle    101391940
    Average L2 Active Cycles         cycle   1808062.62
    Total L2 Elapsed Cycles          cycle     45242112
    Average SM Active Cycles         cycle   1747358.38
    Total SM Elapsed Cycles          cycle    101391940
    Average SMSP Active Cycles       cycle   1750322.69
    Total SMSP Elapsed Cycles        cycle    405567760
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.81
    Elapsed Cycles                cycle      4535129
    Memory Throughput                 %        89.84
    DRAM Throughput                   %        25.28
    Duration                         ms         5.43
    L1/TEX Cache Throughput           %        91.79
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4412705.34
    Compute (SM) Throughput           %        89.84
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8579640
    Total DRAM Elapsed Cycles        cycle    203602944
    Average L1 Active Cycles         cycle   4412705.34
    Total L1 Elapsed Cycles          cycle    261493510
    Average L2 Active Cycles         cycle   4518157.75
    Total L2 Elapsed Cycles          cycle    109684176
    Average SM Active Cycles         cycle   4412705.34
    Total SM Elapsed Cycles          cycle    261493510
    Average SMSP Active Cycles       cycle   4411758.02
    Total SMSP Elapsed Cycles        cycle   1045974040
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.93
    Elapsed Cycles                cycle         6909
    Memory Throughput                 %        44.23
    DRAM Throughput                   %        44.23
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        25.20
    L2 Cache Throughput               %        24.34
    SM Active Cycles              cycle      4483.26
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.35
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4483.26
    Total L1 Elapsed Cycles          cycle       382608
    Average L2 Active Cycles         cycle      4118.38
    Total L2 Elapsed Cycles          cycle       172272
    Average SM Active Cycles         cycle      4483.26
    Total SM Elapsed Cycles          cycle       382608
    Average SMSP Active Cycles       cycle      4455.41
    Total SMSP Elapsed Cycles        cycle      1530432
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.144%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.97% above the average, while the minimum instance value is 2.63% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       788.72
    Elapsed Cycles                cycle         7517
    Memory Throughput                 %        47.72
    DRAM Throughput                   %        47.72
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        26.63
    SM Active Cycles              cycle      4973.33
    Compute (SM) Throughput           %        16.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.99
    Achieved Active Warps Per SM           warp        35.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4973.33
    Total L1 Elapsed Cycles          cycle       406682
    Average L2 Active Cycles         cycle      4505.50
    Total L2 Elapsed Cycles          cycle       185904
    Average SM Active Cycles         cycle      4973.33
    Total SM Elapsed Cycles          cycle       406682
    Average SMSP Active Cycles       cycle      4988.50
    Total SMSP Elapsed Cycles        cycle      1626728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.53
    Elapsed Cycles                cycle         7498
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        26.71
    SM Active Cycles              cycle      4933.33
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.57
    Achieved Active Warps Per SM           warp        35.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4933.33
    Total L1 Elapsed Cycles          cycle       418432
    Average L2 Active Cycles         cycle      4458.38
    Total L2 Elapsed Cycles          cycle       185304
    Average SM Active Cycles         cycle      4933.33
    Total SM Elapsed Cycles          cycle       418432
    Average SMSP Active Cycles       cycle      4961.11
    Total SMSP Elapsed Cycles        cycle      1673728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       800.21
    Elapsed Cycles                cycle         7544
    Memory Throughput                 %        48.08
    DRAM Throughput                   %        48.08
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.20
    L2 Cache Throughput               %        26.99
    SM Active Cycles              cycle      4993.67
    Compute (SM) Throughput           %        15.51
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.10
    Achieved Active Warps Per SM           warp        35.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      4993.67
    Total L1 Elapsed Cycles          cycle       422654
    Average L2 Active Cycles         cycle      4714.54
    Total L2 Elapsed Cycles          cycle       183504
    Average SM Active Cycles         cycle      4993.67
    Total SM Elapsed Cycles          cycle       422654
    Average SMSP Active Cycles       cycle      5267.36
    Total SMSP Elapsed Cycles        cycle      1690616
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       837.59
    Elapsed Cycles                cycle      5185099
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.38
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160247.47
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6304037.33
    Total DRAM Elapsed Cycles        cycle    230919168
    Average L1 Active Cycles         cycle   5160247.47
    Total L1 Elapsed Cycles          cycle    299457064
    Average L2 Active Cycles         cycle   4696497.46
    Total L2 Elapsed Cycles          cycle    125428872
    Average SM Active Cycles         cycle   5160247.47
    Total SM Elapsed Cycles          cycle    299457064
    Average SMSP Active Cycles       cycle   5160054.05
    Total SMSP Elapsed Cycles        cycle   1197828256
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       841.88
    Elapsed Cycles                cycle      1851791
    Memory Throughput                 %        90.13
    DRAM Throughput                   %        90.13
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.43
    L2 Cache Throughput               %        49.51
    SM Active Cycles              cycle   1831814.93
    Compute (SM) Throughput           %        22.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.23
    Achieved Active Warps Per SM           warp        46.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12326232
    Total DRAM Elapsed Cycles        cycle     82056192
    Average L1 Active Cycles         cycle   1831814.93
    Total L1 Elapsed Cycles          cycle    106720216
    Average L2 Active Cycles         cycle   1867091.21
    Total L2 Elapsed Cycles          cycle     44833368
    Average SM Active Cycles         cycle   1831814.93
    Total SM Elapsed Cycles          cycle    106720216
    Average SMSP Active Cycles       cycle   1847375.17
    Total SMSP Elapsed Cycles        cycle    426880864
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.88
    Elapsed Cycles                cycle      4548162
    Memory Throughput                 %        89.82
    DRAM Throughput                   %        25.11
    Duration                         ms         5.48
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.17
    SM Active Cycles              cycle      4414791
    Compute (SM) Throughput           %        89.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8585061.33
    Total DRAM Elapsed Cycles        cycle    205140992
    Average L1 Active Cycles         cycle      4414791
    Total L1 Elapsed Cycles          cycle    261567598
    Average L2 Active Cycles         cycle   4525651.92
    Total L2 Elapsed Cycles          cycle    110053944
    Average SM Active Cycles         cycle      4414791
    Total SM Elapsed Cycles          cycle    261567598
    Average SMSP Active Cycles       cycle   4415241.41
    Total SMSP Elapsed Cycles        cycle   1046270392
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       803.25
    Elapsed Cycles                cycle         7106
    Memory Throughput                 %        43.79
    DRAM Throughput                   %        43.79
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        26.01
    L2 Cache Throughput               %        24.28
    SM Active Cycles              cycle      4344.83
    Compute (SM) Throughput           %        16.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 23.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.33
    Achieved Active Warps Per SM           warp        36.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23765.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4344.83
    Total L1 Elapsed Cycles          cycle       386610
    Average L2 Active Cycles         cycle      3996.62
    Total L2 Elapsed Cycles          cycle       172392
    Average SM Active Cycles         cycle      4344.83
    Total SM Elapsed Cycles          cycle       386610
    Average SMSP Active Cycles       cycle      4398.64
    Total SMSP Elapsed Cycles        cycle      1546440
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       790.07
    Elapsed Cycles                cycle         7453
    Memory Throughput                 %        47.55
    DRAM Throughput                   %        47.55
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        26.89
    SM Active Cycles              cycle      5043.09
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.66
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5043.09
    Total L1 Elapsed Cycles          cycle       410128
    Average L2 Active Cycles         cycle      4531.12
    Total L2 Elapsed Cycles          cycle       184128
    Average SM Active Cycles         cycle      5043.09
    Total SM Elapsed Cycles          cycle       410128
    Average SMSP Active Cycles       cycle      4923.55
    Total SMSP Elapsed Cycles        cycle      1640512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.89
    Elapsed Cycles                cycle         7474
    Memory Throughput                 %        47.59
    DRAM Throughput                   %        47.59
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.94
    L2 Cache Throughput               %        26.78
    SM Active Cycles              cycle      5010.22
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.39
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5010.22
    Total L1 Elapsed Cycles          cycle       409824
    Average L2 Active Cycles         cycle      4530.79
    Total L2 Elapsed Cycles          cycle       184752
    Average SM Active Cycles         cycle      5010.22
    Total SM Elapsed Cycles          cycle       409824
    Average SMSP Active Cycles       cycle      4922.20
    Total SMSP Elapsed Cycles        cycle      1639296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       788.88
    Elapsed Cycles                cycle         7444
    Memory Throughput                 %        48.11
    DRAM Throughput                   %        48.11
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.16
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      5020.71
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.23
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5020.71
    Total L1 Elapsed Cycles          cycle       405572
    Average L2 Active Cycles         cycle      4467.29
    Total L2 Elapsed Cycles          cycle       183720
    Average SM Active Cycles         cycle      5020.71
    Total SM Elapsed Cycles          cycle       405572
    Average SMSP Active Cycles       cycle      4868.41
    Total SMSP Elapsed Cycles        cycle      1622288
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.02
    Elapsed Cycles                cycle      5186389
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.38
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160018.41
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6302888
    Total DRAM Elapsed Cycles        cycle    230815744
    Average L1 Active Cycles         cycle   5160018.41
    Total L1 Elapsed Cycles          cycle    299477280
    Average L2 Active Cycles         cycle   4644937.79
    Total L2 Elapsed Cycles          cycle    125407656
    Average SM Active Cycles         cycle   5160018.41
    Total SM Elapsed Cycles          cycle    299477280
    Average SMSP Active Cycles       cycle   5160351.28
    Total SMSP Elapsed Cycles        cycle   1197909120
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.37
    Elapsed Cycles                cycle      1782609
    Memory Throughput                 %        89.81
    DRAM Throughput                   %        89.81
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.02
    L2 Cache Throughput               %        50.93
    SM Active Cycles              cycle   1759372.14
    Compute (SM) Throughput           %        23.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.56
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12345578.67
    Total DRAM Elapsed Cycles        cycle     82480128
    Average L1 Active Cycles         cycle   1759372.14
    Total L1 Elapsed Cycles          cycle    102706362
    Average L2 Active Cycles         cycle   1803289.12
    Total L2 Elapsed Cycles          cycle     43615608
    Average SM Active Cycles         cycle   1759372.14
    Total SM Elapsed Cycles          cycle    102706362
    Average SMSP Active Cycles       cycle   1760950.98
    Total SMSP Elapsed Cycles        cycle    410825448
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.01
    Elapsed Cycles                cycle      4528940
    Memory Throughput                 %        89.73
    DRAM Throughput                   %        25.53
    Duration                         ms         5.38
    L1/TEX Cache Throughput           %        91.79
    L2 Cache Throughput               %        15.24
    SM Active Cycles              cycle   4412949.90
    Compute (SM) Throughput           %        89.73
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.11
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8575392
    Total DRAM Elapsed Cycles        cycle    201572352
    Average L1 Active Cycles         cycle   4412949.90
    Total L1 Elapsed Cycles          cycle    261831106
    Average L2 Active Cycles         cycle   4524392.92
    Total L2 Elapsed Cycles          cycle    109527936
    Average SM Active Cycles         cycle   4412949.90
    Total SM Elapsed Cycles          cycle    261831106
    Average SMSP Active Cycles       cycle   4414395.36
    Total SMSP Elapsed Cycles        cycle   1047324424
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       800.62
    Elapsed Cycles                cycle         7002
    Memory Throughput                 %        44.75
    DRAM Throughput                   %        44.75
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.53
    L2 Cache Throughput               %        24.59
    SM Active Cycles              cycle      4606.28
    Compute (SM) Throughput           %        17.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.34
    Achieved Active Warps Per SM           warp        33.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24056
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4606.28
    Total L1 Elapsed Cycles          cycle       383330
    Average L2 Active Cycles         cycle      3985.92
    Total L2 Elapsed Cycles          cycle       170568
    Average SM Active Cycles         cycle      4606.28
    Total SM Elapsed Cycles          cycle       383330
    Average SMSP Active Cycles       cycle      4465.77
    Total SMSP Elapsed Cycles        cycle      1533320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       799.28
    Elapsed Cycles                cycle         7605
    Memory Throughput                 %        47.60
    DRAM Throughput                   %        47.60
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        26.69
    SM Active Cycles              cycle      5151.09
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.87
    Achieved Active Warps Per SM           warp        34.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5151.09
    Total L1 Elapsed Cycles          cycle       413878
    Average L2 Active Cycles         cycle      4531.29
    Total L2 Elapsed Cycles          cycle       185544
    Average SM Active Cycles         cycle      5151.09
    Total SM Elapsed Cycles          cycle       413878
    Average SMSP Active Cycles       cycle      4993.71
    Total SMSP Elapsed Cycles        cycle      1655512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       800.93
    Elapsed Cycles                cycle         7604
    Memory Throughput                 %        47.63
    DRAM Throughput                   %        47.63
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        26.73
    SM Active Cycles              cycle      4976.12
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.92
    Achieved Active Warps Per SM           warp        35.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4976.12
    Total L1 Elapsed Cycles          cycle       411498
    Average L2 Active Cycles         cycle      4449.54
    Total L2 Elapsed Cycles          cycle       185208
    Average SM Active Cycles         cycle      4976.12
    Total SM Elapsed Cycles          cycle       411498
    Average SMSP Active Cycles       cycle      4982.27
    Total SMSP Elapsed Cycles        cycle      1645992
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       828.67
    Elapsed Cycles                cycle         7756
    Memory Throughput                 %        48.51
    DRAM Throughput                   %        48.51
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        26.44
    SM Active Cycles              cycle      5055.74
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.87
    Achieved Active Warps Per SM           warp        36.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5055.74
    Total L1 Elapsed Cycles          cycle       414282
    Average L2 Active Cycles         cycle      4628.25
    Total L2 Elapsed Cycles          cycle       187224
    Average SM Active Cycles         cycle      5055.74
    Total SM Elapsed Cycles          cycle       414282
    Average SMSP Active Cycles       cycle      5179.28
    Total SMSP Elapsed Cycles        cycle      1657128
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.52
    Elapsed Cycles                cycle      5185301
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.35
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.67
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160922.62
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302013.33
    Total DRAM Elapsed Cycles        cycle    231230464
    Average L1 Active Cycles         cycle   5160922.62
    Total L1 Elapsed Cycles          cycle    299485720
    Average L2 Active Cycles         cycle      4608172
    Total L2 Elapsed Cycles          cycle    125401272
    Average SM Active Cycles         cycle   5160922.62
    Total SM Elapsed Cycles          cycle    299485720
    Average SMSP Active Cycles       cycle   5160624.28
    Total SMSP Elapsed Cycles        cycle   1197942880
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.45
    Elapsed Cycles                cycle      1753752
    Memory Throughput                 %        90.02
    DRAM Throughput                   %        90.02
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.66
    L2 Cache Throughput               %        51.10
    SM Active Cycles              cycle   1737611.26
    Compute (SM) Throughput           %        23.92
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.22
    Achieved Active Warps Per SM           warp        46.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12357192
    Total DRAM Elapsed Cycles        cycle     82366464
    Average L1 Active Cycles         cycle   1737611.26
    Total L1 Elapsed Cycles          cycle    101243936
    Average L2 Active Cycles         cycle   1801047.71
    Total L2 Elapsed Cycles          cycle     43525680
    Average SM Active Cycles         cycle   1737611.26
    Total SM Elapsed Cycles          cycle    101243936
    Average SMSP Active Cycles       cycle   1738771.18
    Total SMSP Elapsed Cycles        cycle    404975744
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.11
    Elapsed Cycles                cycle      4533697
    Memory Throughput                 %        89.91
    DRAM Throughput                   %        25.16
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4413873.40
    Compute (SM) Throughput           %        89.91
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8582818.67
    Total DRAM Elapsed Cycles        cycle    204690432
    Average L1 Active Cycles         cycle   4413873.40
    Total L1 Elapsed Cycles          cycle    261307258
    Average L2 Active Cycles         cycle   4529336.12
    Total L2 Elapsed Cycles          cycle    109712280
    Average SM Active Cycles         cycle   4413873.40
    Total SM Elapsed Cycles          cycle    261307258
    Average SMSP Active Cycles       cycle   4415089.62
    Total SMSP Elapsed Cycles        cycle   1045229032
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       798.69
    Elapsed Cycles                cycle         6982
    Memory Throughput                 %        44.67
    DRAM Throughput                   %        44.67
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.75
    L2 Cache Throughput               %        24.69
    SM Active Cycles              cycle      4565.36
    Compute (SM) Throughput           %        16.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.16
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23784
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4565.36
    Total L1 Elapsed Cycles          cycle       390976
    Average L2 Active Cycles         cycle      3987.75
    Total L2 Elapsed Cycles          cycle       169848
    Average SM Active Cycles         cycle      4565.36
    Total SM Elapsed Cycles          cycle       390976
    Average SMSP Active Cycles       cycle      4433.22
    Total SMSP Elapsed Cycles        cycle      1563904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       804.53
    Elapsed Cycles                cycle         7631
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.75
    SM Active Cycles              cycle      5047.05
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.07
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5047.05
    Total L1 Elapsed Cycles          cycle       413452
    Average L2 Active Cycles         cycle      4474.29
    Total L2 Elapsed Cycles          cycle       185304
    Average SM Active Cycles         cycle      5047.05
    Total SM Elapsed Cycles          cycle       413452
    Average SMSP Active Cycles       cycle      4954.61
    Total SMSP Elapsed Cycles        cycle      1653808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       802.76
    Elapsed Cycles                cycle         7592
    Memory Throughput                 %        47.79
    DRAM Throughput                   %        47.79
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        26.83
    SM Active Cycles              cycle      5130.74
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5130.74
    Total L1 Elapsed Cycles          cycle       407330
    Average L2 Active Cycles         cycle      4506.50
    Total L2 Elapsed Cycles          cycle       184440
    Average SM Active Cycles         cycle      5130.74
    Total SM Elapsed Cycles          cycle       407330
    Average SMSP Active Cycles       cycle      5016.09
    Total SMSP Elapsed Cycles        cycle      1629320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       799.27
    Elapsed Cycles                cycle         7502
    Memory Throughput                 %        48.18
    DRAM Throughput                   %        48.18
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.57
    L2 Cache Throughput               %        27.09
    SM Active Cycles              cycle      5063.07
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.46
    Achieved Active Warps Per SM           warp        34.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      5063.07
    Total L1 Elapsed Cycles          cycle       416234
    Average L2 Active Cycles         cycle      4537.21
    Total L2 Elapsed Cycles          cycle       182664
    Average SM Active Cycles         cycle      5063.07
    Total SM Elapsed Cycles          cycle       416234
    Average SMSP Active Cycles       cycle      5044.55
    Total SMSP Elapsed Cycles        cycle      1664936
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.60
    Elapsed Cycles                cycle      5184641
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.42
    Duration                         ms         6.15
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160372.60
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6304808
    Total DRAM Elapsed Cycles        cycle    230378496
    Average L1 Active Cycles         cycle   5160372.60
    Total L1 Elapsed Cycles          cycle    299466264
    Average L2 Active Cycles         cycle   4608316.67
    Total L2 Elapsed Cycles          cycle    125386608
    Average SM Active Cycles         cycle   5160372.60
    Total SM Elapsed Cycles          cycle    299466264
    Average SMSP Active Cycles       cycle   5160408.59
    Total SMSP Elapsed Cycles        cycle   1197865056
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.47
    Elapsed Cycles                cycle      1773423
    Memory Throughput                 %        90.43
    DRAM Throughput                   %        90.43
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.06
    L2 Cache Throughput               %        51.25
    SM Active Cycles              cycle   1769848.36
    Compute (SM) Throughput           %        23.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.18
    Achieved Active Warps Per SM           warp        46.17
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12361842.67
    Total DRAM Elapsed Cycles        cycle     82022400
    Average L1 Active Cycles         cycle   1769848.36
    Total L1 Elapsed Cycles          cycle    102717004
    Average L2 Active Cycles         cycle   1804748.79
    Total L2 Elapsed Cycles          cycle     43371744
    Average SM Active Cycles         cycle   1769848.36
    Total SM Elapsed Cycles          cycle    102717004
    Average SMSP Active Cycles       cycle   1764442.25
    Total SMSP Elapsed Cycles        cycle    410868016
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.59
    Elapsed Cycles                cycle      4528091
    Memory Throughput                 %        89.82
    DRAM Throughput                   %        24.58
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %        15.09
    SM Active Cycles              cycle   4415972.43
    Compute (SM) Throughput           %        89.82
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.04
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8577797.33
    Total DRAM Elapsed Cycles        cycle    209387520
    Average L1 Active Cycles         cycle   4415972.43
    Total L1 Elapsed Cycles          cycle    261568064
    Average L2 Active Cycles         cycle   4564604.04
    Total L2 Elapsed Cycles          cycle    110649192
    Average SM Active Cycles         cycle   4415972.43
    Total SM Elapsed Cycles          cycle    261568064
    Average SMSP Active Cycles       cycle   4413970.23
    Total SMSP Elapsed Cycles        cycle   1046272256
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       788.75
    Elapsed Cycles                cycle         6985
    Memory Throughput                 %        44.09
    DRAM Throughput                   %        44.09
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.86
    L2 Cache Throughput               %        24.28
    SM Active Cycles              cycle      4544.50
    Compute (SM) Throughput           %        17.54
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.99
    Achieved Active Warps Per SM           warp        34.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24080
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4544.50
    Total L1 Elapsed Cycles          cycle       373672
    Average L2 Active Cycles         cycle         4076
    Total L2 Elapsed Cycles          cycle       172632
    Average SM Active Cycles         cycle      4544.50
    Total SM Elapsed Cycles          cycle       373672
    Average SMSP Active Cycles       cycle      4454.17
    Total SMSP Elapsed Cycles        cycle      1494688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.099%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.00% above the average, while the minimum instance value is 2.75% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       787.23
    Elapsed Cycles                cycle         7553
    Memory Throughput                 %        47.23
    DRAM Throughput                   %        47.23
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.62
    L2 Cache Throughput               %        26.48
    SM Active Cycles              cycle      4971.31
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.72
    Achieved Active Warps Per SM           warp        35.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4971.31
    Total L1 Elapsed Cycles          cycle       415262
    Average L2 Active Cycles         cycle      4514.08
    Total L2 Elapsed Cycles          cycle       186888
    Average SM Active Cycles         cycle      4971.31
    Total SM Elapsed Cycles          cycle       415262
    Average SMSP Active Cycles       cycle      4933.12
    Total SMSP Elapsed Cycles        cycle      1661048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.86
    Elapsed Cycles                cycle         7446
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      4962.67
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.21
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4962.67
    Total L1 Elapsed Cycles          cycle       409798
    Average L2 Active Cycles         cycle      4468.54
    Total L2 Elapsed Cycles          cycle       184128
    Average SM Active Cycles         cycle      4962.67
    Total SM Elapsed Cycles          cycle       409798
    Average SMSP Active Cycles       cycle      4862.35
    Total SMSP Elapsed Cycles        cycle      1639192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       799.77
    Elapsed Cycles                cycle         7613
    Memory Throughput                 %        47.81
    DRAM Throughput                   %        47.81
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        26.76
    SM Active Cycles              cycle      5383.31
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.35
    Achieved Active Warps Per SM           warp        33.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      5383.31
    Total L1 Elapsed Cycles          cycle       418496
    Average L2 Active Cycles         cycle      4391.38
    Total L2 Elapsed Cycles          cycle       185184
    Average SM Active Cycles         cycle      5383.31
    Total SM Elapsed Cycles          cycle       418496
    Average SMSP Active Cycles       cycle      4882.44
    Total SMSP Elapsed Cycles        cycle      1673984
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       841.89
    Elapsed Cycles                cycle      5185925
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.46
    Duration                         ms         6.13
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160037.26
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6302640
    Total DRAM Elapsed Cycles        cycle    229764096
    Average L1 Active Cycles         cycle   5160037.26
    Total L1 Elapsed Cycles          cycle    299464278
    Average L2 Active Cycles         cycle   4607501.04
    Total L2 Elapsed Cycles          cycle    125391672
    Average SM Active Cycles         cycle   5160037.26
    Total SM Elapsed Cycles          cycle    299464278
    Average SMSP Active Cycles       cycle   5160073.33
    Total SMSP Elapsed Cycles        cycle   1197857112
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.30
    Elapsed Cycles                cycle      1755728
    Memory Throughput                 %        89.67
    DRAM Throughput                   %        89.67
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.69
    L2 Cache Throughput               %        51.09
    SM Active Cycles              cycle   1739175.16
    Compute (SM) Throughput           %        23.93
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.33
    Achieved Active Warps Per SM           warp        46.72
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12325621.33
    Total DRAM Elapsed Cycles        cycle     82471936
    Average L1 Active Cycles         cycle   1739175.16
    Total L1 Elapsed Cycles          cycle    101172190
    Average L2 Active Cycles         cycle   1796532.83
    Total L2 Elapsed Cycles          cycle     43583496
    Average SM Active Cycles         cycle   1739175.16
    Total SM Elapsed Cycles          cycle    101172190
    Average SMSP Active Cycles       cycle   1730551.04
    Total SMSP Elapsed Cycles        cycle    404688760
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.41
    Elapsed Cycles                cycle      4527511
    Memory Throughput                 %        89.59
    DRAM Throughput                   %        25.34
    Duration                         ms         5.42
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.25
    SM Active Cycles              cycle   4413702.28
    Compute (SM) Throughput           %        89.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8580336
    Total DRAM Elapsed Cycles        cycle    203130880
    Average L1 Active Cycles         cycle   4413702.28
    Total L1 Elapsed Cycles          cycle    262236044
    Average L2 Active Cycles         cycle   4524238.17
    Total L2 Elapsed Cycles          cycle    109500240
    Average SM Active Cycles         cycle   4413702.28
    Total SM Elapsed Cycles          cycle    262236044
    Average SMSP Active Cycles       cycle   4415814.21
    Total SMSP Elapsed Cycles        cycle   1048944176
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.03
    Elapsed Cycles                cycle         6949
    Memory Throughput                 %        43.79
    DRAM Throughput                   %        43.79
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        25.89
    L2 Cache Throughput               %        24.34
    SM Active Cycles              cycle      4364.10
    Compute (SM) Throughput           %        17.32
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.56
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23768
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4364.10
    Total L1 Elapsed Cycles          cycle       378460
    Average L2 Active Cycles         cycle      3984.54
    Total L2 Elapsed Cycles          cycle       172056
    Average SM Active Cycles         cycle      4364.10
    Total SM Elapsed Cycles          cycle       378460
    Average SMSP Active Cycles       cycle      4364.91
    Total SMSP Elapsed Cycles        cycle      1513840
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       791.01
    Elapsed Cycles                cycle         7543
    Memory Throughput                 %        47.22
    DRAM Throughput                   %        47.22
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.12
    L2 Cache Throughput               %        26.58
    SM Active Cycles              cycle      4994.55
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.58
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4994.55
    Total L1 Elapsed Cycles          cycle       406546
    Average L2 Active Cycles         cycle      4501.79
    Total L2 Elapsed Cycles          cycle       186312
    Average SM Active Cycles         cycle      4994.55
    Total SM Elapsed Cycles          cycle       406546
    Average SMSP Active Cycles       cycle      4881.66
    Total SMSP Elapsed Cycles        cycle      1626184
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       814.57
    Elapsed Cycles                cycle         7674
    Memory Throughput                 %        47.97
    DRAM Throughput                   %        47.97
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        26.68
    SM Active Cycles              cycle      5075.41
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.94
    Achieved Active Warps Per SM           warp        35.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5075.41
    Total L1 Elapsed Cycles          cycle       413872
    Average L2 Active Cycles         cycle      4440.62
    Total L2 Elapsed Cycles          cycle       185616
    Average SM Active Cycles         cycle      5075.41
    Total SM Elapsed Cycles          cycle       413872
    Average SMSP Active Cycles       cycle      4903.84
    Total SMSP Elapsed Cycles        cycle      1655488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       801.13
    Elapsed Cycles                cycle         7575
    Memory Throughput                 %        47.90
    DRAM Throughput                   %        47.90
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      5115.16
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.12
    Achieved Active Warps Per SM           warp        34.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5115.16
    Total L1 Elapsed Cycles          cycle       411268
    Average L2 Active Cycles         cycle      4519.75
    Total L2 Elapsed Cycles          cycle       184224
    Average SM Active Cycles         cycle      5115.16
    Total SM Elapsed Cycles          cycle       411268
    Average SMSP Active Cycles       cycle      5050.16
    Total SMSP Elapsed Cycles        cycle      1645072
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       835.70
    Elapsed Cycles                cycle      5187911
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.35
    Duration                         ms         6.18
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160305.79
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6306194.67
    Total DRAM Elapsed Cycles        cycle    231467008
    Average L1 Active Cycles         cycle   5160305.79
    Total L1 Elapsed Cycles          cycle    299488796
    Average L2 Active Cycles         cycle   4609716.21
    Total L2 Elapsed Cycles          cycle    125464584
    Average SM Active Cycles         cycle   5160305.79
    Total SM Elapsed Cycles          cycle    299488796
    Average SMSP Active Cycles       cycle   5160649.52
    Total SMSP Elapsed Cycles        cycle   1197955184
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       853.00
    Elapsed Cycles                cycle      1883236
    Memory Throughput                 %        90.01
    DRAM Throughput                   %        90.01
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        40.92
    L2 Cache Throughput               %        48.71
    SM Active Cycles              cycle   1834617.50
    Compute (SM) Throughput           %        22.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.76
    Achieved Active Warps Per SM           warp        47.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12353170.67
    Total DRAM Elapsed Cycles        cycle     82343936
    Average L1 Active Cycles         cycle   1834617.50
    Total L1 Elapsed Cycles          cycle    107928788
    Average L2 Active Cycles         cycle   1856893.08
    Total L2 Elapsed Cycles          cycle     45556056
    Average SM Active Cycles         cycle   1834617.50
    Total SM Elapsed Cycles          cycle    107928788
    Average SMSP Active Cycles       cycle   1837739.47
    Total SMSP Elapsed Cycles        cycle    431715152
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.07
    Elapsed Cycles                cycle      4516807
    Memory Throughput                 %        89.73
    DRAM Throughput                   %        24.19
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.69
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4417614.12
    Compute (SM) Throughput           %        89.73
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.01
    Achieved Active Warps Per SM           warp        42.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8580885.33
    Total DRAM Elapsed Cycles        cycle    212797440
    Average L1 Active Cycles         cycle   4417614.12
    Total L1 Elapsed Cycles          cycle    261821822
    Average L2 Active Cycles         cycle   4640240.92
    Total L2 Elapsed Cycles          cycle    112452024
    Average SM Active Cycles         cycle   4417614.12
    Total SM Elapsed Cycles          cycle    261821822
    Average SMSP Active Cycles       cycle   4419930.43
    Total SMSP Elapsed Cycles        cycle   1047287288
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       818.42
    Elapsed Cycles                cycle         7080
    Memory Throughput                 %        45.11
    DRAM Throughput                   %        45.11
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        24.49
    SM Active Cycles              cycle      4693.14
    Compute (SM) Throughput           %        17.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.04
    Achieved Active Warps Per SM           warp        33.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24018.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4693.14
    Total L1 Elapsed Cycles          cycle       382792
    Average L2 Active Cycles         cycle      4039.21
    Total L2 Elapsed Cycles          cycle       171096
    Average SM Active Cycles         cycle      4693.14
    Total SM Elapsed Cycles          cycle       382792
    Average SMSP Active Cycles       cycle      4491.22
    Total SMSP Elapsed Cycles        cycle      1531168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.277%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.31% above the average, while the minimum instance value is 2.51% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.78
    Elapsed Cycles                cycle         7567
    Memory Throughput                 %        46.57
    DRAM Throughput                   %        46.57
    Duration                         us         9.63
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        26.29
    SM Active Cycles              cycle      4938.95
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.57
    Achieved Active Warps Per SM           warp        35.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27656
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      4938.95
    Total L1 Elapsed Cycles          cycle       411000
    Average L2 Active Cycles         cycle      4462.96
    Total L2 Elapsed Cycles          cycle       188328
    Average SM Active Cycles         cycle      4938.95
    Total SM Elapsed Cycles          cycle       411000
    Average SMSP Active Cycles       cycle      4891.16
    Total SMSP Elapsed Cycles        cycle      1644000
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       848.23
    Elapsed Cycles                cycle         7716
    Memory Throughput                 %        49.46
    DRAM Throughput                   %        49.46
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.18
    L2 Cache Throughput               %        26.54
    SM Active Cycles              cycle      5130.29
    Compute (SM) Throughput           %        15.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.56
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27688
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      5130.29
    Total L1 Elapsed Cycles          cycle       422750
    Average L2 Active Cycles         cycle      4517.12
    Total L2 Elapsed Cycles          cycle       186456
    Average SM Active Cycles         cycle      5130.29
    Total SM Elapsed Cycles          cycle       422750
    Average SMSP Active Cycles       cycle      5072.86
    Total SMSP Elapsed Cycles        cycle      1691000
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       803.41
    Elapsed Cycles                cycle         7441
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        27.40
    SM Active Cycles              cycle      5029.43
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5029.43
    Total L1 Elapsed Cycles          cycle       417228
    Average L2 Active Cycles         cycle      4453.33
    Total L2 Elapsed Cycles          cycle       180768
    Average SM Active Cycles         cycle      5029.43
    Total SM Elapsed Cycles          cycle       417228
    Average SMSP Active Cycles       cycle      4950.59
    Total SMSP Elapsed Cycles        cycle      1668912
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       843.37
    Elapsed Cycles                cycle      5186285
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.49
    Duration                         ms         6.12
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160301.31
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301274.67
    Total DRAM Elapsed Cycles        cycle    229341184
    Average L1 Active Cycles         cycle   5160301.31
    Total L1 Elapsed Cycles          cycle    299459566
    Average L2 Active Cycles         cycle   4611233.75
    Total L2 Elapsed Cycles          cycle    125457888
    Average SM Active Cycles         cycle   5160301.31
    Total SM Elapsed Cycles          cycle    299459566
    Average SMSP Active Cycles       cycle   5160060.16
    Total SMSP Elapsed Cycles        cycle   1197838264
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.33
    Elapsed Cycles                cycle      1856350
    Memory Throughput                 %        90.43
    DRAM Throughput                   %        90.43
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.09
    L2 Cache Throughput               %        49.57
    SM Active Cycles              cycle   1837020.60
    Compute (SM) Throughput           %        22.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.19
    Achieved Active Warps Per SM           warp        46.65
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12356352
    Total DRAM Elapsed Cycles        cycle     81987584
    Average L1 Active Cycles         cycle   1837020.60
    Total L1 Elapsed Cycles          cycle    107600008
    Average L2 Active Cycles         cycle   1851176.21
    Total L2 Elapsed Cycles          cycle     44910648
    Average SM Active Cycles         cycle   1837020.60
    Total SM Elapsed Cycles          cycle    107600008
    Average SMSP Active Cycles       cycle   1828025.03
    Total SMSP Elapsed Cycles        cycle    430400032
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       826.83
    Elapsed Cycles                cycle      4537694
    Memory Throughput                 %        89.64
    DRAM Throughput                   %        25.16
    Duration                         ms         5.46
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4414897.34
    Compute (SM) Throughput           %        89.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8582970.67
    Total DRAM Elapsed Cycles        cycle    204644352
    Average L1 Active Cycles         cycle   4414897.34
    Total L1 Elapsed Cycles          cycle    262071420
    Average L2 Active Cycles         cycle   4522313.83
    Total L2 Elapsed Cycles          cycle    109769520
    Average SM Active Cycles         cycle   4414897.34
    Total SM Elapsed Cycles          cycle    262071420
    Average SMSP Active Cycles       cycle   4414074.24
    Total SMSP Elapsed Cycles        cycle   1048285680
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.19
    Elapsed Cycles                cycle         6798
    Memory Throughput                 %        44.48
    DRAM Throughput                   %        44.48
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.79
    L2 Cache Throughput               %        24.76
    SM Active Cycles              cycle      4381.95
    Compute (SM) Throughput           %        16.79
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.60
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23760
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4381.95
    Total L1 Elapsed Cycles          cycle       390266
    Average L2 Active Cycles         cycle      4032.04
    Total L2 Elapsed Cycles          cycle       169368
    Average SM Active Cycles         cycle      4381.95
    Total SM Elapsed Cycles          cycle       390266
    Average SMSP Active Cycles       cycle      4323.58
    Total SMSP Elapsed Cycles        cycle      1561064
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.61
    Elapsed Cycles                cycle         7417
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.43
    SM Active Cycles              cycle      5014.71
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.48
    Achieved Active Warps Per SM           warp        34.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5014.71
    Total L1 Elapsed Cycles          cycle       411840
    Average L2 Active Cycles         cycle      4415.50
    Total L2 Elapsed Cycles          cycle       180456
    Average SM Active Cycles         cycle      5014.71
    Total SM Elapsed Cycles          cycle       411840
    Average SMSP Active Cycles       cycle      4870.38
    Total SMSP Elapsed Cycles        cycle      1647360
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.62
    Elapsed Cycles                cycle         7511
    Memory Throughput                 %        47.28
    DRAM Throughput                   %        47.28
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        24.68
    L2 Cache Throughput               %        26.47
    SM Active Cycles              cycle      4954.97
    Compute (SM) Throughput           %        16.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4954.97
    Total L1 Elapsed Cycles          cycle       397314
    Average L2 Active Cycles         cycle      4527.92
    Total L2 Elapsed Cycles          cycle       186912
    Average SM Active Cycles         cycle      4954.97
    Total SM Elapsed Cycles          cycle       397314
    Average SMSP Active Cycles       cycle      4913.04
    Total SMSP Elapsed Cycles        cycle      1589256
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       801.63
    Elapsed Cycles                cycle         7711
    Memory Throughput                 %        47.20
    DRAM Throughput                   %        47.20
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        26.44
    SM Active Cycles              cycle      5083.72
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.24
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27872
    Total DRAM Elapsed Cycles        cycle       354304
    Average L1 Active Cycles         cycle      5083.72
    Total L1 Elapsed Cycles          cycle       408736
    Average L2 Active Cycles         cycle      4517.42
    Total L2 Elapsed Cycles          cycle       187368
    Average SM Active Cycles         cycle      5083.72
    Total SM Elapsed Cycles          cycle       408736
    Average SMSP Active Cycles       cycle      5040.81
    Total SMSP Elapsed Cycles        cycle      1634944
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.05
    Elapsed Cycles                cycle      5164863
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.55
    Duration                         ms         6.49
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.05
    SM Active Cycles              cycle   5160518.86
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6304584
    Total DRAM Elapsed Cycles        cycle    243330048
    Average L1 Active Cycles         cycle   5160518.86
    Total L1 Elapsed Cycles          cycle    299449978
    Average L2 Active Cycles         cycle   4694848.79
    Total L2 Elapsed Cycles          cycle    128586864
    Average SM Active Cycles         cycle   5160518.86
    Total SM Elapsed Cycles          cycle    299449978
    Average SMSP Active Cycles       cycle   5160406.07
    Total SMSP Elapsed Cycles        cycle   1197799912
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.76
    Elapsed Cycles                cycle      1860745
    Memory Throughput                 %        90.08
    DRAM Throughput                   %        90.08
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.19
    L2 Cache Throughput               %        49.37
    SM Active Cycles              cycle   1833694.74
    Compute (SM) Throughput           %        22.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.67
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12335608
    Total DRAM Elapsed Cycles        cycle     82165760
    Average L1 Active Cycles         cycle   1833694.74
    Total L1 Elapsed Cycles          cycle    107298034
    Average L2 Active Cycles         cycle   1869772.67
    Total L2 Elapsed Cycles          cycle     45020544
    Average SM Active Cycles         cycle   1833694.74
    Total SM Elapsed Cycles          cycle    107298034
    Average SMSP Active Cycles       cycle   1848397.72
    Total SMSP Elapsed Cycles        cycle    429192136
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.26
    Elapsed Cycles                cycle      4527450
    Memory Throughput                 %        89.52
    DRAM Throughput                   %        25.24
    Duration                         ms         5.44
    L1/TEX Cache Throughput           %        91.79
    L2 Cache Throughput               %        15.24
    SM Active Cycles              cycle   4412964.19
    Compute (SM) Throughput           %        89.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8575581.33
    Total DRAM Elapsed Cycles        cycle    203851776
    Average L1 Active Cycles         cycle   4412964.19
    Total L1 Elapsed Cycles          cycle    262435350
    Average L2 Active Cycles         cycle   4518693.25
    Total L2 Elapsed Cycles          cycle    109499304
    Average SM Active Cycles         cycle   4412964.19
    Total SM Elapsed Cycles          cycle    262435350
    Average SMSP Active Cycles       cycle   4412923.64
    Total SMSP Elapsed Cycles        cycle   1049741400
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.93
    Elapsed Cycles                cycle         6928
    Memory Throughput                 %        45.27
    DRAM Throughput                   %        45.27
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.77
    L2 Cache Throughput               %        24.88
    SM Active Cycles              cycle      4560.93
    Compute (SM) Throughput           %        17.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.46
    Achieved Active Warps Per SM           warp        33.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4560.93
    Total L1 Elapsed Cycles          cycle       384748
    Average L2 Active Cycles         cycle      3956.42
    Total L2 Elapsed Cycles          cycle       168528
    Average SM Active Cycles         cycle      4560.93
    Total SM Elapsed Cycles          cycle       384748
    Average SMSP Active Cycles       cycle      4414.88
    Total SMSP Elapsed Cycles        cycle      1538992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.003%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.88% above the average, while the minimum instance value is 2.59% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.09
    Elapsed Cycles                cycle         7577
    Memory Throughput                 %        47.79
    DRAM Throughput                   %        47.79
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.74
    L2 Cache Throughput               %        26.81
    SM Active Cycles              cycle      5123.59
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.13
    Achieved Active Warps Per SM           warp        34.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27730.67
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5123.59
    Total L1 Elapsed Cycles          cycle       413706
    Average L2 Active Cycles         cycle      4387.71
    Total L2 Elapsed Cycles          cycle       185088
    Average SM Active Cycles         cycle      5123.59
    Total SM Elapsed Cycles          cycle       413706
    Average SMSP Active Cycles       cycle      4885.91
    Total SMSP Elapsed Cycles        cycle      1654824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       801.07
    Elapsed Cycles                cycle         7539
    Memory Throughput                 %        47.65
    DRAM Throughput                   %        47.65
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.55
    L2 Cache Throughput               %        26.96
    SM Active Cycles              cycle      4953.34
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.75
    Achieved Active Warps Per SM           warp        35.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      4953.34
    Total L1 Elapsed Cycles          cycle       417124
    Average L2 Active Cycles         cycle      4506.29
    Total L2 Elapsed Cycles          cycle       184056
    Average SM Active Cycles         cycle      4953.34
    Total SM Elapsed Cycles          cycle       417124
    Average SMSP Active Cycles       cycle      4992.12
    Total SMSP Elapsed Cycles        cycle      1668496
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       809.73
    Elapsed Cycles                cycle         7579
    Memory Throughput                 %        48.10
    DRAM Throughput                   %        48.10
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        27.01
    SM Active Cycles              cycle      5165.09
    Compute (SM) Throughput           %        15.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.92
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      5165.09
    Total L1 Elapsed Cycles          cycle       418866
    Average L2 Active Cycles         cycle      4462.46
    Total L2 Elapsed Cycles          cycle       183720
    Average SM Active Cycles         cycle      5165.09
    Total SM Elapsed Cycles          cycle       418866
    Average SMSP Active Cycles       cycle      4921.57
    Total SMSP Elapsed Cycles        cycle      1675464
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.11
    Elapsed Cycles                cycle      5185992
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.23
    Duration                         ms         6.22
    L1/TEX Cache Throughput           %        82.67
    L2 Cache Throughput               %        14.38
    SM Active Cycles              cycle   5160853.47
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6303301.33
    Total DRAM Elapsed Cycles        cycle    233012224
    Average L1 Active Cycles         cycle   5160853.47
    Total L1 Elapsed Cycles          cycle    299458828
    Average L2 Active Cycles         cycle   4612427.54
    Total L2 Elapsed Cycles          cycle    125499672
    Average SM Active Cycles         cycle   5160853.47
    Total SM Elapsed Cycles          cycle    299458828
    Average SMSP Active Cycles       cycle   5160447.26
    Total SMSP Elapsed Cycles        cycle   1197835312
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.79
    Elapsed Cycles                cycle      1856842
    Memory Throughput                 %        90.13
    DRAM Throughput                   %        90.13
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.32
    L2 Cache Throughput               %        49.46
    SM Active Cycles              cycle   1856463.81
    Compute (SM) Throughput           %        22.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.28
    Achieved Active Warps Per SM           warp        46.21
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12317357.33
    Total DRAM Elapsed Cycles        cycle     81997824
    Average L1 Active Cycles         cycle   1856463.81
    Total L1 Elapsed Cycles          cycle    106899918
    Average L2 Active Cycles         cycle   1868703.33
    Total L2 Elapsed Cycles          cycle     44968536
    Average SM Active Cycles         cycle   1856463.81
    Total SM Elapsed Cycles          cycle    106899918
    Average SMSP Active Cycles       cycle   1843819.78
    Total SMSP Elapsed Cycles        cycle    427599672
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.90
    Elapsed Cycles                cycle      4537086
    Memory Throughput                 %        89.70
    DRAM Throughput                   %        25.64
    Duration                         ms         5.36
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.20
    SM Active Cycles              cycle   4413772.81
    Compute (SM) Throughput           %        89.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8580026.67
    Total DRAM Elapsed Cycles        cycle    200815616
    Average L1 Active Cycles         cycle   4413772.81
    Total L1 Elapsed Cycles          cycle    261918358
    Average L2 Active Cycles         cycle   4569876.21
    Total L2 Elapsed Cycles          cycle    109829328
    Average SM Active Cycles         cycle   4413772.81
    Total SM Elapsed Cycles          cycle    261918358
    Average SMSP Active Cycles       cycle   4413397.48
    Total SMSP Elapsed Cycles        cycle   1047673432
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       789.71
    Elapsed Cycles                cycle         6863
    Memory Throughput                 %        44.34
    DRAM Throughput                   %        44.34
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.40
    L2 Cache Throughput               %        24.72
    SM Active Cycles              cycle      4448.79
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.51
    Achieved Active Warps Per SM           warp        34.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23834.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4448.79
    Total L1 Elapsed Cycles          cycle       384692
    Average L2 Active Cycles         cycle      4156.33
    Total L2 Elapsed Cycles          cycle       169680
    Average SM Active Cycles         cycle      4448.79
    Total SM Elapsed Cycles          cycle       384692
    Average SMSP Active Cycles       cycle      4405.45
    Total SMSP Elapsed Cycles        cycle      1538768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.19
    Elapsed Cycles                cycle         7464
    Memory Throughput                 %        47.80
    DRAM Throughput                   %        47.80
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      5023.59
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.47
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      5023.59
    Total L1 Elapsed Cycles          cycle       411182
    Average L2 Active Cycles         cycle      4460.04
    Total L2 Elapsed Cycles          cycle       184632
    Average SM Active Cycles         cycle      5023.59
    Total SM Elapsed Cycles          cycle       411182
    Average SMSP Active Cycles       cycle      4901.45
    Total SMSP Elapsed Cycles        cycle      1644728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       788.06
    Elapsed Cycles                cycle         7377
    Memory Throughput                 %        47.98
    DRAM Throughput                   %        47.98
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.19
    SM Active Cycles              cycle      4983.34
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.16
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4983.34
    Total L1 Elapsed Cycles          cycle       412838
    Average L2 Active Cycles         cycle      4559.62
    Total L2 Elapsed Cycles          cycle       182520
    Average SM Active Cycles         cycle      4983.34
    Total SM Elapsed Cycles          cycle       412838
    Average SMSP Active Cycles       cycle      4976.92
    Total SMSP Elapsed Cycles        cycle      1651352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       813.39
    Elapsed Cycles                cycle         7558
    Memory Throughput                 %        48.82
    DRAM Throughput                   %        48.82
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.17
    L2 Cache Throughput               %        27.18
    SM Active Cycles              cycle      4893.43
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.62
    Achieved Active Warps Per SM           warp        36.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4893.43
    Total L1 Elapsed Cycles          cycle       406478
    Average L2 Active Cycles         cycle      4487.92
    Total L2 Elapsed Cycles          cycle       182592
    Average SM Active Cycles         cycle      4893.43
    Total SM Elapsed Cycles          cycle       406478
    Average SMSP Active Cycles       cycle      4970.67
    Total SMSP Elapsed Cycles        cycle      1625912
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.74
    Elapsed Cycles                cycle      5185760
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.26
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160253.41
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6304002.67
    Total DRAM Elapsed Cycles        cycle    232571904
    Average L1 Active Cycles         cycle   5160253.41
    Total L1 Elapsed Cycles          cycle    299490668
    Average L2 Active Cycles         cycle   4612219.29
    Total L2 Elapsed Cycles          cycle    125503704
    Average SM Active Cycles         cycle   5160253.41
    Total SM Elapsed Cycles          cycle    299490668
    Average SMSP Active Cycles       cycle   5160429.48
    Total SMSP Elapsed Cycles        cycle   1197962672
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       848.73
    Elapsed Cycles                cycle      1872583
    Memory Throughput                 %        89.75
    DRAM Throughput                   %        89.75
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        40.51
    L2 Cache Throughput               %        48.94
    SM Active Cycles              cycle   1850631.84
    Compute (SM) Throughput           %        22.23
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.41
    Achieved Active Warps Per SM           warp        46.76
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12313418.67
    Total DRAM Elapsed Cycles        cycle     82320384
    Average L1 Active Cycles         cycle   1850631.84
    Total L1 Elapsed Cycles          cycle    108952090
    Average L2 Active Cycles         cycle   1876514.04
    Total L2 Elapsed Cycles          cycle     45359256
    Average SM Active Cycles         cycle   1850631.84
    Total SM Elapsed Cycles          cycle    108952090
    Average SMSP Active Cycles       cycle   1849705.15
    Total SMSP Elapsed Cycles        cycle    435808360
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.69
    Elapsed Cycles                cycle      4527783
    Memory Throughput                 %        89.51
    DRAM Throughput                   %        25.67
    Duration                         ms         5.35
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        15.23
    SM Active Cycles              cycle   4413365.16
    Compute (SM) Throughput           %        89.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8576101.33
    Total DRAM Elapsed Cycles        cycle    200418304
    Average L1 Active Cycles         cycle   4413365.16
    Total L1 Elapsed Cycles          cycle    262456644
    Average L2 Active Cycles         cycle   4522654.88
    Total L2 Elapsed Cycles          cycle    109591176
    Average SM Active Cycles         cycle   4413365.16
    Total SM Elapsed Cycles          cycle    262456644
    Average SMSP Active Cycles       cycle   4414339.67
    Total SMSP Elapsed Cycles        cycle   1049826576
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       796.42
    Elapsed Cycles                cycle         6886
    Memory Throughput                 %        45.14
    DRAM Throughput                   %        45.14
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        24.92
    SM Active Cycles              cycle      4567.12
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.38
    Achieved Active Warps Per SM           warp        33.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24034.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4567.12
    Total L1 Elapsed Cycles          cycle       383650
    Average L2 Active Cycles         cycle      4001.33
    Total L2 Elapsed Cycles          cycle       168240
    Average SM Active Cycles         cycle      4567.12
    Total SM Elapsed Cycles          cycle       383650
    Average SMSP Active Cycles       cycle      4430.50
    Total SMSP Elapsed Cycles        cycle      1534600
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.55
    Elapsed Cycles                cycle         7386
    Memory Throughput                 %        47.67
    DRAM Throughput                   %        47.67
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        26.95
    SM Active Cycles              cycle      4936.10
    Compute (SM) Throughput           %        16.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.63
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4936.10
    Total L1 Elapsed Cycles          cycle       408202
    Average L2 Active Cycles         cycle      4356.88
    Total L2 Elapsed Cycles          cycle       184080
    Average SM Active Cycles         cycle      4936.10
    Total SM Elapsed Cycles          cycle       408202
    Average SMSP Active Cycles       cycle      4758.72
    Total SMSP Elapsed Cycles        cycle      1632808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       813.24
    Elapsed Cycles                cycle         7554
    Memory Throughput                 %        48.80
    DRAM Throughput                   %        48.80
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.50
    L2 Cache Throughput               %        27.14
    SM Active Cycles              cycle      5022.64
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.53
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5022.64
    Total L1 Elapsed Cycles          cycle       418030
    Average L2 Active Cycles         cycle      4424.92
    Total L2 Elapsed Cycles          cycle       182904
    Average SM Active Cycles         cycle      5022.64
    Total SM Elapsed Cycles          cycle       418030
    Average SMSP Active Cycles       cycle      4943.97
    Total SMSP Elapsed Cycles        cycle      1672120
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       787.15
    Elapsed Cycles                cycle         7574
    Memory Throughput                 %        46.89
    DRAM Throughput                   %        46.89
    Duration                         us         9.60
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        26.49
    SM Active Cycles              cycle      4982.41
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.67
    Achieved Active Warps Per SM           warp        35.36
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      4982.41
    Total L1 Elapsed Cycles          cycle       415336
    Average L2 Active Cycles         cycle      4473.54
    Total L2 Elapsed Cycles          cycle       187368
    Average SM Active Cycles         cycle      4982.41
    Total SM Elapsed Cycles          cycle       415336
    Average SMSP Active Cycles       cycle      4887.20
    Total SMSP Elapsed Cycles        cycle      1661344
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.23
    Elapsed Cycles                cycle      5186446
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.17
    Duration                         ms         6.24
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160560.66
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302218.67
    Total DRAM Elapsed Cycles        cycle    233812992
    Average L1 Active Cycles         cycle   5160560.66
    Total L1 Elapsed Cycles          cycle    299472988
    Average L2 Active Cycles         cycle   4612790.79
    Total L2 Elapsed Cycles          cycle    125486352
    Average SM Active Cycles         cycle   5160560.66
    Total SM Elapsed Cycles          cycle    299472988
    Average SMSP Active Cycles       cycle   5160385.90
    Total SMSP Elapsed Cycles        cycle   1197891952
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       847.83
    Elapsed Cycles                cycle      1859902
    Memory Throughput                 %        90.46
    DRAM Throughput                   %        90.46
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        43.16
    L2 Cache Throughput               %        49.42
    SM Active Cycles              cycle   1845768.40
    Compute (SM) Throughput           %        23.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.73
    Achieved Active Warps Per SM           warp        46.43
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12340610.67
    Total DRAM Elapsed Cycles        cycle     81850368
    Average L1 Active Cycles         cycle   1845768.40
    Total L1 Elapsed Cycles          cycle    102505248
    Average L2 Active Cycles         cycle   1806929.58
    Total L2 Elapsed Cycles          cycle     45036120
    Average SM Active Cycles         cycle   1845768.40
    Total SM Elapsed Cycles          cycle    102505248
    Average SMSP Active Cycles       cycle   1764206.20
    Total SMSP Elapsed Cycles        cycle    410020992
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.52
    Elapsed Cycles                cycle      4553985
    Memory Throughput                 %        89.80
    DRAM Throughput                   %        24.47
    Duration                         ms         5.62
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.01
    SM Active Cycles              cycle   4414865.98
    Compute (SM) Throughput           %        89.80
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8585845.33
    Total DRAM Elapsed Cycles        cycle    210534400
    Average L1 Active Cycles         cycle   4414865.98
    Total L1 Elapsed Cycles          cycle    261619132
    Average L2 Active Cycles         cycle   4564088.38
    Total L2 Elapsed Cycles          cycle    111256920
    Average SM Active Cycles         cycle   4414865.98
    Total SM Elapsed Cycles          cycle    261619132
    Average SMSP Active Cycles       cycle      4413930
    Total SMSP Elapsed Cycles        cycle   1046476528
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       786.88
    Elapsed Cycles                cycle         6839
    Memory Throughput                 %        44.71
    DRAM Throughput                   %        44.71
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        24.81
    SM Active Cycles              cycle      4566.55
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.60
    Achieved Active Warps Per SM           warp        33.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23808
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4566.55
    Total L1 Elapsed Cycles          cycle       383912
    Average L2 Active Cycles         cycle      4045.12
    Total L2 Elapsed Cycles          cycle       168888
    Average SM Active Cycles         cycle      4566.55
    Total SM Elapsed Cycles          cycle       383912
    Average SMSP Active Cycles       cycle      4421.30
    Total SMSP Elapsed Cycles        cycle      1535648
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       798.56
    Elapsed Cycles                cycle         7570
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        26.86
    SM Active Cycles              cycle      5000.03
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.83
    Achieved Active Warps Per SM           warp        35.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5000.03
    Total L1 Elapsed Cycles          cycle       412818
    Average L2 Active Cycles         cycle      4495.17
    Total L2 Elapsed Cycles          cycle       184872
    Average SM Active Cycles         cycle      5000.03
    Total SM Elapsed Cycles          cycle       412818
    Average SMSP Active Cycles       cycle      4968.33
    Total SMSP Elapsed Cycles        cycle      1651272
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       797.43
    Elapsed Cycles                cycle         7664
    Memory Throughput                 %        47.16
    DRAM Throughput                   %        47.16
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        24.35
    L2 Cache Throughput               %        26.46
    SM Active Cycles              cycle      5021.86
    Compute (SM) Throughput           %        16.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.04
    Achieved Active Warps Per SM           warp        35.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5021.86
    Total L1 Elapsed Cycles          cycle       403418
    Average L2 Active Cycles         cycle      4520.25
    Total L2 Elapsed Cycles          cycle       187344
    Average SM Active Cycles         cycle      5021.86
    Total SM Elapsed Cycles          cycle       403418
    Average SMSP Active Cycles       cycle      4994.19
    Total SMSP Elapsed Cycles        cycle      1613672
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       820.61
    Elapsed Cycles                cycle         7626
    Memory Throughput                 %        48.35
    DRAM Throughput                   %        48.35
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.64
    L2 Cache Throughput               %        26.93
    SM Active Cycles              cycle      5090.07
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.44
    Achieved Active Warps Per SM           warp        35.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5090.07
    Total L1 Elapsed Cycles          cycle       415578
    Average L2 Active Cycles         cycle      4468.96
    Total L2 Elapsed Cycles          cycle       184104
    Average SM Active Cycles         cycle      5090.07
    Total SM Elapsed Cycles          cycle       415578
    Average SMSP Active Cycles       cycle      4946.08
    Total SMSP Elapsed Cycles        cycle      1662312
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.22
    Elapsed Cycles                cycle      5186583
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.19
    Duration                         ms         6.23
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160248.40
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302690.67
    Total DRAM Elapsed Cycles        cycle    233570304
    Average L1 Active Cycles         cycle   5160248.40
    Total L1 Elapsed Cycles          cycle    299474246
    Average L2 Active Cycles         cycle   4613435.33
    Total L2 Elapsed Cycles          cycle    125493552
    Average SM Active Cycles         cycle   5160248.40
    Total SM Elapsed Cycles          cycle    299474246
    Average SMSP Active Cycles       cycle   5160599.79
    Total SMSP Elapsed Cycles        cycle   1197896984
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       846.21
    Elapsed Cycles                cycle      1855356
    Memory Throughput                 %        90.65
    DRAM Throughput                   %        90.65
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        40.74
    L2 Cache Throughput               %        49.41
    SM Active Cycles              cycle   1853650.48
    Compute (SM) Throughput           %        22.31
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.23
    Achieved Active Warps Per SM           warp        46.19
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12357312
    Total DRAM Elapsed Cycles        cycle     81788928
    Average L1 Active Cycles         cycle   1853650.48
    Total L1 Elapsed Cycles          cycle    108546218
    Average L2 Active Cycles         cycle   1881766.54
    Total L2 Elapsed Cycles          cycle     44948064
    Average SM Active Cycles         cycle   1853650.48
    Total SM Elapsed Cycles          cycle    108546218
    Average SMSP Active Cycles       cycle   1860380.69
    Total SMSP Elapsed Cycles        cycle    434184872
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.09
    Elapsed Cycles                cycle      4516144
    Memory Throughput                 %        89.78
    DRAM Throughput                   %        24.20
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4419603.05
    Compute (SM) Throughput           %        89.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.97
    Achieved Active Warps Per SM           warp        42.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8583261.33
    Total DRAM Elapsed Cycles        cycle    212764672
    Average L1 Active Cycles         cycle   4419603.05
    Total L1 Elapsed Cycles          cycle    261679360
    Average L2 Active Cycles         cycle      4613801
    Total L2 Elapsed Cycles          cycle    112435056
    Average SM Active Cycles         cycle   4419603.05
    Total SM Elapsed Cycles          cycle    261679360
    Average SMSP Active Cycles       cycle   4417321.93
    Total SMSP Elapsed Cycles        cycle   1046717440
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       803.79
    Elapsed Cycles                cycle         7028
    Memory Throughput                 %        44.68
    DRAM Throughput                   %        44.68
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.86
    L2 Cache Throughput               %        24.49
    SM Active Cycles              cycle      4545.83
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.48
    Achieved Active Warps Per SM           warp        34.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24021.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4545.83
    Total L1 Elapsed Cycles          cycle       384556
    Average L2 Active Cycles         cycle      4078.29
    Total L2 Elapsed Cycles          cycle       170952
    Average SM Active Cycles         cycle      4545.83
    Total SM Elapsed Cycles          cycle       384556
    Average SMSP Active Cycles       cycle      4551.97
    Total SMSP Elapsed Cycles        cycle      1538224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       796.88
    Elapsed Cycles                cycle         7526
    Memory Throughput                 %        48.00
    DRAM Throughput                   %        48.00
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        26.96
    SM Active Cycles              cycle      5051.91
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.18
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5051.91
    Total L1 Elapsed Cycles          cycle       412288
    Average L2 Active Cycles         cycle      4425.21
    Total L2 Elapsed Cycles          cycle       184080
    Average SM Active Cycles         cycle      5051.91
    Total SM Elapsed Cycles          cycle       412288
    Average SMSP Active Cycles       cycle      4946.10
    Total SMSP Elapsed Cycles        cycle      1649152
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       800.53
    Elapsed Cycles                cycle         7646
    Memory Throughput                 %        47.31
    DRAM Throughput                   %        47.31
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.73
    L2 Cache Throughput               %        26.63
    SM Active Cycles              cycle      4918.28
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.45
    Achieved Active Warps Per SM           warp        36.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.55%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      4918.28
    Total L1 Elapsed Cycles          cycle       413762
    Average L2 Active Cycles         cycle      4421.21
    Total L2 Elapsed Cycles          cycle       186528
    Average SM Active Cycles         cycle      4918.28
    Total SM Elapsed Cycles          cycle       413762
    Average SMSP Active Cycles       cycle      4979.69
    Total SMSP Elapsed Cycles        cycle      1655048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       804.49
    Elapsed Cycles                cycle         7528
    Memory Throughput                 %        48.37
    DRAM Throughput                   %        48.37
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.22
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle      5016.17
    Compute (SM) Throughput           %        15.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27738.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5016.17
    Total L1 Elapsed Cycles          cycle       423162
    Average L2 Active Cycles         cycle      4398.21
    Total L2 Elapsed Cycles          cycle       183312
    Average SM Active Cycles         cycle      5016.17
    Total SM Elapsed Cycles          cycle       423162
    Average SMSP Active Cycles       cycle      4948.15
    Total SMSP Elapsed Cycles        cycle      1692648
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.15
    Elapsed Cycles                cycle      5184382
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.19
    Duration                         ms         6.24
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160123.79
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6308674.67
    Total DRAM Elapsed Cycles        cycle    233852928
    Average L1 Active Cycles         cycle   5160123.79
    Total L1 Elapsed Cycles          cycle    299441008
    Average L2 Active Cycles         cycle   4610936.42
    Total L2 Elapsed Cycles          cycle    125504040
    Average SM Active Cycles         cycle   5160123.79
    Total SM Elapsed Cycles          cycle    299441008
    Average SMSP Active Cycles       cycle   5160087.59
    Total SMSP Elapsed Cycles        cycle   1197764032
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.49
    Elapsed Cycles                cycle      1761241
    Memory Throughput                 %        89.29
    DRAM Throughput                   %        89.29
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.54
    L2 Cache Throughput               %        50.99
    SM Active Cycles              cycle   1742940.26
    Compute (SM) Throughput           %        23.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.56
    Achieved Active Warps Per SM           warp        46.83
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12291640
    Total DRAM Elapsed Cycles        cycle     82598912
    Average L1 Active Cycles         cycle   1742940.26
    Total L1 Elapsed Cycles          cycle    101467344
    Average L2 Active Cycles         cycle   1798894.04
    Total L2 Elapsed Cycles          cycle     43655160
    Average SM Active Cycles         cycle   1742940.26
    Total SM Elapsed Cycles          cycle    101467344
    Average SMSP Active Cycles       cycle   1735693.36
    Total SMSP Elapsed Cycles        cycle    405869376
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       806.60
    Elapsed Cycles                cycle      4541361
    Memory Throughput                 %        89.59
    DRAM Throughput                   %        24.54
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        15.06
    SM Active Cycles              cycle   4413413.36
    Compute (SM) Throughput           %        89.59
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8588096
    Total DRAM Elapsed Cycles        cycle    209940480
    Average L1 Active Cycles         cycle   4413413.36
    Total L1 Elapsed Cycles          cycle    262242160
    Average L2 Active Cycles         cycle   4564806.67
    Total L2 Elapsed Cycles          cycle    110942400
    Average SM Active Cycles         cycle   4413413.36
    Total SM Elapsed Cycles          cycle    262242160
    Average SMSP Active Cycles       cycle   4412621.23
    Total SMSP Elapsed Cycles        cycle   1048968640
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       785.99
    Elapsed Cycles                cycle         6983
    Memory Throughput                 %        43.73
    DRAM Throughput                   %        43.73
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        25.32
    L2 Cache Throughput               %        24.33
    SM Active Cycles              cycle      4462.14
    Compute (SM) Throughput           %        17.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.08
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23730.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4462.14
    Total L1 Elapsed Cycles          cycle       380170
    Average L2 Active Cycles         cycle      3957.58
    Total L2 Elapsed Cycles          cycle       172488
    Average SM Active Cycles         cycle      4462.14
    Total SM Elapsed Cycles          cycle       380170
    Average SMSP Active Cycles       cycle      4282.58
    Total SMSP Elapsed Cycles        cycle      1520680
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       798.54
    Elapsed Cycles                cycle         7597
    Memory Throughput                 %        47.37
    DRAM Throughput                   %        47.37
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        26.70
    SM Active Cycles              cycle      4975.62
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.49
    Achieved Active Warps Per SM           warp        35.75
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      4975.62
    Total L1 Elapsed Cycles          cycle       413170
    Average L2 Active Cycles         cycle      4485.12
    Total L2 Elapsed Cycles          cycle       185880
    Average SM Active Cycles         cycle      4975.62
    Total SM Elapsed Cycles          cycle       413170
    Average SMSP Active Cycles       cycle      5041.57
    Total SMSP Elapsed Cycles        cycle      1652680
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       804.78
    Elapsed Cycles                cycle         7714
    Memory Throughput                 %        47.11
    DRAM Throughput                   %        47.11
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.56
    L2 Cache Throughput               %        26.44
    SM Active Cycles              cycle      4986.16
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.54
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4986.16
    Total L1 Elapsed Cycles          cycle       416908
    Average L2 Active Cycles         cycle      4390.92
    Total L2 Elapsed Cycles          cycle       187512
    Average SM Active Cycles         cycle      4986.16
    Total SM Elapsed Cycles          cycle       416908
    Average SMSP Active Cycles       cycle      4872.30
    Total SMSP Elapsed Cycles        cycle      1667632
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.139%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.58% above the average, while the minimum instance value is 11.52% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.59
    Elapsed Cycles                cycle         7605
    Memory Throughput                 %        47.55
    DRAM Throughput                   %        47.55
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        26.77
    SM Active Cycles              cycle      5056.60
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.98
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5056.60
    Total L1 Elapsed Cycles          cycle       410838
    Average L2 Active Cycles         cycle      4487.12
    Total L2 Elapsed Cycles          cycle       185424
    Average SM Active Cycles         cycle      5056.60
    Total SM Elapsed Cycles          cycle       410838
    Average SMSP Active Cycles       cycle      4963.32
    Total SMSP Elapsed Cycles        cycle      1643352
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.05
    Elapsed Cycles                cycle      5187163
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.22
    Duration                         ms         6.23
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160585.97
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6307568
    Total DRAM Elapsed Cycles        cycle    233312256
    Average L1 Active Cycles         cycle   5160585.97
    Total L1 Elapsed Cycles          cycle    299484108
    Average L2 Active Cycles         cycle   4611382.25
    Total L2 Elapsed Cycles          cycle    125467656
    Average SM Active Cycles         cycle   5160585.97
    Total SM Elapsed Cycles          cycle    299484108
    Average SMSP Active Cycles       cycle   5160436.03
    Total SMSP Elapsed Cycles        cycle   1197936432
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       847.09
    Elapsed Cycles                cycle      1865635
    Memory Throughput                 %        90.22
    DRAM Throughput                   %        90.22
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.19
    L2 Cache Throughput               %        49.20
    SM Active Cycles              cycle   1846273.40
    Compute (SM) Throughput           %        22.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.09
    Achieved Active Warps Per SM           warp        46.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12352216
    Total DRAM Elapsed Cycles        cycle     82151424
    Average L1 Active Cycles         cycle   1846273.40
    Total L1 Elapsed Cycles          cycle    107320096
    Average L2 Active Cycles         cycle   1881039.21
    Total L2 Elapsed Cycles          cycle     45153600
    Average SM Active Cycles         cycle   1846273.40
    Total SM Elapsed Cycles          cycle    107320096
    Average SMSP Active Cycles       cycle   1861800.63
    Total SMSP Elapsed Cycles        cycle    429280384
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.82
    Elapsed Cycles                cycle      4534849
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        25.55
    Duration                         ms         5.38
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4413199.71
    Compute (SM) Throughput           %        89.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8578986.67
    Total DRAM Elapsed Cycles        cycle    201440256
    Average L1 Active Cycles         cycle   4413199.71
    Total L1 Elapsed Cycles          cycle    261690066
    Average L2 Active Cycles         cycle   4525370.67
    Total L2 Elapsed Cycles          cycle    109722288
    Average SM Active Cycles         cycle   4413199.71
    Total SM Elapsed Cycles          cycle    261690066
    Average SMSP Active Cycles       cycle   4415442.38
    Total SMSP Elapsed Cycles        cycle   1046760264
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       798.99
    Elapsed Cycles                cycle         6953
    Memory Throughput                 %        44.79
    DRAM Throughput                   %        44.79
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.27
    L2 Cache Throughput               %        24.64
    SM Active Cycles              cycle      4471.72
    Compute (SM) Throughput           %        17.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.92
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24000
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4471.72
    Total L1 Elapsed Cycles          cycle       382834
    Average L2 Active Cycles         cycle      4058.67
    Total L2 Elapsed Cycles          cycle       169944
    Average SM Active Cycles         cycle      4471.72
    Total SM Elapsed Cycles          cycle       382834
    Average SMSP Active Cycles       cycle      4475.78
    Total SMSP Elapsed Cycles        cycle      1531336
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       798.31
    Elapsed Cycles                cycle         7486
    Memory Throughput                 %        48.25
    DRAM Throughput                   %        48.25
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.09
    L2 Cache Throughput               %        27.06
    SM Active Cycles              cycle         4985
    Compute (SM) Throughput           %        15.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.07
    Achieved Active Warps Per SM           warp        34.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27752
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle         4985
    Total L1 Elapsed Cycles          cycle       425594
    Average L2 Active Cycles         cycle      4595.21
    Total L2 Elapsed Cycles          cycle       183264
    Average SM Active Cycles         cycle         4985
    Total SM Elapsed Cycles          cycle       425594
    Average SMSP Active Cycles       cycle      5139.66
    Total SMSP Elapsed Cycles        cycle      1702376
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       797.69
    Elapsed Cycles                cycle         7484
    Memory Throughput                 %        48.06
    DRAM Throughput                   %        48.06
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        22.84
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle      5045.36
    Compute (SM) Throughput           %        15.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.60
    Achieved Active Warps Per SM           warp        34.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      5045.36
    Total L1 Elapsed Cycles          cycle       430118
    Average L2 Active Cycles         cycle      4436.75
    Total L2 Elapsed Cycles          cycle       183192
    Average SM Active Cycles         cycle      5045.36
    Total SM Elapsed Cycles          cycle       430118
    Average SMSP Active Cycles       cycle      4911.23
    Total SMSP Elapsed Cycles        cycle      1720472
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.60
    Elapsed Cycles                cycle         7525
    Memory Throughput                 %        47.28
    DRAM Throughput                   %        47.28
    Duration                         us         9.60
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        26.43
    SM Active Cycles              cycle      5035.38
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27837.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5035.38
    Total L1 Elapsed Cycles          cycle       416348
    Average L2 Active Cycles         cycle      4468.54
    Total L2 Elapsed Cycles          cycle       187584
    Average SM Active Cycles         cycle      5035.38
    Total SM Elapsed Cycles          cycle       416348
    Average SMSP Active Cycles       cycle      4881.97
    Total SMSP Elapsed Cycles        cycle      1665392
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       828.77
    Elapsed Cycles                cycle      5186052
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.22
    Duration                         ms         6.23
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160528.22
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6308218.67
    Total DRAM Elapsed Cycles        cycle    233381888
    Average L1 Active Cycles         cycle   5160528.22
    Total L1 Elapsed Cycles          cycle    299479488
    Average L2 Active Cycles         cycle      4613946
    Total L2 Elapsed Cycles          cycle    125464776
    Average SM Active Cycles         cycle   5160528.22
    Total SM Elapsed Cycles          cycle    299479488
    Average SMSP Active Cycles       cycle   5160959.74
    Total SMSP Elapsed Cycles        cycle   1197917952
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.15
    Elapsed Cycles                cycle      1851507
    Memory Throughput                 %        90.69
    DRAM Throughput                   %        90.69
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        40.99
    L2 Cache Throughput               %        49.44
    SM Active Cycles              cycle   1853989.05
    Compute (SM) Throughput           %        22.45
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.94
    Achieved Active Warps Per SM           warp        46.05
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12350770.67
    Total DRAM Elapsed Cycles        cycle     81709056
    Average L1 Active Cycles         cycle   1853989.05
    Total L1 Elapsed Cycles          cycle    107845836
    Average L2 Active Cycles         cycle   1872228.58
    Total L2 Elapsed Cycles          cycle     44838216
    Average SM Active Cycles         cycle   1853989.05
    Total SM Elapsed Cycles          cycle    107845836
    Average SMSP Active Cycles       cycle   1848067.06
    Total SMSP Elapsed Cycles        cycle    431383344
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.72
    Elapsed Cycles                cycle      4532440
    Memory Throughput                 %        89.78
    DRAM Throughput                   %        25.76
    Duration                         ms         5.34
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4414266.52
    Compute (SM) Throughput           %        89.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8584549.33
    Total DRAM Elapsed Cycles        cycle    199943168
    Average L1 Active Cycles         cycle   4414266.52
    Total L1 Elapsed Cycles          cycle    261671016
    Average L2 Active Cycles         cycle   4527437.62
    Total L2 Elapsed Cycles          cycle    109684392
    Average SM Active Cycles         cycle   4414266.52
    Total SM Elapsed Cycles          cycle    261671016
    Average SMSP Active Cycles       cycle   4414939.24
    Total SMSP Elapsed Cycles        cycle   1046684064
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.06
    Elapsed Cycles                cycle         6814
    Memory Throughput                 %        44.28
    DRAM Throughput                   %        44.28
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.31
    L2 Cache Throughput               %        24.73
    SM Active Cycles              cycle      4464.67
    Compute (SM) Throughput           %        17.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.94
    Achieved Active Warps Per SM           warp        34.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23805.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4464.67
    Total L1 Elapsed Cycles          cycle       376916
    Average L2 Active Cycles         cycle      4042.71
    Total L2 Elapsed Cycles          cycle       169560
    Average SM Active Cycles         cycle      4464.67
    Total SM Elapsed Cycles          cycle       376916
    Average SMSP Active Cycles       cycle      4371.22
    Total SMSP Elapsed Cycles        cycle      1507664
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       832.48
    Elapsed Cycles                cycle         7739
    Memory Throughput                 %        48.48
    DRAM Throughput                   %        48.48
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        22.85
    L2 Cache Throughput               %        26.56
    SM Active Cycles              cycle      5002.16
    Compute (SM) Throughput           %        15.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 23.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.11
    Achieved Active Warps Per SM           warp        36.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5002.16
    Total L1 Elapsed Cycles          cycle       429992
    Average L2 Active Cycles         cycle      4545.21
    Total L2 Elapsed Cycles          cycle       186816
    Average SM Active Cycles         cycle      5002.16
    Total SM Elapsed Cycles          cycle       429992
    Average SMSP Active Cycles       cycle      5001.13
    Total SMSP Elapsed Cycles        cycle      1719968
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.08
    Elapsed Cycles                cycle         7595
    Memory Throughput                 %        47.61
    DRAM Throughput                   %        47.61
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        26.75
    SM Active Cycles              cycle      5044.74
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5044.74
    Total L1 Elapsed Cycles          cycle       419388
    Average L2 Active Cycles         cycle      4547.25
    Total L2 Elapsed Cycles          cycle       185544
    Average SM Active Cycles         cycle      5044.74
    Total SM Elapsed Cycles          cycle       419388
    Average SMSP Active Cycles       cycle      5084.46
    Total SMSP Elapsed Cycles        cycle      1677552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       799.38
    Elapsed Cycles                cycle         7558
    Memory Throughput                 %        47.69
    DRAM Throughput                   %        47.69
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        26.89
    SM Active Cycles              cycle      5212.97
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.63
    Achieved Active Warps Per SM           warp        33.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27837.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5212.97
    Total L1 Elapsed Cycles          cycle       411950
    Average L2 Active Cycles         cycle      4378.62
    Total L2 Elapsed Cycles          cycle       184464
    Average SM Active Cycles         cycle      5212.97
    Total SM Elapsed Cycles          cycle       411950
    Average SMSP Active Cycles       cycle      4847.82
    Total SMSP Elapsed Cycles        cycle      1647800
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.68
    Elapsed Cycles                cycle      5185637
    Memory Throughput                 %        82.65
    DRAM Throughput                   %        16.51
    Duration                         ms         6.11
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160559.10
    Compute (SM) Throughput           %        82.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6300696
    Total DRAM Elapsed Cycles        cycle    228996096
    Average L1 Active Cycles         cycle   5160559.10
    Total L1 Elapsed Cycles          cycle    299429464
    Average L2 Active Cycles         cycle   4611026.58
    Total L2 Elapsed Cycles          cycle    125452296
    Average SM Active Cycles         cycle   5160559.10
    Total SM Elapsed Cycles          cycle    299429464
    Average SMSP Active Cycles       cycle   5160457.83
    Total SMSP Elapsed Cycles        cycle   1197717856
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       851.53
    Elapsed Cycles                cycle      1877926
    Memory Throughput                 %        90.03
    DRAM Throughput                   %        90.03
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.04
    L2 Cache Throughput               %        48.84
    SM Active Cycles              cycle   1859247.69
    Compute (SM) Throughput           %        22.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.23
    Achieved Active Warps Per SM           warp        46.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12344066.67
    Total DRAM Elapsed Cycles        cycle     82270208
    Average L1 Active Cycles         cycle   1859247.69
    Total L1 Elapsed Cycles          cycle    107742504
    Average L2 Active Cycles         cycle   1901243.33
    Total L2 Elapsed Cycles          cycle     45460680
    Average SM Active Cycles         cycle   1859247.69
    Total SM Elapsed Cycles          cycle    107742504
    Average SMSP Active Cycles       cycle   1882429.25
    Total SMSP Elapsed Cycles        cycle    430970016
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.57
    Elapsed Cycles                cycle      4525958
    Memory Throughput                 %        89.89
    DRAM Throughput                   %        25.78
    Duration                         ms         5.33
    L1/TEX Cache Throughput           %        91.80
    L2 Cache Throughput               %        15.25
    SM Active Cycles              cycle   4412259.72
    Compute (SM) Throughput           %        89.89
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.12
    Achieved Active Warps Per SM           warp        42.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8586392
    Total DRAM Elapsed Cycles        cycle    199867392
    Average L1 Active Cycles         cycle   4412259.72
    Total L1 Elapsed Cycles          cycle    261358602
    Average L2 Active Cycles         cycle   4526067.62
    Total L2 Elapsed Cycles          cycle    109475568
    Average SM Active Cycles         cycle   4412259.72
    Total SM Elapsed Cycles          cycle    261358602
    Average SMSP Active Cycles       cycle   4415283.64
    Total SMSP Elapsed Cycles        cycle   1045434408
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       797.97
    Elapsed Cycles                cycle         7106
    Memory Throughput                 %        43.85
    DRAM Throughput                   %        43.85
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        24.14
    SM Active Cycles              cycle      4566.40
    Compute (SM) Throughput           %        17.42
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.14
    Achieved Active Warps Per SM           warp        34.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24021.33
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4566.40
    Total L1 Elapsed Cycles          cycle       376236
    Average L2 Active Cycles         cycle      4050.17
    Total L2 Elapsed Cycles          cycle       173544
    Average SM Active Cycles         cycle      4566.40
    Total SM Elapsed Cycles          cycle       376236
    Average SMSP Active Cycles       cycle      4482.35
    Total SMSP Elapsed Cycles        cycle      1504944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.94
    Elapsed Cycles                cycle         7541
    Memory Throughput                 %        48.09
    DRAM Throughput                   %        48.09
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.50
    L2 Cache Throughput               %        26.94
    SM Active Cycles              cycle      5140.21
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.61
    Achieved Active Warps Per SM           warp        33.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5140.21
    Total L1 Elapsed Cycles          cycle       417882
    Average L2 Active Cycles         cycle      4510.83
    Total L2 Elapsed Cycles          cycle       184320
    Average SM Active Cycles         cycle      5140.21
    Total SM Elapsed Cycles          cycle       417882
    Average SMSP Active Cycles       cycle      4965.47
    Total SMSP Elapsed Cycles        cycle      1671528
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.07
    Elapsed Cycles                cycle         7572
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.61
    L2 Cache Throughput               %        26.81
    SM Active Cycles              cycle      5150.21
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.00
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5150.21
    Total L1 Elapsed Cycles          cycle       415926
    Average L2 Active Cycles         cycle      4504.71
    Total L2 Elapsed Cycles          cycle       185016
    Average SM Active Cycles         cycle      5150.21
    Total SM Elapsed Cycles          cycle       415926
    Average SMSP Active Cycles       cycle      4939.10
    Total SMSP Elapsed Cycles        cycle      1663704
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       813.32
    Elapsed Cycles                cycle         7581
    Memory Throughput                 %        48.35
    DRAM Throughput                   %        48.35
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.39
    L2 Cache Throughput               %        27.03
    SM Active Cycles              cycle      5016.52
    Compute (SM) Throughput           %        15.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5016.52
    Total L1 Elapsed Cycles          cycle       420174
    Average L2 Active Cycles         cycle      4581.04
    Total L2 Elapsed Cycles          cycle       183552
    Average SM Active Cycles         cycle      5016.52
    Total SM Elapsed Cycles          cycle       420174
    Average SMSP Active Cycles       cycle      4942.19
    Total SMSP Elapsed Cycles        cycle      1680696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.231%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 12.07% above the average, while the minimum instance value is 4.56% below the       
          average.                                                                                                      

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.00
    Elapsed Cycles                cycle      5186604
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.78
    Duration                         ms         6.40
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.26
    SM Active Cycles              cycle   5160398.33
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6304706.67
    Total DRAM Elapsed Cycles        cycle    239683584
    Average L1 Active Cycles         cycle   5160398.33
    Total L1 Elapsed Cycles          cycle    299463328
    Average L2 Active Cycles         cycle   4641478.62
    Total L2 Elapsed Cycles          cycle    126660264
    Average SM Active Cycles         cycle   5160398.33
    Total SM Elapsed Cycles          cycle    299463328
    Average SMSP Active Cycles       cycle   5159884.47
    Total SMSP Elapsed Cycles        cycle   1197853312
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       853.24
    Elapsed Cycles                cycle      1882662
    Memory Throughput                 %        89.96
    DRAM Throughput                   %        89.96
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.12
    L2 Cache Throughput               %        48.77
    SM Active Cycles              cycle   1842663.02
    Compute (SM) Throughput           %        22.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.33
    Achieved Active Warps Per SM           warp        47.20
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12340410.67
    Total DRAM Elapsed Cycles        cycle     82310144
    Average L1 Active Cycles         cycle   1842663.02
    Total L1 Elapsed Cycles          cycle    107404178
    Average L2 Active Cycles         cycle   1867614.12
    Total L2 Elapsed Cycles          cycle     45571344
    Average SM Active Cycles         cycle   1842663.02
    Total SM Elapsed Cycles          cycle    107404178
    Average SMSP Active Cycles       cycle   1842836.16
    Total SMSP Elapsed Cycles        cycle    429616712
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.12
    Elapsed Cycles                cycle      4516144
    Memory Throughput                 %        89.67
    DRAM Throughput                   %        24.20
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.73
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4415706.71
    Compute (SM) Throughput           %        89.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.05
    Achieved Active Warps Per SM           warp        42.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8581802.67
    Total DRAM Elapsed Cycles        cycle    212739072
    Average L1 Active Cycles         cycle   4415706.71
    Total L1 Elapsed Cycles          cycle    261991684
    Average L2 Active Cycles         cycle   4519658.12
    Total L2 Elapsed Cycles          cycle    112420920
    Average SM Active Cycles         cycle   4415706.71
    Total SM Elapsed Cycles          cycle    261991684
    Average SMSP Active Cycles       cycle   4415496.31
    Total SMSP Elapsed Cycles        cycle   1047966736
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       796.21
    Elapsed Cycles                cycle         7004
    Memory Throughput                 %        44.07
    DRAM Throughput                   %        44.07
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        25.25
    L2 Cache Throughput               %        24.44
    SM Active Cycles              cycle      4475.41
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.71
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23765.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4475.41
    Total L1 Elapsed Cycles          cycle       383664
    Average L2 Active Cycles         cycle      4061.75
    Total L2 Elapsed Cycles          cycle       171624
    Average SM Active Cycles         cycle      4475.41
    Total SM Elapsed Cycles          cycle       383664
    Average SMSP Active Cycles       cycle      4403.41
    Total SMSP Elapsed Cycles        cycle      1534656
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.63
    Elapsed Cycles                cycle         7498
    Memory Throughput                 %        47.00
    DRAM Throughput                   %        47.00
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        26.55
    SM Active Cycles              cycle      4976.28
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27672
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4976.28
    Total L1 Elapsed Cycles          cycle       413988
    Average L2 Active Cycles         cycle      4504.83
    Total L2 Elapsed Cycles          cycle       186792
    Average SM Active Cycles         cycle      4976.28
    Total SM Elapsed Cycles          cycle       413988
    Average SMSP Active Cycles       cycle      4927.14
    Total SMSP Elapsed Cycles        cycle      1655952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       814.86
    Elapsed Cycles                cycle         7706
    Memory Throughput                 %        48.00
    DRAM Throughput                   %        48.00
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.40
    L2 Cache Throughput               %        26.59
    SM Active Cycles              cycle      5023.19
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.83
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5023.19
    Total L1 Elapsed Cycles          cycle       419746
    Average L2 Active Cycles         cycle      4492.83
    Total L2 Elapsed Cycles          cycle       186432
    Average SM Active Cycles         cycle      5023.19
    Total SM Elapsed Cycles          cycle       419746
    Average SMSP Active Cycles       cycle      5042.41
    Total SMSP Elapsed Cycles        cycle      1678984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       787.63
    Elapsed Cycles                cycle         7474
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        26.83
    SM Active Cycles              cycle      4866.28
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.79
    Achieved Active Warps Per SM           warp        35.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4866.28
    Total L1 Elapsed Cycles          cycle       408812
    Average L2 Active Cycles         cycle      4384.29
    Total L2 Elapsed Cycles          cycle       184824
    Average SM Active Cycles         cycle      4866.28
    Total SM Elapsed Cycles          cycle       408812
    Average SMSP Active Cycles       cycle      4813.34
    Total SMSP Elapsed Cycles        cycle      1635248
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.17
    Elapsed Cycles                cycle      5186164
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        15.78
    Duration                         ms         6.40
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.26
    SM Active Cycles              cycle   5160465.14
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6303642.67
    Total DRAM Elapsed Cycles        cycle    239619072
    Average L1 Active Cycles         cycle   5160465.14
    Total L1 Elapsed Cycles          cycle    299451430
    Average L2 Active Cycles         cycle   4610797.96
    Total L2 Elapsed Cycles          cycle    126627024
    Average SM Active Cycles         cycle   5160465.14
    Total SM Elapsed Cycles          cycle    299451430
    Average SMSP Active Cycles       cycle   5160561.26
    Total SMSP Elapsed Cycles        cycle   1197805720
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.17
    Elapsed Cycles                cycle      1784514
    Memory Throughput                 %        89.82
    DRAM Throughput                   %        89.82
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        42.96
    L2 Cache Throughput               %        50.89
    SM Active Cycles              cycle   1763814.34
    Compute (SM) Throughput           %        23.53
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.31
    Achieved Active Warps Per SM           warp        46.71
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12350778.67
    Total DRAM Elapsed Cycles        cycle     82503680
    Average L1 Active Cycles         cycle   1763814.34
    Total L1 Elapsed Cycles          cycle    102902102
    Average L2 Active Cycles         cycle   1803546.54
    Total L2 Elapsed Cycles          cycle     43676256
    Average SM Active Cycles         cycle   1763814.34
    Total SM Elapsed Cycles          cycle    102902102
    Average SMSP Active Cycles       cycle   1764076.01
    Total SMSP Elapsed Cycles        cycle    411608408
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.05
    Elapsed Cycles                cycle      4533449
    Memory Throughput                 %        89.64
    DRAM Throughput                   %        24.56
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        91.74
    L2 Cache Throughput               %        15.09
    SM Active Cycles              cycle   4415237.07
    Compute (SM) Throughput           %        89.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8574792
    Total DRAM Elapsed Cycles        cycle    209473536
    Average L1 Active Cycles         cycle   4415237.07
    Total L1 Elapsed Cycles          cycle    262078160
    Average L2 Active Cycles         cycle   4564284.29
    Total L2 Elapsed Cycles          cycle    110695824
    Average SM Active Cycles         cycle   4415237.07
    Total SM Elapsed Cycles          cycle    262078160
    Average SMSP Active Cycles       cycle   4415276.76
    Total SMSP Elapsed Cycles        cycle   1048312640
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.06
    Elapsed Cycles                cycle         6986
    Memory Throughput                 %        44.72
    DRAM Throughput                   %        44.72
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.90
    L2 Cache Throughput               %        24.62
    SM Active Cycles              cycle      4538.41
    Compute (SM) Throughput           %        17.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.94
    Achieved Active Warps Per SM           warp        34.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24040
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4538.41
    Total L1 Elapsed Cycles          cycle       385406
    Average L2 Active Cycles         cycle      4054.71
    Total L2 Elapsed Cycles          cycle       170472
    Average SM Active Cycles         cycle      4538.41
    Total SM Elapsed Cycles          cycle       385406
    Average SMSP Active Cycles       cycle      4465.09
    Total SMSP Elapsed Cycles        cycle      1541624
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       798.50
    Elapsed Cycles                cycle         7600
    Memory Throughput                 %        47.67
    DRAM Throughput                   %        47.67
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.37
    L2 Cache Throughput               %        26.77
    SM Active Cycles              cycle      5072.50
    Compute (SM) Throughput           %        15.59
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.41
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5072.50
    Total L1 Elapsed Cycles          cycle       420288
    Average L2 Active Cycles         cycle      4515.12
    Total L2 Elapsed Cycles          cycle       185424
    Average SM Active Cycles         cycle      5072.50
    Total SM Elapsed Cycles          cycle       420288
    Average SMSP Active Cycles       cycle      5067.78
    Total SMSP Elapsed Cycles        cycle      1681152
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       798.25
    Elapsed Cycles                cycle         7409
    Memory Throughput                 %        48.40
    DRAM Throughput                   %        48.40
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.24
    L2 Cache Throughput               %        27.33
    SM Active Cycles              cycle      5094.34
    Compute (SM) Throughput           %        15.52
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.53
    Achieved Active Warps Per SM           warp        33.86
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5094.34
    Total L1 Elapsed Cycles          cycle       422308
    Average L2 Active Cycles         cycle      4509.62
    Total L2 Elapsed Cycles          cycle       181440
    Average SM Active Cycles         cycle      5094.34
    Total SM Elapsed Cycles          cycle       422308
    Average SMSP Active Cycles       cycle      4959.34
    Total SMSP Elapsed Cycles        cycle      1689232
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.38
    Elapsed Cycles                cycle         7547
    Memory Throughput                 %        47.99
    DRAM Throughput                   %        47.99
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        22.64
    L2 Cache Throughput               %        26.90
    SM Active Cycles              cycle      5006.36
    Compute (SM) Throughput           %        15.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5006.36
    Total L1 Elapsed Cycles          cycle       433994
    Average L2 Active Cycles         cycle      4516.04
    Total L2 Elapsed Cycles          cycle       184320
    Average SM Active Cycles         cycle      5006.36
    Total SM Elapsed Cycles          cycle       433994
    Average SMSP Active Cycles       cycle      5067.62
    Total SMSP Elapsed Cycles        cycle      1735976
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       833.47
    Elapsed Cycles                cycle      5184826
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.29
    Duration                         ms         6.19
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160644.31
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6301424
    Total DRAM Elapsed Cycles        cycle    232095744
    Average L1 Active Cycles         cycle   5160644.31
    Total L1 Elapsed Cycles          cycle    299452156
    Average L2 Active Cycles         cycle   4609457.54
    Total L2 Elapsed Cycles          cycle    125519400
    Average SM Active Cycles         cycle   5160644.31
    Total SM Elapsed Cycles          cycle    299452156
    Average SMSP Active Cycles       cycle   5160236.33
    Total SMSP Elapsed Cycles        cycle   1197808624
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.02
    Elapsed Cycles                cycle      1749730
    Memory Throughput                 %        90.08
    DRAM Throughput                   %        90.08
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        40.88
    L2 Cache Throughput               %        51.27
    SM Active Cycles              cycle   1739863.03
    Compute (SM) Throughput           %        22.41
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.02
    Achieved Active Warps Per SM           warp        46.57
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12330922.67
    Total DRAM Elapsed Cycles        cycle     82135040
    Average L1 Active Cycles         cycle   1739863.03
    Total L1 Elapsed Cycles          cycle    108042088
    Average L2 Active Cycles         cycle   1860133.92
    Total L2 Elapsed Cycles          cycle     43407696
    Average SM Active Cycles         cycle   1739863.03
    Total SM Elapsed Cycles          cycle    108042088
    Average SMSP Active Cycles       cycle   1834776.16
    Total SMSP Elapsed Cycles        cycle    432168352
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       846.64
    Elapsed Cycles                cycle      4532031
    Memory Throughput                 %        89.70
    DRAM Throughput                   %        25.78
    Duration                         ms         5.33
    L1/TEX Cache Throughput           %        91.80
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4412271.36
    Compute (SM) Throughput           %        89.70
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.13
    Achieved Active Warps Per SM           warp        42.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8580352
    Total DRAM Elapsed Cycles        cycle    199732224
    Average L1 Active Cycles         cycle   4412271.36
    Total L1 Elapsed Cycles          cycle    261920536
    Average L2 Active Cycles         cycle   4562501.96
    Total L2 Elapsed Cycles          cycle    109708224
    Average SM Active Cycles         cycle   4412271.36
    Total SM Elapsed Cycles          cycle    261920536
    Average SMSP Active Cycles       cycle   4412896.05
    Total SMSP Elapsed Cycles        cycle   1047682144
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       784.25
    Elapsed Cycles                cycle         6915
    Memory Throughput                 %        44.22
    DRAM Throughput                   %        44.22
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        25.23
    L2 Cache Throughput               %        24.48
    SM Active Cycles              cycle         4479
    Compute (SM) Throughput           %        17.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.89
    Achieved Active Warps Per SM           warp        34.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23850.67
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle         4479
    Total L1 Elapsed Cycles          cycle       383028
    Average L2 Active Cycles         cycle      3984.54
    Total L2 Elapsed Cycles          cycle       171216
    Average SM Active Cycles         cycle         4479
    Total SM Elapsed Cycles          cycle       383028
    Average SMSP Active Cycles       cycle      4340.41
    Total SMSP Elapsed Cycles        cycle      1532112
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       789.15
    Elapsed Cycles                cycle         7594
    Memory Throughput                 %        46.77
    DRAM Throughput                   %        46.77
    Duration                         us         9.60
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        26.43
    SM Active Cycles              cycle      5023.91
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.43
    Achieved Active Warps Per SM           warp        34.77
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      5023.91
    Total L1 Elapsed Cycles          cycle       419142
    Average L2 Active Cycles         cycle      4538.83
    Total L2 Elapsed Cycles          cycle       187800
    Average SM Active Cycles         cycle      5023.91
    Total SM Elapsed Cycles          cycle       419142
    Average SMSP Active Cycles       cycle      4944.72
    Total SMSP Elapsed Cycles        cycle      1676568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       788.31
    Elapsed Cycles                cycle         7456
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      5062.21
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.86
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5062.21
    Total L1 Elapsed Cycles          cycle       409638
    Average L2 Active Cycles         cycle      4478.88
    Total L2 Elapsed Cycles          cycle       184440
    Average SM Active Cycles         cycle      5062.21
    Total SM Elapsed Cycles          cycle       409638
    Average SMSP Active Cycles       cycle      4950.84
    Total SMSP Elapsed Cycles        cycle      1638552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       790.76
    Elapsed Cycles                cycle         7580
    Memory Throughput                 %        47.06
    DRAM Throughput                   %        47.06
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        26.45
    SM Active Cycles              cycle      5007.41
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.81
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       355328
    Average L1 Active Cycles         cycle      5007.41
    Total L1 Elapsed Cycles          cycle       413180
    Average L2 Active Cycles         cycle      4384.62
    Total L2 Elapsed Cycles          cycle       187440
    Average SM Active Cycles         cycle      5007.41
    Total SM Elapsed Cycles          cycle       413180
    Average SMSP Active Cycles       cycle      4813.38
    Total SMSP Elapsed Cycles        cycle      1652720
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.08
    Elapsed Cycles                cycle      5184772
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.60
    Duration                         ms         6.08
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160218.50
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301162.67
    Total DRAM Elapsed Cycles        cycle    227810304
    Average L1 Active Cycles         cycle   5160218.50
    Total L1 Elapsed Cycles          cycle    299447554
    Average L2 Active Cycles         cycle   4609865.08
    Total L2 Elapsed Cycles          cycle    125476248
    Average SM Active Cycles         cycle   5160218.50
    Total SM Elapsed Cycles          cycle    299447554
    Average SMSP Active Cycles       cycle   5160755.93
    Total SMSP Elapsed Cycles        cycle   1197790216
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.10
    Elapsed Cycles                cycle      1781494
    Memory Throughput                 %        89.84
    DRAM Throughput                   %        89.84
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.10
    L2 Cache Throughput               %        50.92
    SM Active Cycles              cycle   1759861.66
    Compute (SM) Throughput           %        23.60
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.37
    Achieved Active Warps Per SM           warp        46.74
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12333397.33
    Total DRAM Elapsed Cycles        cycle     82370560
    Average L1 Active Cycles         cycle   1759861.66
    Total L1 Elapsed Cycles          cycle    102619696
    Average L2 Active Cycles         cycle   1810739.92
    Total L2 Elapsed Cycles          cycle     43593360
    Average SM Active Cycles         cycle   1759861.66
    Total SM Elapsed Cycles          cycle    102619696
    Average SMSP Active Cycles       cycle   1767881.05
    Total SMSP Elapsed Cycles        cycle    410478784
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       843.82
    Elapsed Cycles                cycle      4531194
    Memory Throughput                 %        89.87
    DRAM Throughput                   %        25.69
    Duration                         ms         5.35
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4413918.78
    Compute (SM) Throughput           %        89.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8577762.67
    Total DRAM Elapsed Cycles        cycle    200352768
    Average L1 Active Cycles         cycle   4413918.78
    Total L1 Elapsed Cycles          cycle    261421144
    Average L2 Active Cycles         cycle   4526754.83
    Total L2 Elapsed Cycles          cycle    109673448
    Average SM Active Cycles         cycle   4413918.78
    Total SM Elapsed Cycles          cycle    261421144
    Average SMSP Active Cycles       cycle   4413225.86
    Total SMSP Elapsed Cycles        cycle   1045684576
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       806.09
    Elapsed Cycles                cycle         6975
    Memory Throughput                 %        45.20
    DRAM Throughput                   %        45.20
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.86
    L2 Cache Throughput               %        24.75
    SM Active Cycles              cycle      4544.91
    Compute (SM) Throughput           %        17.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.30
    Achieved Active Warps Per SM           warp        34.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23989.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4544.91
    Total L1 Elapsed Cycles          cycle       377812
    Average L2 Active Cycles         cycle      4070.71
    Total L2 Elapsed Cycles          cycle       169344
    Average SM Active Cycles         cycle      4544.91
    Total SM Elapsed Cycles          cycle       377812
    Average SMSP Active Cycles       cycle      4492.03
    Total SMSP Elapsed Cycles        cycle      1511248
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       795.49
    Elapsed Cycles                cycle         7608
    Memory Throughput                 %        47.61
    DRAM Throughput                   %        47.61
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.40
    L2 Cache Throughput               %        26.61
    SM Active Cycles              cycle      5061.50
    Compute (SM) Throughput           %        15.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.99
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5061.50
    Total L1 Elapsed Cycles          cycle       419692
    Average L2 Active Cycles         cycle      4497.46
    Total L2 Elapsed Cycles          cycle       186408
    Average SM Active Cycles         cycle      5061.50
    Total SM Elapsed Cycles          cycle       419692
    Average SMSP Active Cycles       cycle      5032.94
    Total SMSP Elapsed Cycles        cycle      1678768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.193%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.47% above the average, while the minimum instance value is 7.69% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       820.32
    Elapsed Cycles                cycle         7541
    Memory Throughput                 %        49.27
    DRAM Throughput                   %        49.27
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        27.22
    SM Active Cycles              cycle      5015.16
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.83
    Achieved Active Warps Per SM           warp        35.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5015.16
    Total L1 Elapsed Cycles          cycle       414234
    Average L2 Active Cycles         cycle      4502.83
    Total L2 Elapsed Cycles          cycle       182304
    Average SM Active Cycles         cycle      5015.16
    Total SM Elapsed Cycles          cycle       414234
    Average SMSP Active Cycles       cycle      5025.61
    Total SMSP Elapsed Cycles        cycle      1656936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       785.97
    Elapsed Cycles                cycle         7527
    Memory Throughput                 %        46.75
    DRAM Throughput                   %        46.75
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.19
    L2 Cache Throughput               %        26.48
    SM Active Cycles              cycle      4977.24
    Compute (SM) Throughput           %        15.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      4977.24
    Total L1 Elapsed Cycles          cycle       423724
    Average L2 Active Cycles         cycle      4522.50
    Total L2 Elapsed Cycles          cycle       187176
    Average SM Active Cycles         cycle      4977.24
    Total SM Elapsed Cycles          cycle       423724
    Average SMSP Active Cycles       cycle      4921.24
    Total SMSP Elapsed Cycles        cycle      1694896
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.32
    Elapsed Cycles                cycle      5183592
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.60
    Duration                         ms         6.08
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160538.19
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302250.67
    Total DRAM Elapsed Cycles        cycle    227748864
    Average L1 Active Cycles         cycle   5160538.19
    Total L1 Elapsed Cycles          cycle    299487932
    Average L2 Active Cycles         cycle   4609770.46
    Total L2 Elapsed Cycles          cycle    125476224
    Average SM Active Cycles         cycle   5160538.19
    Total SM Elapsed Cycles          cycle    299487932
    Average SMSP Active Cycles       cycle   5160075.79
    Total SMSP Elapsed Cycles        cycle   1197951728
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       796.88
    Elapsed Cycles                cycle      1745914
    Memory Throughput                 %        90.26
    DRAM Throughput                   %        90.26
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.20
    L2 Cache Throughput               %        51.33
    SM Active Cycles              cycle   1758304.62
    Compute (SM) Throughput           %        23.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.88
    Achieved Active Warps Per SM           warp        46.02
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12329538.67
    Total DRAM Elapsed Cycles        cycle     81960960
    Average L1 Active Cycles         cycle   1758304.62
    Total L1 Elapsed Cycles          cycle    102484934
    Average L2 Active Cycles         cycle   1801427.50
    Total L2 Elapsed Cycles          cycle     43315080
    Average SM Active Cycles         cycle   1758304.62
    Total SM Elapsed Cycles          cycle    102484934
    Average SMSP Active Cycles       cycle   1766240.77
    Total SMSP Elapsed Cycles        cycle    409939736
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       863.28
    Elapsed Cycles                cycle      4528023
    Memory Throughput                 %        89.54
    DRAM Throughput                   %        26.30
    Duration                         ms         5.23
    L1/TEX Cache Throughput           %        91.72
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4416009.81
    Compute (SM) Throughput           %        89.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8582120
    Total DRAM Elapsed Cycles        cycle    195802112
    Average L1 Active Cycles         cycle   4416009.81
    Total L1 Elapsed Cycles          cycle    262387714
    Average L2 Active Cycles         cycle   4521230.92
    Total L2 Elapsed Cycles          cycle    109678584
    Average SM Active Cycles         cycle   4416009.81
    Total SM Elapsed Cycles          cycle    262387714
    Average SMSP Active Cycles       cycle   4411802.95
    Total SMSP Elapsed Cycles        cycle   1049550856
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       789.05
    Elapsed Cycles                cycle         6962
    Memory Throughput                 %        43.95
    DRAM Throughput                   %        43.95
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        25.52
    L2 Cache Throughput               %        24.39
    SM Active Cycles              cycle      4427.59
    Compute (SM) Throughput           %        17.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.49
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23850.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4427.59
    Total L1 Elapsed Cycles          cycle       380972
    Average L2 Active Cycles         cycle      4028.17
    Total L2 Elapsed Cycles          cycle       171864
    Average SM Active Cycles         cycle      4427.59
    Total SM Elapsed Cycles          cycle       380972
    Average SMSP Active Cycles       cycle      4413.66
    Total SMSP Elapsed Cycles        cycle      1523888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.97
    Elapsed Cycles                cycle         7493
    Memory Throughput                 %        47.32
    DRAM Throughput                   %        47.32
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        26.71
    SM Active Cycles              cycle      5021.50
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.85
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27861.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5021.50
    Total L1 Elapsed Cycles          cycle       408088
    Average L2 Active Cycles         cycle      4387.29
    Total L2 Elapsed Cycles          cycle       185592
    Average SM Active Cycles         cycle      5021.50
    Total SM Elapsed Cycles          cycle       408088
    Average SMSP Active Cycles       cycle      4817.07
    Total SMSP Elapsed Cycles        cycle      1632352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.26%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.68% above the average, while the minimum instance value is 13.02% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.44
    Elapsed Cycles                cycle         7480
    Memory Throughput                 %        47.33
    DRAM Throughput                   %        47.33
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.20
    L2 Cache Throughput               %        26.81
    SM Active Cycles              cycle      4975.64
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      4975.64
    Total L1 Elapsed Cycles          cycle       405844
    Average L2 Active Cycles         cycle      4477.71
    Total L2 Elapsed Cycles          cycle       185208
    Average SM Active Cycles         cycle      4975.64
    Total SM Elapsed Cycles          cycle       405844
    Average SMSP Active Cycles       cycle      4886.91
    Total SMSP Elapsed Cycles        cycle      1623376
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       787.90
    Elapsed Cycles                cycle         7555
    Memory Throughput                 %        46.94
    DRAM Throughput                   %        46.94
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        26.55
    SM Active Cycles              cycle      4959.48
    Compute (SM) Throughput           %        15.59
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.34
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27637.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4959.48
    Total L1 Elapsed Cycles          cycle       420390
    Average L2 Active Cycles         cycle      4478.17
    Total L2 Elapsed Cycles          cycle       186768
    Average SM Active Cycles         cycle      4959.48
    Total SM Elapsed Cycles          cycle       420390
    Average SMSP Active Cycles       cycle      4909.62
    Total SMSP Elapsed Cycles        cycle      1681560
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.28
    Elapsed Cycles                cycle      5185518
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.25
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.67
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160802.90
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6300525.33
    Total DRAM Elapsed Cycles        cycle    232697856
    Average L1 Active Cycles         cycle   5160802.90
    Total L1 Elapsed Cycles          cycle    299459974
    Average L2 Active Cycles         cycle      4609403
    Total L2 Elapsed Cycles          cycle    125508624
    Average SM Active Cycles         cycle   5160802.90
    Total SM Elapsed Cycles          cycle    299459974
    Average SMSP Active Cycles       cycle   5160485.71
    Total SMSP Elapsed Cycles        cycle   1197839896
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       848.54
    Elapsed Cycles                cycle      1873058
    Memory Throughput                 %        89.79
    DRAM Throughput                   %        89.79
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        40.88
    L2 Cache Throughput               %        48.96
    SM Active Cycles              cycle   1856725.02
    Compute (SM) Throughput           %        22.42
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.14
    Achieved Active Warps Per SM           warp        46.63
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12327810.67
    Total DRAM Elapsed Cycles        cycle     82375680
    Average L1 Active Cycles         cycle   1856725.02
    Total L1 Elapsed Cycles          cycle    108021024
    Average L2 Active Cycles         cycle   1885372.08
    Total L2 Elapsed Cycles          cycle     45381240
    Average SM Active Cycles         cycle   1856725.02
    Total SM Elapsed Cycles          cycle    108021024
    Average SMSP Active Cycles       cycle   1859806.27
    Total SMSP Elapsed Cycles        cycle    432084096
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.13
    Elapsed Cycles                cycle      4531534
    Memory Throughput                 %        89.69
    DRAM Throughput                   %        24.58
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        91.80
    L2 Cache Throughput               %        15.10
    SM Active Cycles              cycle   4412458.95
    Compute (SM) Throughput           %        89.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.11
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577736
    Total DRAM Elapsed Cycles        cycle    209381376
    Average L1 Active Cycles         cycle   4412458.95
    Total L1 Elapsed Cycles          cycle    261925828
    Average L2 Active Cycles         cycle   4524705.42
    Total L2 Elapsed Cycles          cycle    110646456
    Average SM Active Cycles         cycle   4412458.95
    Total SM Elapsed Cycles          cycle    261925828
    Average SMSP Active Cycles       cycle   4416558.79
    Total SMSP Elapsed Cycles        cycle   1047703312
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       812.52
    Elapsed Cycles                cycle         6975
    Memory Throughput                 %        45.60
    DRAM Throughput                   %        45.60
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.03
    L2 Cache Throughput               %        24.84
    SM Active Cycles              cycle      4514.05
    Compute (SM) Throughput           %        16.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.25
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24048
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4514.05
    Total L1 Elapsed Cycles          cycle       386108
    Average L2 Active Cycles         cycle      4037.71
    Total L2 Elapsed Cycles          cycle       168552
    Average SM Active Cycles         cycle      4514.05
    Total SM Elapsed Cycles          cycle       386108
    Average SMSP Active Cycles       cycle      4425.90
    Total SMSP Elapsed Cycles        cycle      1544432
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       807.06
    Elapsed Cycles                cycle         7628
    Memory Throughput                 %        47.37
    DRAM Throughput                   %        47.37
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.39
    L2 Cache Throughput               %        26.75
    SM Active Cycles              cycle      5047.48
    Compute (SM) Throughput           %        15.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.42
    Achieved Active Warps Per SM           warp        34.76
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27650.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5047.48
    Total L1 Elapsed Cycles          cycle       420116
    Average L2 Active Cycles         cycle      4522.67
    Total L2 Elapsed Cycles          cycle       185544
    Average SM Active Cycles         cycle      5047.48
    Total SM Elapsed Cycles          cycle       420116
    Average SMSP Active Cycles       cycle      4991.82
    Total SMSP Elapsed Cycles        cycle      1680464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       799.26
    Elapsed Cycles                cycle         7553
    Memory Throughput                 %        47.60
    DRAM Throughput                   %        47.60
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        26.90
    SM Active Cycles              cycle      5005.50
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5005.50
    Total L1 Elapsed Cycles          cycle       416180
    Average L2 Active Cycles         cycle      4420.75
    Total L2 Elapsed Cycles          cycle       184512
    Average SM Active Cycles         cycle      5005.50
    Total SM Elapsed Cycles          cycle       416180
    Average SMSP Active Cycles       cycle      4870.58
    Total SMSP Elapsed Cycles        cycle      1664720
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.37
    Elapsed Cycles                cycle         7591
    Memory Throughput                 %        46.94
    DRAM Throughput                   %        46.94
    Duration                         us         9.66
    L1/TEX Cache Throughput           %        23.20
    L2 Cache Throughput               %        26.23
    SM Active Cycles              cycle      4984.81
    Compute (SM) Throughput           %        15.48
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27877.33
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      4984.81
    Total L1 Elapsed Cycles          cycle       423484
    Average L2 Active Cycles         cycle      4474.62
    Total L2 Elapsed Cycles          cycle       189000
    Average SM Active Cycles         cycle      4984.81
    Total SM Elapsed Cycles          cycle       423484
    Average SMSP Active Cycles       cycle      4869.97
    Total SMSP Elapsed Cycles        cycle      1693936
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       861.58
    Elapsed Cycles                cycle      5183062
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.84
    Duration                         ms         5.99
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.41
    SM Active Cycles              cycle   5160169.71
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302253.33
    Total DRAM Elapsed Cycles        cycle    224507904
    Average L1 Active Cycles         cycle   5160169.71
    Total L1 Elapsed Cycles          cycle    299473722
    Average L2 Active Cycles         cycle   4609094.92
    Total L2 Elapsed Cycles          cycle    125425176
    Average SM Active Cycles         cycle   5160169.71
    Total SM Elapsed Cycles          cycle    299473722
    Average SMSP Active Cycles       cycle   5160457.91
    Total SMSP Elapsed Cycles        cycle   1197894888
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.34
    Elapsed Cycles                cycle      1778263
    Memory Throughput                 %        89.88
    DRAM Throughput                   %        89.88
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        43.33
    L2 Cache Throughput               %        51.02
    SM Active Cycles              cycle   1769597.36
    Compute (SM) Throughput           %        23.74
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.83
    Achieved Active Warps Per SM           warp        46.48
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12317208
    Total DRAM Elapsed Cycles        cycle     82225152
    Average L1 Active Cycles         cycle   1769597.36
    Total L1 Elapsed Cycles          cycle    102022146
    Average L2 Active Cycles         cycle   1802893.04
    Total L2 Elapsed Cycles          cycle     43533120
    Average SM Active Cycles         cycle   1769597.36
    Total SM Elapsed Cycles          cycle    102022146
    Average SMSP Active Cycles       cycle   1761272.24
    Total SMSP Elapsed Cycles        cycle    408088584
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.20
    Elapsed Cycles                cycle      4542753
    Memory Throughput                 %        89.66
    DRAM Throughput                   %        24.53
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.06
    SM Active Cycles              cycle   4414167.84
    Compute (SM) Throughput           %        89.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8580946.67
    Total DRAM Elapsed Cycles        cycle    209904640
    Average L1 Active Cycles         cycle   4414167.84
    Total L1 Elapsed Cycles          cycle    262012766
    Average L2 Active Cycles         cycle      4564544
    Total L2 Elapsed Cycles          cycle    110923008
    Average SM Active Cycles         cycle   4414167.84
    Total SM Elapsed Cycles          cycle    262012766
    Average SMSP Active Cycles       cycle   4413798.09
    Total SMSP Elapsed Cycles        cycle   1048051064
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       800.25
    Elapsed Cycles                cycle         6993
    Memory Throughput                 %        44.15
    DRAM Throughput                   %        44.15
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        25.17
    L2 Cache Throughput               %        24.55
    SM Active Cycles              cycle      4489.76
    Compute (SM) Throughput           %        16.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.03
    Achieved Active Warps Per SM           warp        35.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23736
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4489.76
    Total L1 Elapsed Cycles          cycle       387276
    Average L2 Active Cycles         cycle      3982.17
    Total L2 Elapsed Cycles          cycle       170784
    Average SM Active Cycles         cycle      4489.76
    Total SM Elapsed Cycles          cycle       387276
    Average SMSP Active Cycles       cycle      4352.75
    Total SMSP Elapsed Cycles        cycle      1549104
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.18
    Elapsed Cycles                cycle         7507
    Memory Throughput                 %        47.33
    DRAM Throughput                   %        47.33
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        24.35
    L2 Cache Throughput               %        26.56
    SM Active Cycles              cycle      4944.81
    Compute (SM) Throughput           %        16.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.77
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4944.81
    Total L1 Elapsed Cycles          cycle       403442
    Average L2 Active Cycles         cycle      4530.79
    Total L2 Elapsed Cycles          cycle       186792
    Average SM Active Cycles         cycle      4944.81
    Total SM Elapsed Cycles          cycle       403442
    Average SMSP Active Cycles       cycle      4940.54
    Total SMSP Elapsed Cycles        cycle      1613768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       789.09
    Elapsed Cycles                cycle         7469
    Memory Throughput                 %        47.63
    DRAM Throughput                   %        47.63
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        26.92
    SM Active Cycles              cycle      5049.26
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.74
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.26%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5049.26
    Total L1 Elapsed Cycles          cycle       411880
    Average L2 Active Cycles         cycle      4503.83
    Total L2 Elapsed Cycles          cycle       184272
    Average SM Active Cycles         cycle      5049.26
    Total SM Elapsed Cycles          cycle       411880
    Average SMSP Active Cycles       cycle      4958.77
    Total SMSP Elapsed Cycles        cycle      1647520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       801.93
    Elapsed Cycles                cycle         7585
    Memory Throughput                 %        48.06
    DRAM Throughput                   %        48.06
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        26.86
    SM Active Cycles              cycle      5042.12
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5042.12
    Total L1 Elapsed Cycles          cycle       411430
    Average L2 Active Cycles         cycle      4460.75
    Total L2 Elapsed Cycles          cycle       184752
    Average SM Active Cycles         cycle      5042.12
    Total SM Elapsed Cycles          cycle       411430
    Average SMSP Active Cycles       cycle      4933.14
    Total SMSP Elapsed Cycles        cycle      1645720
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.49
    Elapsed Cycles                cycle      5184713
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.24
    Duration                         ms         6.22
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160236.64
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302541.33
    Total DRAM Elapsed Cycles        cycle    232891392
    Average L1 Active Cycles         cycle   5160236.64
    Total L1 Elapsed Cycles          cycle    299453934
    Average L2 Active Cycles         cycle   4611064.58
    Total L2 Elapsed Cycles          cycle    125533704
    Average SM Active Cycles         cycle   5160236.64
    Total SM Elapsed Cycles          cycle    299453934
    Average SMSP Active Cycles       cycle   5160648.93
    Total SMSP Elapsed Cycles        cycle   1197815736
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.24
    Elapsed Cycles                cycle      1864275
    Memory Throughput                 %        90.11
    DRAM Throughput                   %        90.11
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.40
    L2 Cache Throughput               %        49.27
    SM Active Cycles              cycle   1844743.21
    Compute (SM) Throughput           %        23.79
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.10
    Achieved Active Warps Per SM           warp        46.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12357453.33
    Total DRAM Elapsed Cycles        cycle     82286592
    Average L1 Active Cycles         cycle   1844743.21
    Total L1 Elapsed Cycles          cycle    101790812
    Average L2 Active Cycles         cycle   1873429.58
    Total L2 Elapsed Cycles          cycle     45155088
    Average SM Active Cycles         cycle   1844743.21
    Total SM Elapsed Cycles          cycle    101790812
    Average SMSP Active Cycles       cycle   1849287.19
    Total SMSP Elapsed Cycles        cycle    407163248
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.12
    Elapsed Cycles                cycle      4517139
    Memory Throughput                 %        89.95
    DRAM Throughput                   %        24.19
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4413121.67
    Compute (SM) Throughput           %        89.95
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577312
    Total DRAM Elapsed Cycles        cycle    212785152
    Average L1 Active Cycles         cycle   4413121.67
    Total L1 Elapsed Cycles          cycle    261179114
    Average L2 Active Cycles         cycle   4535844.25
    Total L2 Elapsed Cycles          cycle    112445112
    Average SM Active Cycles         cycle   4413121.67
    Total SM Elapsed Cycles          cycle    261179114
    Average SMSP Active Cycles       cycle   4416544.53
    Total SMSP Elapsed Cycles        cycle   1044716456
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       788.22
    Elapsed Cycles                cycle         7028
    Memory Throughput                 %        43.92
    DRAM Throughput                   %        43.92
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        25.00
    L2 Cache Throughput               %        24.13
    SM Active Cycles              cycle      4519.53
    Compute (SM) Throughput           %        17.38
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.01
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24061.33
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4519.53
    Total L1 Elapsed Cycles          cycle       377060
    Average L2 Active Cycles         cycle      4101.62
    Total L2 Elapsed Cycles          cycle       173712
    Average SM Active Cycles         cycle      4519.53
    Total SM Elapsed Cycles          cycle       377060
    Average SMSP Active Cycles       cycle      4442.22
    Total SMSP Elapsed Cycles        cycle      1508240
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       788.87
    Elapsed Cycles                cycle         7643
    Memory Throughput                 %        46.58
    DRAM Throughput                   %        46.58
    Duration                         us         9.66
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        26.28
    SM Active Cycles              cycle      4955.22
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.06
    Achieved Active Warps Per SM           warp        35.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       358400
    Average L1 Active Cycles         cycle      4955.22
    Total L1 Elapsed Cycles          cycle       411552
    Average L2 Active Cycles         cycle      4497.17
    Total L2 Elapsed Cycles          cycle       188808
    Average SM Active Cycles         cycle      4955.22
    Total SM Elapsed Cycles          cycle       411552
    Average SMSP Active Cycles       cycle      4913.71
    Total SMSP Elapsed Cycles        cycle      1646208
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.00
    Elapsed Cycles                cycle         7463
    Memory Throughput                 %        47.74
    DRAM Throughput                   %        47.74
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        26.88
    SM Active Cycles              cycle      5092.53
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.50
    Achieved Active Warps Per SM           warp        34.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      5092.53
    Total L1 Elapsed Cycles          cycle       413866
    Average L2 Active Cycles         cycle      4511.08
    Total L2 Elapsed Cycles          cycle       184536
    Average SM Active Cycles         cycle      5092.53
    Total SM Elapsed Cycles          cycle       413866
    Average SMSP Active Cycles       cycle      4932.25
    Total SMSP Elapsed Cycles        cycle      1655464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       814.58
    Elapsed Cycles                cycle         7643
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        26.83
    SM Active Cycles              cycle      5003.76
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.25
    Achieved Active Warps Per SM           warp        36.12
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5003.76
    Total L1 Elapsed Cycles          cycle       413300
    Average L2 Active Cycles         cycle      4502.04
    Total L2 Elapsed Cycles          cycle       185040
    Average SM Active Cycles         cycle      5003.76
    Total SM Elapsed Cycles          cycle       413300
    Average SMSP Active Cycles       cycle      4962.27
    Total SMSP Elapsed Cycles        cycle      1653200
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       832.80
    Elapsed Cycles                cycle      5184100
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.28
    Duration                         ms         6.20
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160435.76
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302618.67
    Total DRAM Elapsed Cycles        cycle    232261632
    Average L1 Active Cycles         cycle   5160435.76
    Total L1 Elapsed Cycles          cycle    299472292
    Average L2 Active Cycles         cycle   4611335.62
    Total L2 Elapsed Cycles          cycle    125493816
    Average SM Active Cycles         cycle   5160435.76
    Total SM Elapsed Cycles          cycle    299472292
    Average SMSP Active Cycles       cycle   5160107.93
    Total SMSP Elapsed Cycles        cycle   1197889168
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       848.83
    Elapsed Cycles                cycle      1869163
    Memory Throughput                 %        90.05
    DRAM Throughput                   %        90.05
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.39
    L2 Cache Throughput               %        49.04
    SM Active Cycles              cycle   1847691.55
    Compute (SM) Throughput           %        22.68
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.44
    Achieved Active Warps Per SM           warp        46.77
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12328122.67
    Total DRAM Elapsed Cycles        cycle     82145280
    Average L1 Active Cycles         cycle   1847691.55
    Total L1 Elapsed Cycles          cycle    106752398
    Average L2 Active Cycles         cycle   1874356.96
    Total L2 Elapsed Cycles          cycle     45252600
    Average SM Active Cycles         cycle   1847691.55
    Total SM Elapsed Cycles          cycle    106752398
    Average SMSP Active Cycles       cycle   1851013.34
    Total SMSP Elapsed Cycles        cycle    427009592
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       843.65
    Elapsed Cycles                cycle      4537932
    Memory Throughput                 %        89.55
    DRAM Throughput                   %        25.65
    Duration                         ms         5.36
    L1/TEX Cache Throughput           %        91.78
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4413198.24
    Compute (SM) Throughput           %        89.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.10
    Achieved Active Warps Per SM           warp        42.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8580834.67
    Total DRAM Elapsed Cycles        cycle    200731648
    Average L1 Active Cycles         cycle   4413198.24
    Total L1 Elapsed Cycles          cycle    262337648
    Average L2 Active Cycles         cycle   4526995.17
    Total L2 Elapsed Cycles          cycle    109853304
    Average SM Active Cycles         cycle   4413198.24
    Total SM Elapsed Cycles          cycle    262337648
    Average SMSP Active Cycles       cycle   4415379.62
    Total SMSP Elapsed Cycles        cycle   1049350592
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.91
    Elapsed Cycles                cycle         6996
    Memory Throughput                 %        44.27
    DRAM Throughput                   %        44.27
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        25.18
    L2 Cache Throughput               %        24.61
    SM Active Cycles              cycle      4486.93
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.99
    Achieved Active Warps Per SM           warp        34.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23797.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4486.93
    Total L1 Elapsed Cycles          cycle       384542
    Average L2 Active Cycles         cycle      4007.50
    Total L2 Elapsed Cycles          cycle       170592
    Average SM Active Cycles         cycle      4486.93
    Total SM Elapsed Cycles          cycle       384542
    Average SMSP Active Cycles       cycle      4407.60
    Total SMSP Elapsed Cycles        cycle      1538168
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       796.80
    Elapsed Cycles                cycle         7552
    Memory Throughput                 %        47.49
    DRAM Throughput                   %        47.49
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      5016.28
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.47
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5016.28
    Total L1 Elapsed Cycles          cycle       412580
    Average L2 Active Cycles         cycle      4458.92
    Total L2 Elapsed Cycles          cycle       184776
    Average SM Active Cycles         cycle      5016.28
    Total SM Elapsed Cycles          cycle       412580
    Average SMSP Active Cycles       cycle      4965.69
    Total SMSP Elapsed Cycles        cycle      1650320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       806.64
    Elapsed Cycles                cycle         7523
    Memory Throughput                 %        48.43
    DRAM Throughput                   %        48.43
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.13
    SM Active Cycles              cycle      5170.29
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.93
    Achieved Active Warps Per SM           warp        34.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5170.29
    Total L1 Elapsed Cycles          cycle       411698
    Average L2 Active Cycles         cycle      4721.88
    Total L2 Elapsed Cycles          cycle       182928
    Average SM Active Cycles         cycle      5170.29
    Total SM Elapsed Cycles          cycle       411698
    Average SMSP Active Cycles       cycle      5278.28
    Total SMSP Elapsed Cycles        cycle      1646792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.24
    Elapsed Cycles                cycle         7543
    Memory Throughput                 %        48.11
    DRAM Throughput                   %        48.11
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        26.91
    SM Active Cycles              cycle      5067.19
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5067.19
    Total L1 Elapsed Cycles          cycle       411952
    Average L2 Active Cycles         cycle      4472.88
    Total L2 Elapsed Cycles          cycle       184296
    Average SM Active Cycles         cycle      5067.19
    Total SM Elapsed Cycles          cycle       411952
    Average SMSP Active Cycles       cycle      5043.58
    Total SMSP Elapsed Cycles        cycle      1647808
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       829.84
    Elapsed Cycles                cycle      5187253
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.22
    Duration                         ms         6.22
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160553.67
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301645.33
    Total DRAM Elapsed Cycles        cycle    233112576
    Average L1 Active Cycles         cycle   5160553.67
    Total L1 Elapsed Cycles          cycle    299484642
    Average L2 Active Cycles         cycle   4610775.21
    Total L2 Elapsed Cycles          cycle    125502144
    Average SM Active Cycles         cycle   5160553.67
    Total SM Elapsed Cycles          cycle    299484642
    Average SMSP Active Cycles       cycle   5160073.64
    Total SMSP Elapsed Cycles        cycle   1197938568
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       848.83
    Elapsed Cycles                cycle      1859810
    Memory Throughput                 %        90.78
    DRAM Throughput                   %        90.78
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        43.59
    L2 Cache Throughput               %        49.39
    SM Active Cycles              cycle   1738651.16
    Compute (SM) Throughput           %        23.88
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       102.69
    Achieved Active Warps Per SM           warp        49.29
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12365069.33
    Total DRAM Elapsed Cycles        cycle     81727488
    Average L1 Active Cycles         cycle   1738651.16
    Total L1 Elapsed Cycles          cycle    101390746
    Average L2 Active Cycles         cycle      1805297
    Total L2 Elapsed Cycles          cycle     45032064
    Average SM Active Cycles         cycle   1738651.16
    Total SM Elapsed Cycles          cycle    101390746
    Average SMSP Active Cycles       cycle   1743289.14
    Total SMSP Elapsed Cycles        cycle    405562984
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       843.21
    Elapsed Cycles                cycle      4532240
    Memory Throughput                 %        89.81
    DRAM Throughput                   %        25.66
    Duration                         ms         5.35
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4414103.14
    Compute (SM) Throughput           %        89.81
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.07
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8574298.67
    Total DRAM Elapsed Cycles        cycle    200518656
    Average L1 Active Cycles         cycle   4414103.14
    Total L1 Elapsed Cycles          cycle    261596444
    Average L2 Active Cycles         cycle   4529813.29
    Total L2 Elapsed Cycles          cycle    109734384
    Average SM Active Cycles         cycle   4414103.14
    Total SM Elapsed Cycles          cycle    261596444
    Average SMSP Active Cycles       cycle   4413098.57
    Total SMSP Elapsed Cycles        cycle   1046385776
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.19
    Elapsed Cycles                cycle         6861
    Memory Throughput                 %        44.52
    DRAM Throughput                   %        44.52
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        25.15
    L2 Cache Throughput               %        24.49
    SM Active Cycles              cycle      4491.93
    Compute (SM) Throughput           %        17.26
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.06
    Achieved Active Warps Per SM           warp        34.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24088
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4491.93
    Total L1 Elapsed Cycles          cycle       379688
    Average L2 Active Cycles         cycle      4113.12
    Total L2 Elapsed Cycles          cycle       171024
    Average SM Active Cycles         cycle      4491.93
    Total SM Elapsed Cycles          cycle       379688
    Average SMSP Active Cycles       cycle      4463.79
    Total SMSP Elapsed Cycles        cycle      1518752
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       803.60
    Elapsed Cycles                cycle         7543
    Memory Throughput                 %        48.34
    DRAM Throughput                   %        48.34
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        27.00
    SM Active Cycles              cycle      5060.52
    Compute (SM) Throughput           %        15.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.30
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5060.52
    Total L1 Elapsed Cycles          cycle       419976
    Average L2 Active Cycles         cycle      4452.54
    Total L2 Elapsed Cycles          cycle       183768
    Average SM Active Cycles         cycle      5060.52
    Total SM Elapsed Cycles          cycle       419976
    Average SMSP Active Cycles       cycle      4890.63
    Total SMSP Elapsed Cycles        cycle      1679904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       789.64
    Elapsed Cycles                cycle         7471
    Memory Throughput                 %        47.67
    DRAM Throughput                   %        47.67
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        26.86
    SM Active Cycles              cycle      4930.55
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.27
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4930.55
    Total L1 Elapsed Cycles          cycle       410782
    Average L2 Active Cycles         cycle      4490.58
    Total L2 Elapsed Cycles          cycle       184752
    Average SM Active Cycles         cycle      4930.55
    Total SM Elapsed Cycles          cycle       410782
    Average SMSP Active Cycles       cycle      4896.77
    Total SMSP Elapsed Cycles        cycle      1643128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.42
    Elapsed Cycles                cycle         7481
    Memory Throughput                 %        47.53
    DRAM Throughput                   %        47.53
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        26.80
    SM Active Cycles              cycle      4886.40
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.18
    Achieved Active Warps Per SM           warp        36.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       351232
    Average L1 Active Cycles         cycle      4886.40
    Total L1 Elapsed Cycles          cycle       407568
    Average L2 Active Cycles         cycle         4429
    Total L2 Elapsed Cycles          cycle       185064
    Average SM Active Cycles         cycle      4886.40
    Total SM Elapsed Cycles          cycle       407568
    Average SMSP Active Cycles       cycle      4869.44
    Total SMSP Elapsed Cycles        cycle      1630272
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       824.70
    Elapsed Cycles                cycle      5180986
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.13
    Duration                         ms         6.26
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.30
    SM Active Cycles              cycle   5160074.40
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6307397.33
    Total DRAM Elapsed Cycles        cycle    234553344
    Average L1 Active Cycles         cycle   5160074.40
    Total L1 Elapsed Cycles          cycle    299445346
    Average L2 Active Cycles         cycle      4697841
    Total L2 Elapsed Cycles          cycle    126325512
    Average SM Active Cycles         cycle   5160074.40
    Total SM Elapsed Cycles          cycle    299445346
    Average SMSP Active Cycles       cycle   5160064.38
    Total SMSP Elapsed Cycles        cycle   1197781384
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       850.12
    Elapsed Cycles                cycle      1867158
    Memory Throughput                 %        90.36
    DRAM Throughput                   %        90.36
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.00
    L2 Cache Throughput               %        49.10
    SM Active Cycles              cycle   1836719.66
    Compute (SM) Throughput           %        22.48
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.66
    Achieved Active Warps Per SM           warp        46.88
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12341458.67
    Total DRAM Elapsed Cycles        cycle     81945600
    Average L1 Active Cycles         cycle   1836719.66
    Total L1 Elapsed Cycles          cycle    107726270
    Average L2 Active Cycles         cycle   1872302.83
    Total L2 Elapsed Cycles          cycle     45201600
    Average SM Active Cycles         cycle   1836719.66
    Total SM Elapsed Cycles          cycle    107726270
    Average SMSP Active Cycles       cycle   1845771.56
    Total SMSP Elapsed Cycles        cycle    430905080
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.19
    Elapsed Cycles                cycle      4527054
    Memory Throughput                 %        89.63
    DRAM Throughput                   %        25.74
    Duration                         ms         5.34
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.24
    SM Active Cycles              cycle   4414468.98
    Compute (SM) Throughput           %        89.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8583813.33
    Total DRAM Elapsed Cycles        cycle    200085504
    Average L1 Active Cycles         cycle   4414468.98
    Total L1 Elapsed Cycles          cycle    262114706
    Average L2 Active Cycles         cycle   4530339.62
    Total L2 Elapsed Cycles          cycle    109575432
    Average SM Active Cycles         cycle   4414468.98
    Total SM Elapsed Cycles          cycle    262114706
    Average SMSP Active Cycles       cycle   4415958.09
    Total SMSP Elapsed Cycles        cycle   1048458824
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.95
    Elapsed Cycles                cycle         7014
    Memory Throughput                 %        44.26
    DRAM Throughput                   %        44.26
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        25.52
    L2 Cache Throughput               %        24.53
    SM Active Cycles              cycle      4427.12
    Compute (SM) Throughput           %        17.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.27
    Achieved Active Warps Per SM           warp        35.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4427.12
    Total L1 Elapsed Cycles          cycle       385460
    Average L2 Active Cycles         cycle      3972.29
    Total L2 Elapsed Cycles          cycle       171000
    Average SM Active Cycles         cycle      4427.12
    Total SM Elapsed Cycles          cycle       385460
    Average SMSP Active Cycles       cycle      4381.45
    Total SMSP Elapsed Cycles        cycle      1541840
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       829.38
    Elapsed Cycles                cycle         7757
    Memory Throughput                 %        48.32
    DRAM Throughput                   %        48.32
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        26.46
    SM Active Cycles              cycle      5033.86
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.50
    Achieved Active Warps Per SM           warp        35.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27709.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5033.86
    Total L1 Elapsed Cycles          cycle       413198
    Average L2 Active Cycles         cycle         4469
    Total L2 Elapsed Cycles          cycle       187536
    Average SM Active Cycles         cycle      5033.86
    Total SM Elapsed Cycles          cycle       413198
    Average SMSP Active Cycles       cycle      4922.56
    Total SMSP Elapsed Cycles        cycle      1652792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       800.04
    Elapsed Cycles                cycle         7558
    Memory Throughput                 %        47.81
    DRAM Throughput                   %        47.81
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        26.89
    SM Active Cycles              cycle      4988.14
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.21
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       349184
    Average L1 Active Cycles         cycle      4988.14
    Total L1 Elapsed Cycles          cycle       407526
    Average L2 Active Cycles         cycle      4458.75
    Total L2 Elapsed Cycles          cycle       184488
    Average SM Active Cycles         cycle      4988.14
    Total SM Elapsed Cycles          cycle       407526
    Average SMSP Active Cycles       cycle      4924.13
    Total SMSP Elapsed Cycles        cycle      1630104
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       805.59
    Elapsed Cycles                cycle         7564
    Memory Throughput                 %        48.33
    DRAM Throughput                   %        48.33
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        24.14
    L2 Cache Throughput               %        26.94
    SM Active Cycles              cycle      4919.71
    Compute (SM) Throughput           %        16.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.78
    Achieved Active Warps Per SM           warp        36.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27877.33
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      4919.71
    Total L1 Elapsed Cycles          cycle       406900
    Average L2 Active Cycles         cycle      4429.17
    Total L2 Elapsed Cycles          cycle       184104
    Average SM Active Cycles         cycle      4919.71
    Total SM Elapsed Cycles          cycle       406900
    Average SMSP Active Cycles       cycle      4924.37
    Total SMSP Elapsed Cycles        cycle      1627600
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       839.64
    Elapsed Cycles                cycle      5185511
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.42
    Duration                         ms         6.15
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160542.45
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6303336
    Total DRAM Elapsed Cycles        cycle    230367232
    Average L1 Active Cycles         cycle   5160542.45
    Total L1 Elapsed Cycles          cycle    299467002
    Average L2 Active Cycles         cycle   4640317.71
    Total L2 Elapsed Cycles          cycle    125458080
    Average SM Active Cycles         cycle   5160542.45
    Total SM Elapsed Cycles          cycle    299467002
    Average SMSP Active Cycles       cycle   5160389.05
    Total SMSP Elapsed Cycles        cycle   1197868008
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       850.42
    Elapsed Cycles                cycle      1865400
    Memory Throughput                 %        90.75
    DRAM Throughput                   %        90.75
    Duration                         ms         2.18
    L1/TEX Cache Throughput           %        40.66
    L2 Cache Throughput               %        49.15
    SM Active Cycles              cycle   1869500.16
    Compute (SM) Throughput           %        22.30
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        95.82
    Achieved Active Warps Per SM           warp        45.99
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12375957.33
    Total DRAM Elapsed Cycles        cycle     81827840
    Average L1 Active Cycles         cycle   1869500.16
    Total L1 Elapsed Cycles          cycle    108613828
    Average L2 Active Cycles         cycle   1883027.71
    Total L2 Elapsed Cycles          cycle     45149088
    Average SM Active Cycles         cycle   1869500.16
    Total SM Elapsed Cycles          cycle    108613828
    Average SMSP Active Cycles       cycle   1858885.45
    Total SMSP Elapsed Cycles        cycle    434455312
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.22
    Elapsed Cycles                cycle      4527657
    Memory Throughput                 %        89.66
    DRAM Throughput                   %        24.13
    Duration                         ms         5.69
    L1/TEX Cache Throughput           %        91.65
    L2 Cache Throughput               %        14.83
    SM Active Cycles              cycle   4419359.40
    Compute (SM) Throughput           %        89.66
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.97
    Achieved Active Warps Per SM           warp        42.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8574997.33
    Total DRAM Elapsed Cycles        cycle    213180416
    Average L1 Active Cycles         cycle   4419359.40
    Total L1 Elapsed Cycles          cycle    262019476
    Average L2 Active Cycles         cycle   4640187.67
    Total L2 Elapsed Cycles          cycle    112654176
    Average SM Active Cycles         cycle   4419359.40
    Total SM Elapsed Cycles          cycle    262019476
    Average SMSP Active Cycles       cycle   4416818.14
    Total SMSP Elapsed Cycles        cycle   1048077904
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       803.49
    Elapsed Cycles                cycle         7132
    Memory Throughput                 %        44.04
    DRAM Throughput                   %        44.04
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.77
    L2 Cache Throughput               %        24.16
    SM Active Cycles              cycle      4561.43
    Compute (SM) Throughput           %        16.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.39
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24053.33
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4561.43
    Total L1 Elapsed Cycles          cycle       387346
    Average L2 Active Cycles         cycle      4066.21
    Total L2 Elapsed Cycles          cycle       173544
    Average SM Active Cycles         cycle      4561.43
    Total SM Elapsed Cycles          cycle       387346
    Average SMSP Active Cycles       cycle      4459.44
    Total SMSP Elapsed Cycles        cycle      1549384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.45
    Elapsed Cycles                cycle         7682
    Memory Throughput                 %        46.21
    DRAM Throughput                   %        46.21
    Duration                         us         9.76
    L1/TEX Cache Throughput           %        23.03
    L2 Cache Throughput               %        25.95
    SM Active Cycles              cycle      5047.78
    Compute (SM) Throughput           %        15.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.92
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.08%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27920
    Total DRAM Elapsed Cycles        cycle       362496
    Average L1 Active Cycles         cycle      5047.78
    Total L1 Elapsed Cycles          cycle       426530
    Average L2 Active Cycles         cycle      4496.67
    Total L2 Elapsed Cycles          cycle       190992
    Average SM Active Cycles         cycle      5047.78
    Total SM Elapsed Cycles          cycle       426530
    Average SMSP Active Cycles       cycle      4911.06
    Total SMSP Elapsed Cycles        cycle      1706120
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       797.08
    Elapsed Cycles                cycle         7399
    Memory Throughput                 %        48.79
    DRAM Throughput                   %        48.79
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.36
    SM Active Cycles              cycle      5075.36
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.84
    Achieved Active Warps Per SM           warp        34.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5075.36
    Total L1 Elapsed Cycles          cycle       413014
    Average L2 Active Cycles         cycle      4447.25
    Total L2 Elapsed Cycles          cycle       181152
    Average SM Active Cycles         cycle      5075.36
    Total SM Elapsed Cycles          cycle       413014
    Average SMSP Active Cycles       cycle      4998.69
    Total SMSP Elapsed Cycles        cycle      1652056
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       817.63
    Elapsed Cycles                cycle         7647
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      4989.90
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.67
    Achieved Active Warps Per SM           warp        36.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4989.90
    Total L1 Elapsed Cycles          cycle       416678
    Average L2 Active Cycles         cycle      4459.04
    Total L2 Elapsed Cycles          cycle       184848
    Average SM Active Cycles         cycle      4989.90
    Total SM Elapsed Cycles          cycle       416678
    Average SMSP Active Cycles       cycle      4968.05
    Total SMSP Elapsed Cycles        cycle      1666712
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.79
    Elapsed Cycles                cycle      5185725
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.26
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160484.57
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302090.67
    Total DRAM Elapsed Cycles        cycle    232538112
    Average L1 Active Cycles         cycle   5160484.57
    Total L1 Elapsed Cycles          cycle    299457266
    Average L2 Active Cycles         cycle   4610067.29
    Total L2 Elapsed Cycles          cycle    125515920
    Average SM Active Cycles         cycle   5160484.57
    Total SM Elapsed Cycles          cycle    299457266
    Average SMSP Active Cycles       cycle   5160625.93
    Total SMSP Elapsed Cycles        cycle   1197829064
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.75
    Elapsed Cycles                cycle      1872194
    Memory Throughput                 %        89.81
    DRAM Throughput                   %        89.81
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        40.78
    L2 Cache Throughput               %        48.99
    SM Active Cycles              cycle   1851957.55
    Compute (SM) Throughput           %        22.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.49
    Achieved Active Warps Per SM           warp        46.79
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12302477.33
    Total DRAM Elapsed Cycles        cycle     82191360
    Average L1 Active Cycles         cycle   1851957.55
    Total L1 Elapsed Cycles          cycle    108316016
    Average L2 Active Cycles         cycle   1865034.96
    Total L2 Elapsed Cycles          cycle     45342624
    Average SM Active Cycles         cycle   1851957.55
    Total SM Elapsed Cycles          cycle    108316016
    Average SMSP Active Cycles       cycle   1839856.31
    Total SMSP Elapsed Cycles        cycle    433264064
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.16
    Elapsed Cycles                cycle      4519784
    Memory Throughput                 %        89.78
    DRAM Throughput                   %        24.19
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.61
    L2 Cache Throughput               %        14.85
    SM Active Cycles              cycle   4421368.28
    Compute (SM) Throughput           %        89.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.94
    Achieved Active Warps Per SM           warp        42.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8584074.67
    Total DRAM Elapsed Cycles        cycle    212880384
    Average L1 Active Cycles         cycle   4421368.28
    Total L1 Elapsed Cycles          cycle    261683236
    Average L2 Active Cycles         cycle      4642657
    Total L2 Elapsed Cycles          cycle    112495200
    Average SM Active Cycles         cycle   4421368.28
    Total SM Elapsed Cycles          cycle    261683236
    Average SMSP Active Cycles       cycle   4416689.76
    Total SMSP Elapsed Cycles        cycle   1046732944
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       825.71
    Elapsed Cycles                cycle         7196
    Memory Throughput                 %        44.32
    DRAM Throughput                   %        44.32
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        24.10
    SM Active Cycles              cycle      4659.43
    Compute (SM) Throughput           %        16.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.83
    Achieved Active Warps Per SM           warp        34.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23749.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4659.43
    Total L1 Elapsed Cycles          cycle       388430
    Average L2 Active Cycles         cycle      4015.33
    Total L2 Elapsed Cycles          cycle       173832
    Average SM Active Cycles         cycle      4659.43
    Total SM Elapsed Cycles          cycle       388430
    Average SMSP Active Cycles       cycle      4467.41
    Total SMSP Elapsed Cycles        cycle      1553720
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       827.37
    Elapsed Cycles                cycle         7633
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        26.91
    SM Active Cycles              cycle      5068.69
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.57
    Achieved Active Warps Per SM           warp        35.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5068.69
    Total L1 Elapsed Cycles          cycle       412212
    Average L2 Active Cycles         cycle      4488.50
    Total L2 Elapsed Cycles          cycle       184344
    Average SM Active Cycles         cycle      5068.69
    Total SM Elapsed Cycles          cycle       412212
    Average SMSP Active Cycles       cycle      5005.92
    Total SMSP Elapsed Cycles        cycle      1648848
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       791.38
    Elapsed Cycles                cycle         7567
    Memory Throughput                 %        47.12
    DRAM Throughput                   %        47.12
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        26.56
    SM Active Cycles              cycle      5038.09
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.68
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5038.09
    Total L1 Elapsed Cycles          cycle       412292
    Average L2 Active Cycles         cycle      4532.46
    Total L2 Elapsed Cycles          cycle       186816
    Average SM Active Cycles         cycle      5038.09
    Total SM Elapsed Cycles          cycle       412292
    Average SMSP Active Cycles       cycle      4933.33
    Total SMSP Elapsed Cycles        cycle      1649168
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.285%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.62% above the average, while the minimum instance value is 17.40% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       789.57
    Elapsed Cycles                cycle         7571
    Memory Throughput                 %        46.82
    DRAM Throughput                   %        46.82
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        22.96
    L2 Cache Throughput               %        26.52
    SM Active Cycles              cycle      5014.24
    Compute (SM) Throughput           %        15.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.90
    Achieved Active Warps Per SM           warp        35.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       356352
    Average L1 Active Cycles         cycle      5014.24
    Total L1 Elapsed Cycles          cycle       427942
    Average L2 Active Cycles         cycle      4408.38
    Total L2 Elapsed Cycles          cycle       187032
    Average SM Active Cycles         cycle      5014.24
    Total SM Elapsed Cycles          cycle       427942
    Average SMSP Active Cycles       cycle      4924.73
    Total SMSP Elapsed Cycles        cycle      1711768
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.017%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.52% above the average, while the minimum instance value is 7.97% below    
          the average.                                                                                                  

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       838.13
    Elapsed Cycles                cycle      5181825
    Memory Throughput                 %        82.65
    DRAM Throughput                   %        16.40
    Duration                         ms         6.16
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.32
    SM Active Cycles              cycle   5160455.60
    Compute (SM) Throughput           %        82.65
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6307837.33
    Total DRAM Elapsed Cycles        cycle    230808576
    Average L1 Active Cycles         cycle   5160455.60
    Total L1 Elapsed Cycles          cycle    299427970
    Average L2 Active Cycles         cycle   4696311.42
    Total L2 Elapsed Cycles          cycle    126089520
    Average SM Active Cycles         cycle   5160455.60
    Total SM Elapsed Cycles          cycle    299427970
    Average SMSP Active Cycles       cycle   5159875.12
    Total SMSP Elapsed Cycles        cycle   1197711880
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       847.90
    Elapsed Cycles                cycle      1868820
    Memory Throughput                 %        90.13
    DRAM Throughput                   %        90.13
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        40.79
    L2 Cache Throughput               %        49.02
    SM Active Cycles              cycle   1865195.81
    Compute (SM) Throughput           %        22.36
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.37
    Achieved Active Warps Per SM           warp        46.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12350352
    Total DRAM Elapsed Cycles        cycle     82219008
    Average L1 Active Cycles         cycle   1865195.81
    Total L1 Elapsed Cycles          cycle    108294348
    Average L2 Active Cycles         cycle   1878112.54
    Total L2 Elapsed Cycles          cycle     45265152
    Average SM Active Cycles         cycle   1865195.81
    Total SM Elapsed Cycles          cycle    108294348
    Average SMSP Active Cycles       cycle   1854469.44
    Total SMSP Elapsed Cycles        cycle    433177392
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.37
    Elapsed Cycles                cycle      4528041
    Memory Throughput                 %        89.57
    DRAM Throughput                   %        24.61
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        91.81
    L2 Cache Throughput               %        15.12
    SM Active Cycles              cycle   4411903.40
    Compute (SM) Throughput           %        89.57
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.13
    Achieved Active Warps Per SM           warp        42.30
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577016
    Total DRAM Elapsed Cycles        cycle    209123328
    Average L1 Active Cycles         cycle   4411903.40
    Total L1 Elapsed Cycles          cycle    262290148
    Average L2 Active Cycles         cycle      4526760
    Total L2 Elapsed Cycles          cycle    110509584
    Average SM Active Cycles         cycle   4411903.40
    Total SM Elapsed Cycles          cycle    262290148
    Average SMSP Active Cycles       cycle   4414476.16
    Total SMSP Elapsed Cycles        cycle   1049160592
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       800.21
    Elapsed Cycles                cycle         6969
    Memory Throughput                 %        45.06
    DRAM Throughput                   %        45.06
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.87
    L2 Cache Throughput               %        24.66
    SM Active Cycles              cycle      4543.66
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.19
    Achieved Active Warps Per SM           warp        34.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23992
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4543.66
    Total L1 Elapsed Cycles          cycle       383758
    Average L2 Active Cycles         cycle      3962.04
    Total L2 Elapsed Cycles          cycle       169968
    Average SM Active Cycles         cycle      4543.66
    Total SM Elapsed Cycles          cycle       383758
    Average SMSP Active Cycles       cycle      4410.21
    Total SMSP Elapsed Cycles        cycle      1535032
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       797.87
    Elapsed Cycles                cycle         7590
    Memory Throughput                 %        47.32
    DRAM Throughput                   %        47.32
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        22.67
    L2 Cache Throughput               %        26.74
    SM Active Cycles              cycle      5025.50
    Compute (SM) Throughput           %        15.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.73
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.27%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      5025.50
    Total L1 Elapsed Cycles          cycle       433124
    Average L2 Active Cycles         cycle      4481.33
    Total L2 Elapsed Cycles          cycle       185568
    Average SM Active Cycles         cycle      5025.50
    Total SM Elapsed Cycles          cycle       433124
    Average SMSP Active Cycles       cycle      4999.13
    Total SMSP Elapsed Cycles        cycle      1732496
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.21
    Elapsed Cycles                cycle         7453
    Memory Throughput                 %        47.20
    DRAM Throughput                   %        47.20
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.54
    L2 Cache Throughput               %        26.74
    SM Active Cycles              cycle      4940.10
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27712
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      4940.10
    Total L1 Elapsed Cycles          cycle       417470
    Average L2 Active Cycles         cycle      4392.33
    Total L2 Elapsed Cycles          cycle       185472
    Average SM Active Cycles         cycle      4940.10
    Total SM Elapsed Cycles          cycle       417470
    Average SMSP Active Cycles       cycle      4799.12
    Total SMSP Elapsed Cycles        cycle      1669880
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       797.53
    Elapsed Cycles                cycle         7461
    Memory Throughput                 %        48.21
    DRAM Throughput                   %        48.21
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.57
    L2 Cache Throughput               %        27.19
    SM Active Cycles              cycle      5051.60
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.29
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5051.60
    Total L1 Elapsed Cycles          cycle       416862
    Average L2 Active Cycles         cycle      4508.96
    Total L2 Elapsed Cycles          cycle       182424
    Average SM Active Cycles         cycle      5051.60
    Total SM Elapsed Cycles          cycle       416862
    Average SMSP Active Cycles       cycle      4997.69
    Total SMSP Elapsed Cycles        cycle      1667448
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.63
    Elapsed Cycles                cycle      5185175
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.24
    Duration                         ms         6.22
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160546.60
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6303773.33
    Total DRAM Elapsed Cycles        cycle    232871936
    Average L1 Active Cycles         cycle   5160546.60
    Total L1 Elapsed Cycles          cycle    299467448
    Average L2 Active Cycles         cycle      4639977
    Total L2 Elapsed Cycles          cycle    125454264
    Average SM Active Cycles         cycle   5160546.60
    Total SM Elapsed Cycles          cycle    299467448
    Average SMSP Active Cycles       cycle   5160179.24
    Total SMSP Elapsed Cycles        cycle   1197869792
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       853.22
    Elapsed Cycles                cycle      1888342
    Memory Throughput                 %        89.33
    DRAM Throughput                   %        89.33
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        41.31
    L2 Cache Throughput               %        48.62
    SM Active Cycles              cycle   1856437.14
    Compute (SM) Throughput           %        22.61
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        98.06
    Achieved Active Warps Per SM           warp        47.07
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12296000
    Total DRAM Elapsed Cycles        cycle     82590720
    Average L1 Active Cycles         cycle   1856437.14
    Total L1 Elapsed Cycles          cycle    107115314
    Average L2 Active Cycles         cycle   1871240.88
    Total L2 Elapsed Cycles          cycle     45724344
    Average SM Active Cycles         cycle   1856437.14
    Total SM Elapsed Cycles          cycle    107115314
    Average SMSP Active Cycles       cycle   1853405.80
    Total SMSP Elapsed Cycles        cycle    428461256
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       841.61
    Elapsed Cycles                cycle      4525745
    Memory Throughput                 %        89.67
    DRAM Throughput                   %        25.66
    Duration                         ms         5.35
    L1/TEX Cache Throughput           %        91.70
    L2 Cache Throughput               %        15.24
    SM Active Cycles              cycle   4417183.88
    Compute (SM) Throughput           %        89.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.02
    Achieved Active Warps Per SM           warp        42.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8579024
    Total DRAM Elapsed Cycles        cycle    200626176
    Average L1 Active Cycles         cycle   4417183.88
    Total L1 Elapsed Cycles          cycle    261981744
    Average L2 Active Cycles         cycle   4524617.38
    Total L2 Elapsed Cycles          cycle    109531440
    Average SM Active Cycles         cycle   4417183.88
    Total SM Elapsed Cycles          cycle    261981744
    Average SMSP Active Cycles       cycle   4414892.28
    Total SMSP Elapsed Cycles        cycle   1047926976
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.88
    Elapsed Cycles                cycle         6889
    Memory Throughput                 %        44.03
    DRAM Throughput                   %        44.03
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        26.08
    L2 Cache Throughput               %        24.42
    SM Active Cycles              cycle      4332.78
    Compute (SM) Throughput           %        16.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.98
    Achieved Active Warps Per SM           warp        35.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23896
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4332.78
    Total L1 Elapsed Cycles          cycle       387788
    Average L2 Active Cycles         cycle      3938.46
    Total L2 Elapsed Cycles          cycle       171408
    Average SM Active Cycles         cycle      4332.78
    Total SM Elapsed Cycles          cycle       387788
    Average SMSP Active Cycles       cycle      4292.56
    Total SMSP Elapsed Cycles        cycle      1551152
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       817.19
    Elapsed Cycles                cycle         7594
    Memory Throughput                 %        48.58
    DRAM Throughput                   %        48.58
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        26.99
    SM Active Cycles              cycle      5068.10
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.76
    Achieved Active Warps Per SM           warp        35.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5068.10
    Total L1 Elapsed Cycles          cycle       413476
    Average L2 Active Cycles         cycle      4470.42
    Total L2 Elapsed Cycles          cycle       183840
    Average SM Active Cycles         cycle      5068.10
    Total SM Elapsed Cycles          cycle       413476
    Average SMSP Active Cycles       cycle      4940.91
    Total SMSP Elapsed Cycles        cycle      1653904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       797.97
    Elapsed Cycles                cycle         7480
    Memory Throughput                 %        48.01
    DRAM Throughput                   %        48.01
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle      5015.90
    Compute (SM) Throughput           %        15.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5015.90
    Total L1 Elapsed Cycles          cycle       420144
    Average L2 Active Cycles         cycle      4558.33
    Total L2 Elapsed Cycles          cycle       183264
    Average SM Active Cycles         cycle      5015.90
    Total SM Elapsed Cycles          cycle       420144
    Average SMSP Active Cycles       cycle      5102.50
    Total SMSP Elapsed Cycles        cycle      1680576
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       797.23
    Elapsed Cycles                cycle         7505
    Memory Throughput                 %        47.89
    DRAM Throughput                   %        47.89
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.04
    SM Active Cycles              cycle      4911.98
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.58
    Achieved Active Warps Per SM           warp        35.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27709.33
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4911.98
    Total L1 Elapsed Cycles          cycle       412364
    Average L2 Active Cycles         cycle      4489.38
    Total L2 Elapsed Cycles          cycle       183456
    Average SM Active Cycles         cycle      4911.98
    Total SM Elapsed Cycles          cycle       412364
    Average SMSP Active Cycles       cycle      4972.67
    Total SMSP Elapsed Cycles        cycle      1649456
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       827.63
    Elapsed Cycles                cycle      5185838
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.19
    Duration                         ms         6.24
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160327.53
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6306669.33
    Total DRAM Elapsed Cycles        cycle    233724928
    Average L1 Active Cycles         cycle   5160327.53
    Total L1 Elapsed Cycles          cycle    299470508
    Average L2 Active Cycles         cycle   4613021.58
    Total L2 Elapsed Cycles          cycle    125499576
    Average SM Active Cycles         cycle   5160327.53
    Total SM Elapsed Cycles          cycle    299470508
    Average SMSP Active Cycles       cycle   5160291.05
    Total SMSP Elapsed Cycles        cycle   1197882032
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.92
    Elapsed Cycles                cycle      1878183
    Memory Throughput                 %        89.73
    DRAM Throughput                   %        89.73
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.41
    L2 Cache Throughput               %        48.83
    SM Active Cycles              cycle   1738776.81
    Compute (SM) Throughput           %        23.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %       104.02
    Achieved Active Warps Per SM           warp        49.93
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12332256
    Total DRAM Elapsed Cycles        cycle     82458624
    Average L1 Active Cycles         cycle   1738776.81
    Total L1 Elapsed Cycles          cycle    101887672
    Average L2 Active Cycles         cycle   1805870.25
    Total L2 Elapsed Cycles          cycle     45494808
    Average SM Active Cycles         cycle   1738776.81
    Total SM Elapsed Cycles          cycle    101887672
    Average SMSP Active Cycles       cycle   1744099.53
    Total SMSP Elapsed Cycles        cycle    407550688
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       846.04
    Elapsed Cycles                cycle      4535378
    Memory Throughput                 %        89.78
    DRAM Throughput                   %        25.74
    Duration                         ms         5.34
    L1/TEX Cache Throughput           %        91.69
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4417614.83
    Compute (SM) Throughput           %        89.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.00
    Achieved Active Warps Per SM           warp        42.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      8577592
    Total DRAM Elapsed Cycles        cycle    199965696
    Average L1 Active Cycles         cycle   4417614.83
    Total L1 Elapsed Cycles          cycle    261677028
    Average L2 Active Cycles         cycle   4528537.88
    Total L2 Elapsed Cycles          cycle    109758864
    Average SM Active Cycles         cycle   4417614.83
    Total SM Elapsed Cycles          cycle    261677028
    Average SMSP Active Cycles       cycle   4414922.17
    Total SMSP Elapsed Cycles        cycle   1046708112
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       798.77
    Elapsed Cycles                cycle         7061
    Memory Throughput                 %        44.34
    DRAM Throughput                   %        44.34
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.96
    L2 Cache Throughput               %        24.32
    SM Active Cycles              cycle      4527.24
    Compute (SM) Throughput           %        17.33
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.28
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24066.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4527.24
    Total L1 Elapsed Cycles          cycle       378256
    Average L2 Active Cycles         cycle      3958.21
    Total L2 Elapsed Cycles          cycle       172272
    Average SM Active Cycles         cycle      4527.24
    Total SM Elapsed Cycles          cycle       378256
    Average SMSP Active Cycles       cycle      4405.50
    Total SMSP Elapsed Cycles        cycle      1513024
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       800.83
    Elapsed Cycles                cycle         7519
    Memory Throughput                 %        48.49
    DRAM Throughput                   %        48.49
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.14
    L2 Cache Throughput               %        27.10
    SM Active Cycles              cycle      5071.98
    Compute (SM) Throughput           %        15.45
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.71
    Achieved Active Warps Per SM           warp        34.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5071.98
    Total L1 Elapsed Cycles          cycle       424056
    Average L2 Active Cycles         cycle      4457.83
    Total L2 Elapsed Cycles          cycle       183240
    Average SM Active Cycles         cycle      5071.98
    Total SM Elapsed Cycles          cycle       424056
    Average SMSP Active Cycles       cycle      5037.55
    Total SMSP Elapsed Cycles        cycle      1696224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       796.85
    Elapsed Cycles                cycle         7472
    Memory Throughput                 %        48.30
    DRAM Throughput                   %        48.30
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.14
    SM Active Cycles              cycle      5023.29
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.15
    Achieved Active Warps Per SM           warp        34.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5023.29
    Total L1 Elapsed Cycles          cycle       415298
    Average L2 Active Cycles         cycle      4536.79
    Total L2 Elapsed Cycles          cycle       182952
    Average SM Active Cycles         cycle      5023.29
    Total SM Elapsed Cycles          cycle       415298
    Average SMSP Active Cycles       cycle      4999.15
    Total SMSP Elapsed Cycles        cycle      1661192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       802.81
    Elapsed Cycles                cycle         7458
    Memory Throughput                 %        48.72
    DRAM Throughput                   %        48.72
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        27.24
    SM Active Cycles              cycle         5061
    Compute (SM) Throughput           %        15.68
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.34
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle         5061
    Total L1 Elapsed Cycles          cycle       417864
    Average L2 Active Cycles         cycle      4551.92
    Total L2 Elapsed Cycles          cycle       182016
    Average SM Active Cycles         cycle         5061
    Total SM Elapsed Cycles          cycle       417864
    Average SMSP Active Cycles       cycle      5093.15
    Total SMSP Elapsed Cycles        cycle      1671456
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.99
    Elapsed Cycles                cycle      5186337
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.24
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.38
    SM Active Cycles              cycle   5160275.57
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301709.33
    Total DRAM Elapsed Cycles        cycle    232772608
    Average L1 Active Cycles         cycle   5160275.57
    Total L1 Elapsed Cycles          cycle    299451898
    Average L2 Active Cycles         cycle   4640113.96
    Total L2 Elapsed Cycles          cycle    125537112
    Average SM Active Cycles         cycle   5160275.57
    Total SM Elapsed Cycles          cycle    299451898
    Average SMSP Active Cycles       cycle   5160263.24
    Total SMSP Elapsed Cycles        cycle   1197807592
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       853.14
    Elapsed Cycles                cycle      1878695
    Memory Throughput                 %        89.92
    DRAM Throughput                   %        89.92
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.12
    L2 Cache Throughput               %        48.89
    SM Active Cycles              cycle   1852860.67
    Compute (SM) Throughput           %        22.52
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.54
    Achieved Active Warps Per SM           warp        46.82
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12311669.33
    Total DRAM Elapsed Cycles        cycle     82154496
    Average L1 Active Cycles         cycle   1852860.67
    Total L1 Elapsed Cycles          cycle    107513184
    Average L2 Active Cycles         cycle   1870441.21
    Total L2 Elapsed Cycles          cycle     45502536
    Average SM Active Cycles         cycle   1852860.67
    Total SM Elapsed Cycles          cycle    107513184
    Average SMSP Active Cycles       cycle   1845833.56
    Total SMSP Elapsed Cycles        cycle    430052736
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.63
    Elapsed Cycles                cycle      4530434
    Memory Throughput                 %        89.44
    DRAM Throughput                   %        25.68
    Duration                         ms         5.35
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.22
    SM Active Cycles              cycle   4414185.64
    Compute (SM) Throughput           %        89.44
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.08
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8584741.33
    Total DRAM Elapsed Cycles        cycle    200564736
    Average L1 Active Cycles         cycle   4414185.64
    Total L1 Elapsed Cycles          cycle    262675220
    Average L2 Active Cycles         cycle   4528535.17
    Total L2 Elapsed Cycles          cycle    109657128
    Average SM Active Cycles         cycle   4414185.64
    Total SM Elapsed Cycles          cycle    262675220
    Average SMSP Active Cycles       cycle   4414617.57
    Total SMSP Elapsed Cycles        cycle   1050700880
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       797.55
    Elapsed Cycles                cycle         6913
    Memory Throughput                 %        44.58
    DRAM Throughput                   %        44.58
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.90
    L2 Cache Throughput               %        24.76
    SM Active Cycles              cycle      4537.12
    Compute (SM) Throughput           %        17.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.72
    Achieved Active Warps Per SM           warp        33.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23813.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4537.12
    Total L1 Elapsed Cycles          cycle       381692
    Average L2 Active Cycles         cycle      4025.58
    Total L2 Elapsed Cycles          cycle       169296
    Average SM Active Cycles         cycle      4537.12
    Total SM Elapsed Cycles          cycle       381692
    Average SMSP Active Cycles       cycle      4434.42
    Total SMSP Elapsed Cycles        cycle      1526768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       801.00
    Elapsed Cycles                cycle         7600
    Memory Throughput                 %        47.69
    DRAM Throughput                   %        47.69
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.55
    L2 Cache Throughput               %        26.77
    SM Active Cycles              cycle      5068.14
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.22
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5068.14
    Total L1 Elapsed Cycles          cycle       417150
    Average L2 Active Cycles         cycle      4479.83
    Total L2 Elapsed Cycles          cycle       185232
    Average SM Active Cycles         cycle      5068.14
    Total SM Elapsed Cycles          cycle       417150
    Average SMSP Active Cycles       cycle      5024.00
    Total SMSP Elapsed Cycles        cycle      1668600
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.61
    Elapsed Cycles                cycle         7453
    Memory Throughput                 %        48.43
    DRAM Throughput                   %        48.43
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        27.24
    SM Active Cycles              cycle      4983.28
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.31
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.69%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4983.28
    Total L1 Elapsed Cycles          cycle       414636
    Average L2 Active Cycles         cycle      4535.21
    Total L2 Elapsed Cycles          cycle       182184
    Average SM Active Cycles         cycle      4983.28
    Total SM Elapsed Cycles          cycle       414636
    Average SMSP Active Cycles       cycle      5031.34
    Total SMSP Elapsed Cycles        cycle      1658544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       802.72
    Elapsed Cycles                cycle         7584
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        26.78
    SM Active Cycles              cycle      5115.83
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.91
    Achieved Active Warps Per SM           warp        34.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5115.83
    Total L1 Elapsed Cycles          cycle       418458
    Average L2 Active Cycles         cycle      4457.04
    Total L2 Elapsed Cycles          cycle       185208
    Average SM Active Cycles         cycle      5115.83
    Total SM Elapsed Cycles          cycle       418458
    Average SMSP Active Cycles       cycle      4978.32
    Total SMSP Elapsed Cycles        cycle      1673832
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       830.48
    Elapsed Cycles                cycle      5186714
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.24
    Duration                         ms         6.22
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160228.09
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302493.33
    Total DRAM Elapsed Cycles        cycle    232912896
    Average L1 Active Cycles         cycle   5160228.09
    Total L1 Elapsed Cycles          cycle    299453520
    Average L2 Active Cycles         cycle   4609706.96
    Total L2 Elapsed Cycles          cycle    125489424
    Average SM Active Cycles         cycle   5160228.09
    Total SM Elapsed Cycles          cycle    299453520
    Average SMSP Active Cycles       cycle   5160297.69
    Total SMSP Elapsed Cycles        cycle   1197814080
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.67
    Elapsed Cycles                cycle      1760340
    Memory Throughput                 %        89.72
    DRAM Throughput                   %        89.72
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        43.44
    L2 Cache Throughput               %        50.92
    SM Active Cycles              cycle   1739611.64
    Compute (SM) Throughput           %        23.78
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.53
    Achieved Active Warps Per SM           warp        46.81
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12337837.33
    Total DRAM Elapsed Cycles        cycle     82507776
    Average L1 Active Cycles         cycle   1739611.64
    Total L1 Elapsed Cycles          cycle    101829680
    Average L2 Active Cycles         cycle   1806666.62
    Total L2 Elapsed Cycles          cycle     43605264
    Average SM Active Cycles         cycle   1739611.64
    Total SM Elapsed Cycles          cycle    101829680
    Average SMSP Active Cycles       cycle   1746213.69
    Total SMSP Elapsed Cycles        cycle    407318720
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.33
    Elapsed Cycles                cycle      4535864
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        25.70
    Duration                         ms         5.34
    L1/TEX Cache Throughput           %        91.76
    L2 Cache Throughput               %        15.21
    SM Active Cycles              cycle   4414141.47
    Compute (SM) Throughput           %        89.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8574618.67
    Total DRAM Elapsed Cycles        cycle    200206336
    Average L1 Active Cycles         cycle   4414141.47
    Total L1 Elapsed Cycles          cycle    261688164
    Average L2 Active Cycles         cycle   4531481.12
    Total L2 Elapsed Cycles          cycle    109760880
    Average SM Active Cycles         cycle   4414141.47
    Total SM Elapsed Cycles          cycle    261688164
    Average SMSP Active Cycles       cycle   4417080.40
    Total SMSP Elapsed Cycles        cycle   1046752656
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.45
    Elapsed Cycles                cycle         7006
    Memory Throughput                 %        43.92
    DRAM Throughput                   %        43.92
    Duration                         us         8.93
    L1/TEX Cache Throughput           %        25.27
    L2 Cache Throughput               %        24.06
    SM Active Cycles              cycle      4472.29
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.1%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24061.33
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4472.29
    Total L1 Elapsed Cycles          cycle       384012
    Average L2 Active Cycles         cycle      4225.75
    Total L2 Elapsed Cycles          cycle       174408
    Average SM Active Cycles         cycle      4472.29
    Total SM Elapsed Cycles          cycle       384012
    Average SMSP Active Cycles       cycle      4518.41
    Total SMSP Elapsed Cycles        cycle      1536048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       789.55
    Elapsed Cycles                cycle         7393
    Memory Throughput                 %        48.30
    DRAM Throughput                   %        48.30
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.14
    SM Active Cycles              cycle      4996.38
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.72
    Achieved Active Warps Per SM           warp        34.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4996.38
    Total L1 Elapsed Cycles          cycle       416396
    Average L2 Active Cycles         cycle      4440.83
    Total L2 Elapsed Cycles          cycle       182856
    Average SM Active Cycles         cycle      4996.38
    Total SM Elapsed Cycles          cycle       416396
    Average SMSP Active Cycles       cycle      4882.36
    Total SMSP Elapsed Cycles        cycle      1665584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       796.74
    Elapsed Cycles                cycle         7501
    Memory Throughput                 %        48.55
    DRAM Throughput                   %        48.55
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.42
    L2 Cache Throughput               %        27.08
    SM Active Cycles              cycle      5261.76
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 31.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.44
    Achieved Active Warps Per SM           warp        32.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.56%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27842.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5261.76
    Total L1 Elapsed Cycles          cycle       419342
    Average L2 Active Cycles         cycle      4515.75
    Total L2 Elapsed Cycles          cycle       183312
    Average SM Active Cycles         cycle      5261.76
    Total SM Elapsed Cycles          cycle       419342
    Average SMSP Active Cycles       cycle      5022.22
    Total SMSP Elapsed Cycles        cycle      1677368
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       826.89
    Elapsed Cycles                cycle         7652
    Memory Throughput                 %        48.73
    DRAM Throughput                   %        48.73
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.47
    L2 Cache Throughput               %        26.85
    SM Active Cycles              cycle      5005.95
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.38
    Achieved Active Warps Per SM           warp        36.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27696
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5005.95
    Total L1 Elapsed Cycles          cycle       418416
    Average L2 Active Cycles         cycle      4470.67
    Total L2 Elapsed Cycles          cycle       184920
    Average SM Active Cycles         cycle      5005.95
    Total SM Elapsed Cycles          cycle       418416
    Average SMSP Active Cycles       cycle      4980.74
    Total SMSP Elapsed Cycles        cycle      1673664
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.94
    Elapsed Cycles                cycle      5184673
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.27
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160419.12
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.67
    Achieved Active Warps Per SM           warp        46.40
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302610.67
    Total DRAM Elapsed Cycles        cycle    232496128
    Average L1 Active Cycles         cycle   5160419.12
    Total L1 Elapsed Cycles          cycle    299474718
    Average L2 Active Cycles         cycle   4612032.79
    Total L2 Elapsed Cycles          cycle    125475288
    Average SM Active Cycles         cycle   5160419.12
    Total SM Elapsed Cycles          cycle    299474718
    Average SMSP Active Cycles       cycle   5160473.16
    Total SMSP Elapsed Cycles        cycle   1197898872
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       797.77
    Elapsed Cycles                cycle      1752879
    Memory Throughput                 %        90.15
    DRAM Throughput                   %        90.15
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.07
    L2 Cache Throughput               %        51.23
    SM Active Cycles              cycle   1751599.26
    Compute (SM) Throughput           %        22.51
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.37
    Achieved Active Warps Per SM           warp        46.26
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12344429.33
    Total DRAM Elapsed Cycles        cycle     82157568
    Average L1 Active Cycles         cycle   1751599.26
    Total L1 Elapsed Cycles          cycle    107585426
    Average L2 Active Cycles         cycle   1870881.50
    Total L2 Elapsed Cycles          cycle     43421280
    Average SM Active Cycles         cycle   1751599.26
    Total SM Elapsed Cycles          cycle    107585426
    Average SMSP Active Cycles       cycle      1845876
    Total SMSP Elapsed Cycles        cycle    430341704
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.41
    Elapsed Cycles                cycle      4537910
    Memory Throughput                 %        89.67
    DRAM Throughput                   %        25.62
    Duration                         ms         5.36
    L1/TEX Cache Throughput           %        91.66
    L2 Cache Throughput               %        15.19
    SM Active Cycles              cycle   4419130.03
    Compute (SM) Throughput           %        89.67
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.98
    Achieved Active Warps Per SM           warp        42.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8583834.67
    Total DRAM Elapsed Cycles        cycle    200991744
    Average L1 Active Cycles         cycle   4419130.03
    Total L1 Elapsed Cycles          cycle    261993472
    Average L2 Active Cycles         cycle   4639392.25
    Total L2 Elapsed Cycles          cycle    109961424
    Average SM Active Cycles         cycle   4419130.03
    Total SM Elapsed Cycles          cycle    261993472
    Average SMSP Active Cycles       cycle   4416467.50
    Total SMSP Elapsed Cycles        cycle   1047973888
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       786.02
    Elapsed Cycles                cycle         6906
    Memory Throughput                 %        44.23
    DRAM Throughput                   %        44.23
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        25.46
    L2 Cache Throughput               %        24.51
    SM Active Cycles              cycle      4437.55
    Compute (SM) Throughput           %        16.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23778.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4437.55
    Total L1 Elapsed Cycles          cycle       386482
    Average L2 Active Cycles         cycle      4012.25
    Total L2 Elapsed Cycles          cycle       170856
    Average SM Active Cycles         cycle      4437.55
    Total SM Elapsed Cycles          cycle       386482
    Average SMSP Active Cycles       cycle      4356.01
    Total SMSP Elapsed Cycles        cycle      1545928
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       788.51
    Elapsed Cycles                cycle         7537
    Memory Throughput                 %        47.38
    DRAM Throughput                   %        47.38
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        26.61
    SM Active Cycles              cycle      4965.57
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.80
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       352256
    Average L1 Active Cycles         cycle      4965.57
    Total L1 Elapsed Cycles          cycle       411120
    Average L2 Active Cycles         cycle      4469.71
    Total L2 Elapsed Cycles          cycle       186312
    Average SM Active Cycles         cycle      4965.57
    Total SM Elapsed Cycles          cycle       411120
    Average SMSP Active Cycles       cycle      4889.55
    Total SMSP Elapsed Cycles        cycle      1644480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       790.44
    Elapsed Cycles                cycle         7559
    Memory Throughput                 %        47.12
    DRAM Throughput                   %        47.12
    Duration                         us         9.54
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        26.60
    SM Active Cycles              cycle      5115.74
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.96
    Achieved Active Warps Per SM           warp        34.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      5115.74
    Total L1 Elapsed Cycles          cycle       419038
    Average L2 Active Cycles         cycle      4472.54
    Total L2 Elapsed Cycles          cycle       186528
    Average SM Active Cycles         cycle      5115.74
    Total SM Elapsed Cycles          cycle       419038
    Average SMSP Active Cycles       cycle      4882.88
    Total SMSP Elapsed Cycles        cycle      1676152
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       814.79
    Elapsed Cycles                cycle         7652
    Memory Throughput                 %        48.20
    DRAM Throughput                   %        48.20
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.34
    L2 Cache Throughput               %        26.81
    SM Active Cycles              cycle      4997.66
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.22
    Achieved Active Warps Per SM           warp        35.63
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27720
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      4997.66
    Total L1 Elapsed Cycles          cycle       420724
    Average L2 Active Cycles         cycle      4468.79
    Total L2 Elapsed Cycles          cycle       185160
    Average SM Active Cycles         cycle      4997.66
    Total SM Elapsed Cycles          cycle       420724
    Average SMSP Active Cycles       cycle      5098.01
    Total SMSP Elapsed Cycles        cycle      1682896
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       850.79
    Elapsed Cycles                cycle      5184925
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.63
    Duration                         ms         6.07
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160278.45
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6302274.67
    Total DRAM Elapsed Cycles        cycle    227349504
    Average L1 Active Cycles         cycle   5160278.45
    Total L1 Elapsed Cycles          cycle    299464576
    Average L2 Active Cycles         cycle   4636417.38
    Total L2 Elapsed Cycles          cycle    125473800
    Average SM Active Cycles         cycle   5160278.45
    Total SM Elapsed Cycles          cycle    299464576
    Average SMSP Active Cycles       cycle   5160152.36
    Total SMSP Elapsed Cycles        cycle   1197858304
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       807.51
    Elapsed Cycles                cycle      1785388
    Memory Throughput                 %        89.75
    DRAM Throughput                   %        89.75
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        42.99
    L2 Cache Throughput               %        50.84
    SM Active Cycles              cycle   1768553.72
    Compute (SM) Throughput           %        23.54
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.11
    Achieved Active Warps Per SM           warp        46.61
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12341210.67
    Total DRAM Elapsed Cycles        cycle     82501632
    Average L1 Active Cycles         cycle   1768553.72
    Total L1 Elapsed Cycles          cycle    102852734
    Average L2 Active Cycles         cycle   1811319.33
    Total L2 Elapsed Cycles          cycle     43679040
    Average SM Active Cycles         cycle   1768553.72
    Total SM Elapsed Cycles          cycle    102852734
    Average SMSP Active Cycles       cycle   1770805.10
    Total SMSP Elapsed Cycles        cycle    411410936
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.92
    Elapsed Cycles                cycle      4543954
    Memory Throughput                 %        89.87
    DRAM Throughput                   %        25.58
    Duration                         ms         5.37
    L1/TEX Cache Throughput           %        91.77
    L2 Cache Throughput               %        15.17
    SM Active Cycles              cycle   4413867.98
    Compute (SM) Throughput           %        89.87
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.09
    Achieved Active Warps Per SM           warp        42.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8575013.33
    Total DRAM Elapsed Cycles        cycle    201145344
    Average L1 Active Cycles         cycle   4413867.98
    Total L1 Elapsed Cycles          cycle    261417056
    Average L2 Active Cycles         cycle   4524351.42
    Total L2 Elapsed Cycles          cycle    110001624
    Average SM Active Cycles         cycle   4413867.98
    Total SM Elapsed Cycles          cycle    261417056
    Average SMSP Active Cycles       cycle   4414685.38
    Total SMSP Elapsed Cycles        cycle   1045668224
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       812.80
    Elapsed Cycles                cycle         7110
    Memory Throughput                 %        44.64
    DRAM Throughput                   %        44.64
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        25.22
    L2 Cache Throughput               %        24.42
    SM Active Cycles              cycle      4479.47
    Compute (SM) Throughput           %        16.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23997.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4479.47
    Total L1 Elapsed Cycles          cycle       389694
    Average L2 Active Cycles         cycle      3999.21
    Total L2 Elapsed Cycles          cycle       171768
    Average SM Active Cycles         cycle      4479.47
    Total SM Elapsed Cycles          cycle       389694
    Average SMSP Active Cycles       cycle      4424.74
    Total SMSP Elapsed Cycles        cycle      1558776
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       797.40
    Elapsed Cycles                cycle         7589
    Memory Throughput                 %        47.93
    DRAM Throughput                   %        47.93
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        26.77
    SM Active Cycles              cycle      5033.28
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.50
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5033.28
    Total L1 Elapsed Cycles          cycle       416244
    Average L2 Active Cycles         cycle      4467.62
    Total L2 Elapsed Cycles          cycle       185280
    Average SM Active Cycles         cycle      5033.28
    Total SM Elapsed Cycles          cycle       416244
    Average SMSP Active Cycles       cycle      4958.79
    Total SMSP Elapsed Cycles        cycle      1664976
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       788.81
    Elapsed Cycles                cycle         7466
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.41
    L2 Cache Throughput               %        26.91
    SM Active Cycles              cycle      5039.40
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.12
    Achieved Active Warps Per SM           warp        34.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5039.40
    Total L1 Elapsed Cycles          cycle       419786
    Average L2 Active Cycles         cycle      4519.38
    Total L2 Elapsed Cycles          cycle       184368
    Average SM Active Cycles         cycle      5039.40
    Total SM Elapsed Cycles          cycle       419786
    Average SMSP Active Cycles       cycle      4939.61
    Total SMSP Elapsed Cycles        cycle      1679144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       789.29
    Elapsed Cycles                cycle         7517
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.52
    L2 Cache Throughput               %        26.73
    SM Active Cycles              cycle      5048.07
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.99
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5048.07
    Total L1 Elapsed Cycles          cycle       417414
    Average L2 Active Cycles         cycle      4497.29
    Total L2 Elapsed Cycles          cycle       185688
    Average SM Active Cycles         cycle      5048.07
    Total SM Elapsed Cycles          cycle       417414
    Average SMSP Active Cycles       cycle      4891.65
    Total SMSP Elapsed Cycles        cycle      1669656
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       851.05
    Elapsed Cycles                cycle      5184497
    Memory Throughput                 %        82.63
    DRAM Throughput                   %        16.65
    Duration                         ms         6.07
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160434.78
    Compute (SM) Throughput           %        82.63
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6306754.67
    Total DRAM Elapsed Cycles        cycle    227305472
    Average L1 Active Cycles         cycle   5160434.78
    Total L1 Elapsed Cycles          cycle    299471588
    Average L2 Active Cycles         cycle   4611583.58
    Total L2 Elapsed Cycles          cycle    125462256
    Average SM Active Cycles         cycle   5160434.78
    Total SM Elapsed Cycles          cycle    299471588
    Average SMSP Active Cycles       cycle   5160782.76
    Total SMSP Elapsed Cycles        cycle   1197886352
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       842.53
    Elapsed Cycles                cycle      1855511
    Memory Throughput                 %        89.92
    DRAM Throughput                   %        89.92
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.02
    L2 Cache Throughput               %        49.41
    SM Active Cycles              cycle   1849267.47
    Compute (SM) Throughput           %        22.47
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12319989.33
    Total DRAM Elapsed Cycles        cycle     82204672
    Average L1 Active Cycles         cycle   1849267.47
    Total L1 Elapsed Cycles          cycle    107773182
    Average L2 Active Cycles         cycle   1872033.79
    Total L2 Elapsed Cycles          cycle     44951280
    Average SM Active Cycles         cycle   1849267.47
    Total SM Elapsed Cycles          cycle    107773182
    Average SMSP Active Cycles       cycle   1851129.17
    Total SMSP Elapsed Cycles        cycle    431092728
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.18
    Elapsed Cycles                cycle      4516093
    Memory Throughput                 %        89.77
    DRAM Throughput                   %        24.22
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.67
    L2 Cache Throughput               %        14.86
    SM Active Cycles              cycle   4418761.90
    Compute (SM) Throughput           %        89.77
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        87.99
    Achieved Active Warps Per SM           warp        42.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8585309.33
    Total DRAM Elapsed Cycles        cycle    212680704
    Average L1 Active Cycles         cycle   4418761.90
    Total L1 Elapsed Cycles          cycle    261706872
    Average L2 Active Cycles         cycle   4639363.29
    Total L2 Elapsed Cycles          cycle    112390416
    Average SM Active Cycles         cycle   4418761.90
    Total SM Elapsed Cycles          cycle    261706872
    Average SMSP Active Cycles       cycle   4414800.17
    Total SMSP Elapsed Cycles        cycle   1046827488
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.14
    Elapsed Cycles                cycle         6885
    Memory Throughput                 %        45.12
    DRAM Throughput                   %        45.12
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.06
    L2 Cache Throughput               %        24.95
    SM Active Cycles              cycle      4508.38
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.97
    Achieved Active Warps Per SM           warp        34.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4508.38
    Total L1 Elapsed Cycles          cycle       381906
    Average L2 Active Cycles         cycle      4051.62
    Total L2 Elapsed Cycles          cycle       167928
    Average SM Active Cycles         cycle      4508.38
    Total SM Elapsed Cycles          cycle       381906
    Average SMSP Active Cycles       cycle      4516.75
    Total SMSP Elapsed Cycles        cycle      1527624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.101%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.81% above the average, while the minimum instance value is 3.20% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       797.34
    Elapsed Cycles                cycle         7526
    Memory Throughput                 %        47.95
    DRAM Throughput                   %        47.95
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        26.92
    SM Active Cycles              cycle      5027.91
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.78
    Achieved Active Warps Per SM           warp        34.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.22%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5027.91
    Total L1 Elapsed Cycles          cycle       407568
    Average L2 Active Cycles         cycle      4376.50
    Total L2 Elapsed Cycles          cycle       184272
    Average SM Active Cycles         cycle      5027.91
    Total SM Elapsed Cycles          cycle       407568
    Average SMSP Active Cycles       cycle      4844.36
    Total SMSP Elapsed Cycles        cycle      1630272
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       804.68
    Elapsed Cycles                cycle         7631
    Memory Throughput                 %        48.00
    DRAM Throughput                   %        48.00
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        26.68
    SM Active Cycles              cycle      5166.95
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.86
    Achieved Active Warps Per SM           warp        34.97
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      5166.95
    Total L1 Elapsed Cycles          cycle       416456
    Average L2 Active Cycles         cycle      4432.83
    Total L2 Elapsed Cycles          cycle       185784
    Average SM Active Cycles         cycle      5166.95
    Total SM Elapsed Cycles          cycle       416456
    Average SMSP Active Cycles       cycle      4906.75
    Total SMSP Elapsed Cycles        cycle      1665824
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       799.38
    Elapsed Cycles                cycle         7529
    Memory Throughput                 %        48.00
    DRAM Throughput                   %        48.00
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        26.97
    SM Active Cycles              cycle      5123.33
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.06
    Achieved Active Warps Per SM           warp        34.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      5123.33
    Total L1 Elapsed Cycles          cycle       410016
    Average L2 Active Cycles         cycle      4435.12
    Total L2 Elapsed Cycles          cycle       183936
    Average SM Active Cycles         cycle      5123.33
    Total SM Elapsed Cycles          cycle       410016
    Average SMSP Active Cycles       cycle      4948.02
    Total SMSP Elapsed Cycles        cycle      1640064
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       832.57
    Elapsed Cycles                cycle      5183847
    Memory Throughput                 %        82.62
    DRAM Throughput                   %        16.28
    Duration                         ms         6.20
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160314.36
    Compute (SM) Throughput           %        82.62
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6302280
    Total DRAM Elapsed Cycles        cycle    232329216
    Average L1 Active Cycles         cycle   5160314.36
    Total L1 Elapsed Cycles          cycle    299505358
    Average L2 Active Cycles         cycle   4609975.58
    Total L2 Elapsed Cycles          cycle    125457096
    Average SM Active Cycles         cycle   5160314.36
    Total SM Elapsed Cycles          cycle    299505358
    Average SMSP Active Cycles       cycle   5160507.14
    Total SMSP Elapsed Cycles        cycle   1198021432
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       849.57
    Elapsed Cycles                cycle      1864584
    Memory Throughput                 %        90.59
    DRAM Throughput                   %        90.59
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.08
    L2 Cache Throughput               %        49.18
    SM Active Cycles              cycle   1842172.69
    Compute (SM) Throughput           %        22.55
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.22
    Achieved Active Warps Per SM           warp        46.67
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     12367144
    Total DRAM Elapsed Cycles        cycle     81911808
    Average L1 Active Cycles         cycle   1842172.69
    Total L1 Elapsed Cycles          cycle    107388636
    Average L2 Active Cycles         cycle   1866228.71
    Total L2 Elapsed Cycles          cycle     45167712
    Average SM Active Cycles         cycle   1842172.69
    Total SM Elapsed Cycles          cycle    107388636
    Average SMSP Active Cycles       cycle   1839326.06
    Total SMSP Elapsed Cycles        cycle    429554544
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       847.68
    Elapsed Cycles                cycle      4529013
    Memory Throughput                 %        89.69
    DRAM Throughput                   %        25.81
    Duration                         ms         5.32
    L1/TEX Cache Throughput           %        91.67
    L2 Cache Throughput               %        15.25
    SM Active Cycles              cycle   4418698.19
    Compute (SM) Throughput           %        89.69
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.00
    Achieved Active Warps Per SM           warp        42.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8573917.33
    Total DRAM Elapsed Cycles        cycle    199345152
    Average L1 Active Cycles         cycle   4418698.19
    Total L1 Elapsed Cycles          cycle    261926624
    Average L2 Active Cycles         cycle   4639359.42
    Total L2 Elapsed Cycles          cycle    109580808
    Average SM Active Cycles         cycle   4418698.19
    Total SM Elapsed Cycles          cycle    261926624
    Average SMSP Active Cycles       cycle   4417932.52
    Total SMSP Elapsed Cycles        cycle   1047706496
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.81
    Elapsed Cycles                cycle         6909
    Memory Throughput                 %        45.35
    DRAM Throughput                   %        45.35
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.79
    L2 Cache Throughput               %        24.81
    SM Active Cycles              cycle      4557.53
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.59
    Achieved Active Warps Per SM           warp        33.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23994.67
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4557.53
    Total L1 Elapsed Cycles          cycle       382030
    Average L2 Active Cycles         cycle      4112.71
    Total L2 Elapsed Cycles          cycle       168936
    Average SM Active Cycles         cycle      4557.53
    Total SM Elapsed Cycles          cycle       382030
    Average SMSP Active Cycles       cycle      4515.69
    Total SMSP Elapsed Cycles        cycle      1528120
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       810.73
    Elapsed Cycles                cycle         7714
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        26.62
    SM Active Cycles              cycle      5068.90
    Compute (SM) Throughput           %        15.59
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.62
    Achieved Active Warps Per SM           warp        35.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5068.90
    Total L1 Elapsed Cycles          cycle       420254
    Average L2 Active Cycles         cycle         4515
    Total L2 Elapsed Cycles          cycle       186456
    Average SM Active Cycles         cycle      5068.90
    Total SM Elapsed Cycles          cycle       420254
    Average SMSP Active Cycles       cycle      5044.53
    Total SMSP Elapsed Cycles        cycle      1681016
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       810.22
    Elapsed Cycles                cycle         7500
    Memory Throughput                 %        48.56
    DRAM Throughput                   %        48.56
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.25
    SM Active Cycles              cycle      5099.59
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.75
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5099.59
    Total L1 Elapsed Cycles          cycle       416552
    Average L2 Active Cycles         cycle      4558.25
    Total L2 Elapsed Cycles          cycle       182088
    Average SM Active Cycles         cycle      5099.59
    Total SM Elapsed Cycles          cycle       416552
    Average SMSP Active Cycles       cycle      5109.94
    Total SMSP Elapsed Cycles        cycle      1666208
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       798.03
    Elapsed Cycles                cycle         7569
    Memory Throughput                 %        47.78
    DRAM Throughput                   %        47.78
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.12
    L2 Cache Throughput               %        26.87
    SM Active Cycles              cycle      4886.86
    Compute (SM) Throughput           %        15.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.36
    Achieved Active Warps Per SM           warp        36.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       348160
    Average L1 Active Cycles         cycle      4886.86
    Total L1 Elapsed Cycles          cycle       425972
    Average L2 Active Cycles         cycle      4443.92
    Total L2 Elapsed Cycles          cycle       184704
    Average SM Active Cycles         cycle      4886.86
    Total SM Elapsed Cycles          cycle       425972
    Average SMSP Active Cycles       cycle      4903.13
    Total SMSP Elapsed Cycles        cycle      1703888
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       836.43
    Elapsed Cycles                cycle      5183628
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.35
    Duration                         ms         6.17
    L1/TEX Cache Throughput           %        82.68
    L2 Cache Throughput               %        14.39
    SM Active Cycles              cycle   5160153.14
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.68
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301818.67
    Total DRAM Elapsed Cycles        cycle    231255040
    Average L1 Active Cycles         cycle   5160153.14
    Total L1 Elapsed Cycles          cycle    299452490
    Average L2 Active Cycles         cycle      4611574
    Total L2 Elapsed Cycles          cycle    125496648
    Average SM Active Cycles         cycle   5160153.14
    Total SM Elapsed Cycles          cycle    299452490
    Average SMSP Active Cycles       cycle   5160212.43
    Total SMSP Elapsed Cycles        cycle   1197809960
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       850.00
    Elapsed Cycles                cycle      1865266
    Memory Throughput                 %        90.39
    DRAM Throughput                   %        90.39
    Duration                         ms         2.19
    L1/TEX Cache Throughput           %        41.15
    L2 Cache Throughput               %        49.23
    SM Active Cycles              cycle   1836202.93
    Compute (SM) Throughput           %        22.56
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        97.70
    Achieved Active Warps Per SM           warp        46.90
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12340749.33
    Total DRAM Elapsed Cycles        cycle     81914880
    Average L1 Active Cycles         cycle   1836202.93
    Total L1 Elapsed Cycles          cycle    107337372
    Average L2 Active Cycles         cycle   1865029.96
    Total L2 Elapsed Cycles          cycle     45178464
    Average SM Active Cycles         cycle   1836202.93
    Total SM Elapsed Cycles          cycle    107337372
    Average SMSP Active Cycles       cycle   1840420.27
    Total SMSP Elapsed Cycles        cycle    429349488
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       844.04
    Elapsed Cycles                cycle      4526873
    Memory Throughput                 %        89.58
    DRAM Throughput                   %        25.71
    Duration                         ms         5.34
    L1/TEX Cache Throughput           %        91.75
    L2 Cache Throughput               %        15.23
    SM Active Cycles              cycle   4414658.02
    Compute (SM) Throughput           %        89.58
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.06
    Achieved Active Warps Per SM           warp        42.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8578882.67
    Total DRAM Elapsed Cycles        cycle    200183808
    Average L1 Active Cycles         cycle   4414658.02
    Total L1 Elapsed Cycles          cycle    262262690
    Average L2 Active Cycles         cycle   4522068.62
    Total L2 Elapsed Cycles          cycle    109572768
    Average SM Active Cycles         cycle   4414658.02
    Total SM Elapsed Cycles          cycle    262262690
    Average SMSP Active Cycles       cycle   4413536.53
    Total SMSP Elapsed Cycles        cycle   1049050760
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.03
    Elapsed Cycles                cycle         7012
    Memory Throughput                 %        43.08
    DRAM Throughput                   %        43.08
    Duration                         us         8.96
    L1/TEX Cache Throughput           %        25.14
    L2 Cache Throughput               %        24.01
    SM Active Cycles              cycle      4494.69
    Compute (SM) Throughput           %        17.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.03
    Achieved Active Warps Per SM           warp        35.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23746.67
    Total DRAM Elapsed Cycles        cycle       330752
    Average L1 Active Cycles         cycle      4494.69
    Total L1 Elapsed Cycles          cycle       385226
    Average L2 Active Cycles         cycle      3914.25
    Total L2 Elapsed Cycles          cycle       174456
    Average SM Active Cycles         cycle      4494.69
    Total SM Elapsed Cycles          cycle       385226
    Average SMSP Active Cycles       cycle      4350.03
    Total SMSP Elapsed Cycles        cycle      1540904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.505%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.22% above the average, while the minimum instance value is 3.05% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       789.96
    Elapsed Cycles                cycle         7525
    Memory Throughput                 %        47.18
    DRAM Throughput                   %        47.18
    Duration                         us         9.50
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        26.71
    SM Active Cycles              cycle      4924.84
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.35
    Achieved Active Warps Per SM           warp        35.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       353280
    Average L1 Active Cycles         cycle      4924.84
    Total L1 Elapsed Cycles          cycle       413458
    Average L2 Active Cycles         cycle      4460.67
    Total L2 Elapsed Cycles          cycle       185832
    Average SM Active Cycles         cycle      4924.84
    Total SM Elapsed Cycles          cycle       413458
    Average SMSP Active Cycles       cycle      4920.85
    Total SMSP Elapsed Cycles        cycle      1653832
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       799.62
    Elapsed Cycles                cycle         7583
    Memory Throughput                 %        47.62
    DRAM Throughput                   %        47.62
    Duration                         us         9.44
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        26.80
    SM Active Cycles              cycle      5194.07
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.28
    Achieved Active Warps Per SM           warp        33.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      5194.07
    Total L1 Elapsed Cycles          cycle       409662
    Average L2 Active Cycles         cycle      4520.42
    Total L2 Elapsed Cycles          cycle       185112
    Average SM Active Cycles         cycle      5194.07
    Total SM Elapsed Cycles          cycle       409662
    Average SMSP Active Cycles       cycle      5029.84
    Total SMSP Elapsed Cycles        cycle      1638648
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       797.38
    Elapsed Cycles                cycle         7505
    Memory Throughput                 %        48.46
    DRAM Throughput                   %        48.46
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        22.81
    L2 Cache Throughput               %        27.07
    SM Active Cycles              cycle      5045.07
    Compute (SM) Throughput           %        15.22
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.12
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5045.07
    Total L1 Elapsed Cycles          cycle       430658
    Average L2 Active Cycles         cycle      4557.92
    Total L2 Elapsed Cycles          cycle       183336
    Average SM Active Cycles         cycle      5045.07
    Total SM Elapsed Cycles          cycle       430658
    Average SMSP Active Cycles       cycle      5103.96
    Total SMSP Elapsed Cycles        cycle      1722632
    -------------------------- ----------- ------------

  void mma_A_Bt<16>(const float *, const float *, float *, int, int, int, float) (512, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       831.63
    Elapsed Cycles                cycle      5184896
    Memory Throughput                 %        82.64
    DRAM Throughput                   %        16.25
    Duration                         ms         6.21
    L1/TEX Cache Throughput           %        82.69
    L2 Cache Throughput               %        14.40
    SM Active Cycles              cycle   5160050.09
    Compute (SM) Throughput           %        82.64
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 262144
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread        67108864
    Uses Green Context                                             0
    Waves Per SM                                              753.29
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.69
    Achieved Active Warps Per SM           warp        46.41
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   6301466.67
    Total DRAM Elapsed Cycles        cycle    232599552
    Average L1 Active Cycles         cycle   5160050.09
    Total L1 Elapsed Cycles          cycle    299453048
    Average L2 Active Cycles         cycle   4610183.04
    Total L2 Elapsed Cycles          cycle    125446224
    Average SM Active Cycles         cycle   5160050.09
    Total SM Elapsed Cycles          cycle    299453048
    Average SMSP Active Cycles       cycle   5160074.48
    Total SMSP Elapsed Cycles        cycle   1197812192
    -------------------------- ----------- ------------

  void softmax<16>(const float *, float *, int) (8192, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       845.54
    Elapsed Cycles                cycle      1864543
    Memory Throughput                 %        89.98
    DRAM Throughput                   %        89.98
    Duration                         ms         2.20
    L1/TEX Cache Throughput           %        40.75
    L2 Cache Throughput               %        49.31
    SM Active Cycles              cycle   1866435.41
    Compute (SM) Throughput           %        22.34
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              39
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            1.02
    # SMs                                         SM              58
    Threads                                   thread         2097152
    Uses Green Context                                             0
    Waves Per SM                                               23.54
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        96.09
    Achieved Active Warps Per SM           warp        46.12
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle  12336101.33
    Total DRAM Elapsed Cycles        cycle     82262016
    Average L1 Active Cycles         cycle   1866435.41
    Total L1 Elapsed Cycles          cycle    108418288
    Average L2 Active Cycles         cycle   1871751.54
    Total L2 Elapsed Cycles          cycle     45139824
    Average SM Active Cycles         cycle   1866435.41
    Total SM Elapsed Cycles          cycle    108418288
    Average SMSP Active Cycles       cycle   1850870.43
    Total SMSP Elapsed Cycles        cycle    433673152
    -------------------------- ----------- ------------

  void mma_A_B<16>(const float *, const float *, float *, int, int, int, float) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       795.20
    Elapsed Cycles                cycle      4522061
    Memory Throughput                 %        89.75
    DRAM Throughput                   %        24.16
    Duration                         ms         5.68
    L1/TEX Cache Throughput           %        91.69
    L2 Cache Throughput               %        14.84
    SM Active Cycles              cycle   4417678.81
    Compute (SM) Throughput           %        89.75
    ----------------------- ----------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing workloads in the Compute Workload Analysis section.                                        

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              37
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            2.05
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           21
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.01
    Achieved Active Warps Per SM           warp        42.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle   8574757.33
    Total DRAM Elapsed Cycles        cycle    212943872
    Average L1 Active Cycles         cycle   4417678.81
    Total L1 Elapsed Cycles          cycle    261756656
    Average L2 Active Cycles         cycle   4635423.42
    Total L2 Elapsed Cycles          cycle    112528968
    Average SM Active Cycles         cycle   4417678.81
    Total SM Elapsed Cycles          cycle    261756656
    Average SMSP Active Cycles       cycle   4415158.17
    Total SMSP Elapsed Cycles        cycle   1047026624
    -------------------------- ----------- ------------

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       801.12
    Elapsed Cycles                cycle         7161
    Memory Throughput                 %        43.84
    DRAM Throughput                   %        43.84
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        25.39
    L2 Cache Throughput               %        24.01
    SM Active Cycles              cycle      4449.43
    Compute (SM) Throughput           %        17.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.98
    Achieved Active Warps Per SM           warp        35.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24018.67
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4449.43
    Total L1 Elapsed Cycles          cycle       381064
    Average L2 Active Cycles         cycle      4116.54
    Total L2 Elapsed Cycles          cycle       174408
    Average SM Active Cycles         cycle      4449.43
    Total SM Elapsed Cycles          cycle       381064
    Average SMSP Active Cycles       cycle      4507.52
    Total SMSP Elapsed Cycles        cycle      1524256
    -------------------------- ----------- ------------

