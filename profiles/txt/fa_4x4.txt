[55583] profile_fa_4x4@127.0.0.1
  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.61
    Elapsed Cycles                cycle         6919
    Memory Throughput                 %        47.63
    DRAM Throughput                   %        47.63
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.53
    L2 Cache Throughput               %        28.81
    SM Active Cycles              cycle      4771.84
    Compute (SM) Throughput           %        16.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.76
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.76
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.12%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.00
    Mem Busy                               %        19.46
    Max Bandwidth                          %        47.63
    L1/TEX Hit Rate                        %         8.17
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.36
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.33
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.67
    Active Warps Per Scheduler          warp         8.84
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.84 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.24
    Warp Cycles Per Executed Instruction           cycle        55.13
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.37%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.5% of the total average of 48.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.52
    Issued Instructions                             inst       196625
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.83
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.17%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25850.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4771.84
    Total L1 Elapsed Cycles          cycle       400482
    Average L2 Active Cycles         cycle         4317
    Total L2 Elapsed Cycles          cycle       172032
    Average SM Active Cycles         cycle      4771.84
    Total SM Elapsed Cycles          cycle       400482
    Average SMSP Active Cycles       cycle      4623.69
    Total SMSP Elapsed Cycles        cycle      1601928
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.40
    Elapsed Cycles                cycle         6938
    Memory Throughput                 %        47.43
    DRAM Throughput                   %        47.43
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.48
    L2 Cache Throughput               %        28.77
    SM Active Cycles              cycle      4774.40
    Compute (SM) Throughput           %        16.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.75
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.75
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.13%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.35
    Mem Busy                               %        19.42
    Max Bandwidth                          %        47.43
    L1/TEX Hit Rate                        %         6.72
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.04
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.96
    Active Warps Per Scheduler          warp         8.63
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.63 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.85
    Warp Cycles Per Executed Instruction           cycle        54.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.57%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.8% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.62
    Issued Instructions                             inst       196649
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.71
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25824
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4774.40
    Total L1 Elapsed Cycles          cycle       401314
    Average L2 Active Cycles         cycle      4246.67
    Total L2 Elapsed Cycles          cycle       172416
    Average SM Active Cycles         cycle      4774.40
    Total SM Elapsed Cycles          cycle       401314
    Average SMSP Active Cycles       cycle      4698.83
    Total SMSP Elapsed Cycles        cycle      1605256
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.48
    Elapsed Cycles                cycle         6981
    Memory Throughput                 %        47.24
    DRAM Throughput                   %        47.24
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        24.35
    L2 Cache Throughput               %        28.67
    SM Active Cycles              cycle      4774.05
    Compute (SM) Throughput           %        16.25
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.76
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.76
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.12%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       139.71
    Mem Busy                               %        19.33
    Max Bandwidth                          %        47.24
    L1/TEX Hit Rate                        %         7.14
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.25
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.20
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.80
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.76%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.85
    Warp Cycles Per Executed Instruction           cycle        54.73
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.76%                                                                                          
          On average, each warp of this workload spends 30.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.2% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.10
    Issued Instructions                             inst       196760
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.03%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25800
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4774.05
    Total L1 Elapsed Cycles          cycle       403314
    Average L2 Active Cycles         cycle      4193.46
    Total L2 Elapsed Cycles          cycle       173208
    Average SM Active Cycles         cycle      4774.05
    Total SM Elapsed Cycles          cycle       403314
    Average SMSP Active Cycles       cycle      4658.88
    Total SMSP Elapsed Cycles        cycle      1613256
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       799.54
    Elapsed Cycles                cycle      7377029
    Memory Throughput                 %        45.92
    DRAM Throughput                   %         0.14
    Duration                         ms         9.12
    L1/TEX Cache Throughput           %        76.82
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4356890.86
    Compute (SM) Throughput           %        45.92
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.45
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.45
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       414.08
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.92
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.50
    Mem Pipes Busy                         %        45.92
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680072 bank conflicts,  which represent   
          15.30% of the overall 95969288 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351117 bank conflicts,  which represent   
          68.56% of the overall 26764722 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.44
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.56
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.57
    Issued Instructions                             inst    509995756
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78640
    Total DRAM Elapsed Cycles        cycle    341559296
    Average L1 Active Cycles         cycle   4356890.86
    Total L1 Elapsed Cycles          cycle    422741730
    Average L2 Active Cycles         cycle    346360.79
    Total L2 Elapsed Cycles          cycle    180494760
    Average SM Active Cycles         cycle   4356890.86
    Total SM Elapsed Cycles          cycle    422741730
    Average SMSP Active Cycles       cycle   4357793.72
    Total SMSP Elapsed Cycles        cycle   1690966920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.93% above the average, while the minimum instance value is 8.10% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.88% above the average, while the minimum instance value is 8.08% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.10% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.33
    Elapsed Cycles                cycle         6801
    Memory Throughput                 %        45.08
    DRAM Throughput                   %        45.08
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.64
    L2 Cache Throughput               %        29.03
    SM Active Cycles              cycle      4585.34
    Compute (SM) Throughput           %        16.67
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.66
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.76%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.36
    Mem Busy                               %        20.15
    Max Bandwidth                          %        45.08
    L1/TEX Hit Rate                        %         7.26
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.67
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.31
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.69
    Active Warps Per Scheduler          warp         8.86
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.86 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.36
    Warp Cycles Per Executed Instruction           cycle        52.81
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.92%                                                                                          
          On average, each warp of this workload spends 27.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 56.5% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.78
    Issued Instructions                             inst       187868
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.20
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.8%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4585.34
    Total L1 Elapsed Cycles          cycle       393050
    Average L2 Active Cycles         cycle      4149.04
    Total L2 Elapsed Cycles          cycle       168744
    Average SM Active Cycles         cycle      4585.34
    Total SM Elapsed Cycles          cycle       393050
    Average SMSP Active Cycles       cycle      4422.09
    Total SMSP Elapsed Cycles        cycle      1572200
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       794.67
    Elapsed Cycles                cycle         7023
    Memory Throughput                 %        48.01
    DRAM Throughput                   %        48.01
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.39
    L2 Cache Throughput               %        29.05
    SM Active Cycles              cycle      4778.40
    Compute (SM) Throughput           %        16.28
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.74
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.13%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.27
    Mem Busy                               %        19.61
    Max Bandwidth                          %        48.01
    L1/TEX Hit Rate                        %         6.91
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.28
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.86
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.14
    Active Warps Per Scheduler          warp         8.86
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.99%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.86 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.59
    Warp Cycles Per Executed Instruction           cycle        56.70
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.99%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.1% of the total average of 49.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.82
    Issued Instructions                             inst       196695
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.00
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27%                                                                                             
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25893.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4778.40
    Total L1 Elapsed Cycles          cycle       402658
    Average L2 Active Cycles         cycle      4208.75
    Total L2 Elapsed Cycles          cycle       170712
    Average SM Active Cycles         cycle      4778.40
    Total SM Elapsed Cycles          cycle       402658
    Average SMSP Active Cycles       cycle      4745.90
    Total SMSP Elapsed Cycles        cycle      1610632
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       793.26
    Elapsed Cycles                cycle         6925
    Memory Throughput                 %        48.44
    DRAM Throughput                   %        48.44
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.68
    L2 Cache Throughput               %        29.36
    SM Active Cycles              cycle      4697.10
    Compute (SM) Throughput           %        16.49
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.05
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.05
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.98%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.75
    Mem Busy                               %        19.83
    Max Bandwidth                          %        48.44
    L1/TEX Hit Rate                        %         7.82
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.49
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.96
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.04
    Active Warps Per Scheduler          warp         8.74
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.74 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.68
    Warp Cycles Per Executed Instruction           cycle        55.67
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.56%                                                                                          
          On average, each warp of this workload spends 30.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.3% of the total average of 48.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.96
    Issued Instructions                             inst       196727
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.76
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.24%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25874.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4697.10
    Total L1 Elapsed Cycles          cycle       397520
    Average L2 Active Cycles         cycle      4233.83
    Total L2 Elapsed Cycles          cycle       168792
    Average SM Active Cycles         cycle      4697.10
    Total SM Elapsed Cycles          cycle       397520
    Average SMSP Active Cycles       cycle      4721.61
    Total SMSP Elapsed Cycles        cycle      1590080
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.19
    Elapsed Cycles                cycle         6932
    Memory Throughput                 %        47.30
    DRAM Throughput                   %        47.30
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.43
    L2 Cache Throughput               %        28.74
    SM Active Cycles              cycle      4747.64
    Compute (SM) Throughput           %        16.31
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.86
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.86
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.08%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.39
    Mem Busy                               %        19.41
    Max Bandwidth                          %        47.30
    L1/TEX Hit Rate                        %         7.62
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.31
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.54
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.46
    Active Warps Per Scheduler          warp         8.74
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.7%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.74 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.16
    Warp Cycles Per Executed Instruction           cycle        53.92
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.7%                                                                                           
          On average, each warp of this workload spends 29.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.9% of the total average of 47.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.73
    Issued Instructions                             inst       196673
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.75
    Achieved Active Warps Per SM           warp        35.40
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25832
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4747.64
    Total L1 Elapsed Cycles          cycle       401708
    Average L2 Active Cycles         cycle      4169.04
    Total L2 Elapsed Cycles          cycle       172464
    Average SM Active Cycles         cycle      4747.64
    Total SM Elapsed Cycles          cycle       401708
    Average SMSP Active Cycles       cycle      4571.94
    Total SMSP Elapsed Cycles        cycle      1606832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       799.65
    Elapsed Cycles                cycle      7374245
    Memory Throughput                 %        45.94
    DRAM Throughput                   %         0.14
    Duration                         ms         9.11
    L1/TEX Cache Throughput           %        76.80
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4358128.03
    Compute (SM) Throughput           %        45.94
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.21
    Issue Slots Busy               %        50.44
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.44
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       414.86
    Mem Busy                               %        30.81
    Max Bandwidth                          %        45.94
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.85
    Mem Pipes Busy                         %        45.94
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.66%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351341 bank conflicts,  which represent   
          68.56% of the overall 26764993 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.44
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.56
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.53%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.57
    Issued Instructions                             inst    509995756
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78752
    Total DRAM Elapsed Cycles        cycle    341400576
    Average L1 Active Cycles         cycle   4358128.03
    Total L1 Elapsed Cycles          cycle    422596662
    Average L2 Active Cycles         cycle    345782.17
    Total L2 Elapsed Cycles          cycle    180411096
    Average SM Active Cycles         cycle   4358128.03
    Total SM Elapsed Cycles          cycle    422596662
    Average SMSP Active Cycles       cycle   4358346.05
    Total SMSP Elapsed Cycles        cycle   1690386648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.88% above the average, while the minimum instance value is 8.09% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.91% above the average, while the minimum instance value is 8.11% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.88% above the average, while the minimum instance value is 8.09% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.1%                                                                                           
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.32
    Elapsed Cycles                cycle         6749
    Memory Throughput                 %        45.59
    DRAM Throughput                   %        45.59
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.69
    L2 Cache Throughput               %        29.23
    SM Active Cycles              cycle      4576.71
    Compute (SM) Throughput           %        16.76
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.68
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.74%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.93
    Mem Busy                               %        20.28
    Max Bandwidth                          %        45.59
    L1/TEX Hit Rate                        %         7.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.76
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.91
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.09
    Active Warps Per Scheduler          warp         8.61
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.61 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.11
    Warp Cycles Per Executed Instruction           cycle        52.51
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.41%                                                                                          
          On average, each warp of this workload spends 28.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.8% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.31
    Issued Instructions                             inst       187760
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.22
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24197.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4576.71
    Total L1 Elapsed Cycles          cycle       391082
    Average L2 Active Cycles         cycle      4161.25
    Total L2 Elapsed Cycles          cycle       167808
    Average SM Active Cycles         cycle      4576.71
    Total SM Elapsed Cycles          cycle       391082
    Average SMSP Active Cycles       cycle      4519.81
    Total SMSP Elapsed Cycles        cycle      1564328
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.27
    Elapsed Cycles                cycle         6921
    Memory Throughput                 %        47.88
    DRAM Throughput                   %        47.88
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.48
    L2 Cache Throughput               %        28.80
    SM Active Cycles              cycle         4749
    Compute (SM) Throughput           %        16.37
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.82
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.82
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.08%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.75
    Mem Busy                               %        19.48
    Max Bandwidth                          %        47.88
    L1/TEX Hit Rate                        %         7.74
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.37
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.85
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.15
    Active Warps Per Scheduler          warp         8.68
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.12%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.68 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.64
    Warp Cycles Per Executed Instruction           cycle        55.51
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.12%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 48.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.22
    Issued Instructions                             inst       196323
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.56
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.44%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25986.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle         4749
    Total L1 Elapsed Cycles          cycle       400312
    Average L2 Active Cycles         cycle      4126.54
    Total L2 Elapsed Cycles          cycle       171864
    Average SM Active Cycles         cycle         4749
    Total SM Elapsed Cycles          cycle       400312
    Average SMSP Active Cycles       cycle      4740.25
    Total SMSP Elapsed Cycles        cycle      1601248
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.58
    Elapsed Cycles                cycle         6967
    Memory Throughput                 %        47.36
    DRAM Throughput                   %        47.36
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        24.26
    L2 Cache Throughput               %        28.55
    SM Active Cycles              cycle      4760.50
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.77
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.77
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.1%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s          140
    Mem Busy                               %        19.32
    Max Bandwidth                          %        47.36
    L1/TEX Hit Rate                        %         7.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.23
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.11
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.89
    Active Warps Per Scheduler          warp         8.55
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.64%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.55 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.24
    Warp Cycles Per Executed Instruction           cycle        53.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.64%                                                                                          
          On average, each warp of this workload spends 28.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 47.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.11
    Issued Instructions                             inst       196298
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.92
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.08%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25946.67
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4760.50
    Total L1 Elapsed Cycles          cycle       403784
    Average L2 Active Cycles         cycle      4155.46
    Total L2 Elapsed Cycles          cycle       173280
    Average SM Active Cycles         cycle      4760.50
    Total SM Elapsed Cycles          cycle       403784
    Average SMSP Active Cycles       cycle      4672.37
    Total SMSP Elapsed Cycles        cycle      1615136
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.76
    Elapsed Cycles                cycle         6870
    Memory Throughput                 %        48.03
    DRAM Throughput                   %        48.03
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.66
    L2 Cache Throughput               %        29.00
    SM Active Cycles              cycle      4677.90
    Compute (SM) Throughput           %        16.50
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.09
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.09
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.40
    Mem Busy                               %        19.63
    Max Bandwidth                          %        48.03
    L1/TEX Hit Rate                        %         7.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.50
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.79
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.21
    Active Warps Per Scheduler          warp         8.69
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.69 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.85
    Warp Cycles Per Executed Instruction           cycle        55.74
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.97%                                                                                          
          On average, each warp of this workload spends 29.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.6% of the total average of 48.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.22
    Issued Instructions                             inst       196322
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.48
    Achieved Active Warps Per SM           warp        36.23
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25821.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4677.90
    Total L1 Elapsed Cycles          cycle       397202
    Average L2 Active Cycles         cycle      4224.58
    Total L2 Elapsed Cycles          cycle       170544
    Average SM Active Cycles         cycle      4677.90
    Total SM Elapsed Cycles          cycle       397202
    Average SMSP Active Cycles       cycle      4757.09
    Total SMSP Elapsed Cycles        cycle      1588808
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.01
    Elapsed Cycles                cycle      7371708
    Memory Throughput                 %        45.96
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.81
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4357095.48
    Compute (SM) Throughput           %        45.96
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.21
    Issue Slots Busy               %        50.45
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.45
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       412.86
    Mem Busy                               %        30.83
    Max Bandwidth                          %        45.96
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.66
    Mem Pipes Busy                         %        45.96
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680068 bank conflicts,  which represent   
          15.30% of the overall 95969284 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351324 bank conflicts,  which represent   
          68.56% of the overall 26764952 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.45
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.55
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.54%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995749
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78290.67
    Total DRAM Elapsed Cycles        cycle    341039104
    Average L1 Active Cycles         cycle   4357095.48
    Total L1 Elapsed Cycles          cycle    422339038
    Average L2 Active Cycles         cycle       345775
    Total L2 Elapsed Cycles          cycle    180220224
    Average SM Active Cycles         cycle   4357095.48
    Total SM Elapsed Cycles          cycle    422339038
    Average SMSP Active Cycles       cycle   4356941.93
    Total SMSP Elapsed Cycles        cycle   1689356152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.89%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.93% above the average, while the minimum instance value is 8.12% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.89%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.12% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.89%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.12% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.1%                                                                                           
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.72
    Elapsed Cycles                cycle         6676
    Memory Throughput                 %        45.94
    DRAM Throughput                   %        45.94
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        24.87
    L2 Cache Throughput               %        29.57
    SM Active Cycles              cycle      4542.66
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.45
    Issue Slots Busy               %        17.82
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.82
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.67%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.79
    Mem Busy                               %        20.48
    Max Bandwidth                          %        45.94
    L1/TEX Hit Rate                        %         6.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.98
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.08
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.92
    Active Warps Per Scheduler          warp         8.55
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.55 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.30
    Warp Cycles Per Executed Instruction           cycle        51.65
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.06%                                                                                          
          On average, each warp of this workload spends 27.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.7% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.67
    Issued Instructions                             inst       187844
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.07
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23989.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4542.66
    Total L1 Elapsed Cycles          cycle       385960
    Average L2 Active Cycles         cycle      4136.08
    Total L2 Elapsed Cycles          cycle       165768
    Average SM Active Cycles         cycle      4542.66
    Total SM Elapsed Cycles          cycle       385960
    Average SMSP Active Cycles       cycle      4477.90
    Total SMSP Elapsed Cycles        cycle      1543840
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.64
    Elapsed Cycles                cycle         6879
    Memory Throughput                 %        48.29
    DRAM Throughput                   %        48.29
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.67
    L2 Cache Throughput               %        28.98
    SM Active Cycles              cycle      4712.59
    Compute (SM) Throughput           %        16.48
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.94
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.94
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.64
    Mem Busy                               %        19.60
    Max Bandwidth                          %        48.29
    L1/TEX Hit Rate                        %         6.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.48
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.92
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.08
    Active Warps Per Scheduler          warp         8.50
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.71%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.50 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.43
    Warp Cycles Per Executed Instruction           cycle        54.09
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.71%                                                                                          
          On average, each warp of this workload spends 29.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.1% of the total average of 47.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.56
    Issued Instructions                             inst       196171
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.53
    Achieved Active Warps Per SM           warp        35.78
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25960
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4712.59
    Total L1 Elapsed Cycles          cycle       397574
    Average L2 Active Cycles         cycle      4223.50
    Total L2 Elapsed Cycles          cycle       170808
    Average SM Active Cycles         cycle      4712.59
    Total SM Elapsed Cycles          cycle       397574
    Average SMSP Active Cycles       cycle      4717.90
    Total SMSP Elapsed Cycles        cycle      1590296
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       794.31
    Elapsed Cycles                cycle         6894
    Memory Throughput                 %        48.70
    DRAM Throughput                   %        48.70
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.80
    L2 Cache Throughput               %        29.56
    SM Active Cycles              cycle      4662.81
    Compute (SM) Throughput           %        16.59
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.14
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.91%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.21
    Mem Busy                               %        20.01
    Max Bandwidth                          %        48.70
    L1/TEX Hit Rate                        %         7.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.59
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.05
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.95
    Active Warps Per Scheduler          warp         8.54
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.3%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.54 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.33
    Warp Cycles Per Executed Instruction           cycle        54.00
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.3%                                                                                           
          On average, each warp of this workload spends 29.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.03
    Issued Instructions                             inst       196280
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.24
    Achieved Active Warps Per SM           warp        36.60
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.76%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25765.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4662.81
    Total L1 Elapsed Cycles          cycle       395090
    Average L2 Active Cycles         cycle      4276.38
    Total L2 Elapsed Cycles          cycle       167352
    Average SM Active Cycles         cycle      4662.81
    Total SM Elapsed Cycles          cycle       395090
    Average SMSP Active Cycles       cycle      4688.26
    Total SMSP Elapsed Cycles        cycle      1580360
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.77
    Elapsed Cycles                cycle         6855
    Memory Throughput                 %        48.16
    DRAM Throughput                   %        48.16
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.66
    L2 Cache Throughput               %        29.04
    SM Active Cycles              cycle         4679
    Compute (SM) Throughput           %        16.50
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.07
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.07
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.27
    Mem Busy                               %        19.64
    Max Bandwidth                          %        48.16
    L1/TEX Hit Rate                        %         7.64
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.50
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.58
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.42
    Active Warps Per Scheduler          warp         8.81
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.84%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.81 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.39
    Warp Cycles Per Executed Instruction           cycle        54.04
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.84%                                                                                          
          On average, each warp of this workload spends 30.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 64.2% of the total average of 47.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.59
    Issued Instructions                             inst       196177
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.64
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25893.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle         4679
    Total L1 Elapsed Cycles          cycle       397128
    Average L2 Active Cycles         cycle      4243.29
    Total L2 Elapsed Cycles          cycle       170496
    Average SM Active Cycles         cycle         4679
    Total SM Elapsed Cycles          cycle       397128
    Average SMSP Active Cycles       cycle      4549.92
    Total SMSP Elapsed Cycles        cycle      1588512
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.42
    Elapsed Cycles                cycle      7374612
    Memory Throughput                 %        45.94
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.82
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4356744.88
    Compute (SM) Throughput           %        45.94
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.21
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.46
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.74
    Mem Busy                               %        30.82
    Max Bandwidth                          %        45.94
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.60
    Mem Pipes Busy                         %        45.94
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351325 bank conflicts,  which represent   
          68.56% of the overall 26764955 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.53%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.53
    Issued Instructions                             inst    509995747
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78458.67
    Total DRAM Elapsed Cycles        cycle    341045248
    Average L1 Active Cycles         cycle   4356744.88
    Total L1 Elapsed Cycles          cycle    422561178
    Average L2 Active Cycles         cycle    347230.62
    Total L2 Elapsed Cycles          cycle    180222960
    Average SM Active Cycles         cycle   4356744.88
    Total SM Elapsed Cycles          cycle    422561178
    Average SMSP Active Cycles       cycle   4356226.28
    Total SMSP Elapsed Cycles        cycle   1690244712
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.89% above the average, while the minimum instance value is 8.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.14% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.89% above the average, while the minimum instance value is 8.13% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       795.86
    Elapsed Cycles                cycle         6800
    Memory Throughput                 %        46.47
    DRAM Throughput                   %        46.47
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        24.81
    L2 Cache Throughput               %        29.63
    SM Active Cycles              cycle      4554.41
    Compute (SM) Throughput           %        16.81
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.77
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.77
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.7%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       137.88
    Mem Busy                               %        20.65
    Max Bandwidth                          %        46.47
    L1/TEX Hit Rate                        %         6.91
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.81
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.62
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.38
    Active Warps Per Scheduler          warp         8.37
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.37 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.47
    Warp Cycles Per Executed Instruction           cycle        51.82
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.53%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.0% of the total average of 47.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.46
    Issued Instructions                             inst       187795
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.17
    Achieved Active Warps Per SM           warp        34.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.83%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24266.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4554.41
    Total L1 Elapsed Cycles          cycle       389962
    Average L2 Active Cycles         cycle         4087
    Total L2 Elapsed Cycles          cycle       165360
    Average SM Active Cycles         cycle      4554.41
    Total SM Elapsed Cycles          cycle       389962
    Average SMSP Active Cycles       cycle      4592.97
    Total SMSP Elapsed Cycles        cycle      1559848
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.60
    Elapsed Cycles                cycle         6881
    Memory Throughput                 %        47.58
    DRAM Throughput                   %        47.58
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.56
    L2 Cache Throughput               %        28.92
    SM Active Cycles              cycle      4708.19
    Compute (SM) Throughput           %        16.42
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.97
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.97
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91%                                                                                       
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.37
    Mem Busy                               %        19.55
    Max Bandwidth                          %        47.58
    L1/TEX Hit Rate                        %         7.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.42
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.11
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.89
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.12
    Warp Cycles Per Executed Instruction           cycle        54.91
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.42%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.13
    Issued Instructions                             inst       196302
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.11%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25824
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4708.19
    Total L1 Elapsed Cycles          cycle       399002
    Average L2 Active Cycles         cycle      4314.42
    Total L2 Elapsed Cycles          cycle       171240
    Average SM Active Cycles         cycle      4708.19
    Total SM Elapsed Cycles          cycle       399002
    Average SMSP Active Cycles       cycle      4672.40
    Total SMSP Elapsed Cycles        cycle      1596008
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz          784
    Elapsed Cycles                cycle         6923
    Memory Throughput                 %        48.17
    DRAM Throughput                   %        48.17
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.49
    L2 Cache Throughput               %        28.81
    SM Active Cycles              cycle      4727.19
    Compute (SM) Throughput           %        16.38
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.90
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.90
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.59
    Mem Busy                               %        19.53
    Max Bandwidth                          %        48.17
    L1/TEX Hit Rate                        %         7.02
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.81
    Mem Pipes Busy                         %        16.38
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.92
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.08
    Active Warps Per Scheduler          warp         8.75
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.83%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.75 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.83
    Warp Cycles Per Executed Instruction           cycle        55.72
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.83%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 48.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.22
    Issued Instructions                             inst       196322
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.87
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.13%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     26141.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4727.19
    Total L1 Elapsed Cycles          cycle       400154
    Average L2 Active Cycles         cycle      4179.75
    Total L2 Elapsed Cycles          cycle       171792
    Average SM Active Cycles         cycle      4727.19
    Total SM Elapsed Cycles          cycle       400154
    Average SMSP Active Cycles       cycle      4721.85
    Total SMSP Elapsed Cycles        cycle      1600616
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       795.18
    Elapsed Cycles                cycle         7083
    Memory Throughput                 %        47.76
    DRAM Throughput                   %        47.76
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.15
    L2 Cache Throughput               %        28.90
    SM Active Cycles              cycle      4737.78
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.86
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.86
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.93
    Mem Busy                               %        19.55
    Max Bandwidth                          %        47.76
    L1/TEX Hit Rate                        %         7.45
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.15
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.88
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.12
    Active Warps Per Scheduler          warp         8.96
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.24%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.96 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        50.10
    Warp Cycles Per Executed Instruction           cycle        57.17
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.24%                                                                                          
          On average, each warp of this workload spends 31.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 50.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.23
    Issued Instructions                             inst       196325
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.14
    Achieved Active Warps Per SM           warp        35.59
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25837.33
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4737.78
    Total L1 Elapsed Cycles          cycle       405846
    Average L2 Active Cycles         cycle      4272.92
    Total L2 Elapsed Cycles          cycle       171288
    Average SM Active Cycles         cycle      4737.78
    Total SM Elapsed Cycles          cycle       405846
    Average SMSP Active Cycles       cycle      4731.97
    Total SMSP Elapsed Cycles        cycle      1623384
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.53
    Elapsed Cycles                cycle      7376108
    Memory Throughput                 %        45.93
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.81
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4357303.40
    Compute (SM) Throughput           %        45.93
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.21
    Issue Slots Busy               %        50.45
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.45
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       431.80
    Mem Busy                               %        30.81
    Max Bandwidth                          %        45.93
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.50
    Mem Pipes Busy                         %        45.93
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680065 bank conflicts,  which represent   
          15.30% of the overall 95969281 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351343 bank conflicts,  which represent   
          68.56% of the overall 26764949 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.53%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995750
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     81890.67
    Total DRAM Elapsed Cycles        cycle    341074944
    Average L1 Active Cycles         cycle   4357303.40
    Total L1 Elapsed Cycles          cycle    422652354
    Average L2 Active Cycles         cycle    353365.33
    Total L2 Elapsed Cycles          cycle    180239112
    Average SM Active Cycles         cycle   4357303.40
    Total SM Elapsed Cycles          cycle    422652354
    Average SMSP Active Cycles       cycle   4356421.73
    Total SMSP Elapsed Cycles        cycle   1690609416
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.88% above the average, while the minimum instance value is 8.13% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.89% above the average, while the minimum instance value is 8.15% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.88% above the average, while the minimum instance value is 8.13% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       790.79
    Elapsed Cycles                cycle         6858
    Memory Throughput                 %        45.39
    DRAM Throughput                   %        45.39
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.46
    L2 Cache Throughput               %        29.34
    SM Active Cycles              cycle      4618.59
    Compute (SM) Throughput           %        16.66
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.53
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.53
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.83%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.40
    Mem Busy                               %        20.29
    Max Bandwidth                          %        45.39
    L1/TEX Hit Rate                        %         8.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.66
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.94
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.06
    Active Warps Per Scheduler          warp         8.63
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.61%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.63 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.11
    Warp Cycles Per Executed Instruction           cycle        52.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.61%                                                                                          
          On average, each warp of this workload spends 28.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.9% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.74
    Issued Instructions                             inst       187859
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.42
    Achieved Active Warps Per SM           warp        34.28
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.58%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24013.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4618.59
    Total L1 Elapsed Cycles          cycle       393350
    Average L2 Active Cycles         cycle      4019.17
    Total L2 Elapsed Cycles          cycle       167064
    Average SM Active Cycles         cycle      4618.59
    Total SM Elapsed Cycles          cycle       393350
    Average SMSP Active Cycles       cycle      4513.28
    Total SMSP Elapsed Cycles        cycle      1573400
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.15
    Elapsed Cycles                cycle         6827
    Memory Throughput                 %        48.35
    DRAM Throughput                   %        48.35
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.75
    L2 Cache Throughput               %        29.11
    SM Active Cycles              cycle      4670.26
    Compute (SM) Throughput           %        16.56
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.11
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.11
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.93%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.35
    Mem Busy                               %        19.70
    Max Bandwidth                          %        48.35
    L1/TEX Hit Rate                        %         7.23
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.56
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.31
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.69
    Active Warps Per Scheduler          warp         8.85
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.85 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.35
    Warp Cycles Per Executed Instruction           cycle        55.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.65%                                                                                          
          On average, each warp of this workload spends 30.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.4% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.79
    Issued Instructions                             inst       196223
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.25
    Achieved Active Warps Per SM           warp        36.12
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25994.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4670.26
    Total L1 Elapsed Cycles          cycle       395862
    Average L2 Active Cycles         cycle      4159.21
    Total L2 Elapsed Cycles          cycle       169944
    Average SM Active Cycles         cycle      4670.26
    Total SM Elapsed Cycles          cycle       395862
    Average SMSP Active Cycles       cycle      4619.81
    Total SMSP Elapsed Cycles        cycle      1583448
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.90
    Elapsed Cycles                cycle         6926
    Memory Throughput                 %        48.34
    DRAM Throughput                   %        48.34
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.51
    L2 Cache Throughput               %        28.81
    SM Active Cycles              cycle      4727.52
    Compute (SM) Throughput           %        16.40
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.89
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.73
    Mem Busy                               %        19.52
    Max Bandwidth                          %        48.34
    L1/TEX Hit Rate                        %         7.92
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.40
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.11
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.89
    Active Warps Per Scheduler          warp         8.93
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.93 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.29
    Warp Cycles Per Executed Instruction           cycle        56.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.66%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.7% of the total average of 49.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.80
    Issued Instructions                             inst       196225
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.36
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.64%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        26072
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4727.52
    Total L1 Elapsed Cycles          cycle       399674
    Average L2 Active Cycles         cycle         4184
    Total L2 Elapsed Cycles          cycle       171552
    Average SM Active Cycles         cycle      4727.52
    Total SM Elapsed Cycles          cycle       399674
    Average SMSP Active Cycles       cycle      4669.25
    Total SMSP Elapsed Cycles        cycle      1598696
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       834.08
    Elapsed Cycles                cycle         7215
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        29.16
    SM Active Cycles              cycle      4920.21
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.60
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.19
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.19
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.39%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.75
    Mem Busy                               %        19.73
    Max Bandwidth                          %        48.86
    L1/TEX Hit Rate                        %         7.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.86
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.32
    Issued Warp Per Scheduler                        0.17
    No Eligible                            %        82.68
    Active Warps Per Scheduler          warp         8.53
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.14%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.53 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.28
    Warp Cycles Per Executed Instruction           cycle        56.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.14%                                                                                          
          On average, each warp of this workload spends 31.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.4% of the total average of 49.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.94
    Issued Instructions                             inst       196257
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.59
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.95%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25765.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4920.21
    Total L1 Elapsed Cycles          cycle       413312
    Average L2 Active Cycles         cycle      4221.83
    Total L2 Elapsed Cycles          cycle       169656
    Average SM Active Cycles         cycle      4920.21
    Total SM Elapsed Cycles          cycle       413312
    Average SMSP Active Cycles       cycle      4885.21
    Total SMSP Elapsed Cycles        cycle      1653248
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.76
    Elapsed Cycles                cycle      7379089
    Memory Throughput                 %        45.92
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.81
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4357124.22
    Compute (SM) Throughput           %        45.92
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.45
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.45
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.91
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.92
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.66
    Mem Pipes Busy                         %        45.92
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680073 bank conflicts,  which represent   
          15.30% of the overall 95969289 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351457 bank conflicts,  which represent   
          68.56% of the overall 26765081 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.44
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.56
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.53
    Issued Instructions                             inst    509995748
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78493.33
    Total DRAM Elapsed Cycles        cycle    341056512
    Average L1 Active Cycles         cycle   4357124.22
    Total L1 Elapsed Cycles          cycle    422753298
    Average L2 Active Cycles         cycle    349823.50
    Total L2 Elapsed Cycles          cycle    180229104
    Average SM Active Cycles         cycle   4357124.22
    Total SM Elapsed Cycles          cycle    422753298
    Average SMSP Active Cycles       cycle   4358220.31
    Total SMSP Elapsed Cycles        cycle   1691013192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.92% above the average, while the minimum instance value is 8.14% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.85% above the average, while the minimum instance value is 8.14% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.14% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.15
    Elapsed Cycles                cycle         6789
    Memory Throughput                 %        45.59
    DRAM Throughput                   %        45.59
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.70
    L2 Cache Throughput               %        29.09
    SM Active Cycles              cycle      4573.81
    Compute (SM) Throughput           %        16.72
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.70
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.74%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.43
    Mem Busy                               %        20.24
    Max Bandwidth                          %        45.59
    L1/TEX Hit Rate                        %         7.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.72
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.82
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.18
    Active Warps Per Scheduler          warp         8.35
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.35 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.86
    Warp Cycles Per Executed Instruction           cycle        51.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.41%                                                                                          
          On average, each warp of this workload spends 28.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.3% of the total average of 46.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.42
    Issued Instructions                             inst       187785
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.78
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24197.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4573.81
    Total L1 Elapsed Cycles          cycle       391956
    Average L2 Active Cycles         cycle      4048.96
    Total L2 Elapsed Cycles          cycle       168360
    Average SM Active Cycles         cycle      4573.81
    Total SM Elapsed Cycles          cycle       391956
    Average SMSP Active Cycles       cycle      4542.28
    Total SMSP Elapsed Cycles        cycle      1567824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.36
    Elapsed Cycles                cycle         6996
    Memory Throughput                 %        47.36
    DRAM Throughput                   %        47.36
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        24.28
    L2 Cache Throughput               %        28.56
    SM Active Cycles              cycle      4754.71
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.80
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.80
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.07
    Mem Busy                               %        19.32
    Max Bandwidth                          %        47.36
    L1/TEX Hit Rate                        %         7.17
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.23
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.32
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.68
    Active Warps Per Scheduler          warp         8.80
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.64%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.80 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.06
    Warp Cycles Per Executed Instruction           cycle        54.85
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.64%                                                                                          
          On average, each warp of this workload spends 30.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 64.4% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.25
    Issued Instructions                             inst       196331
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.49
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.51%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25866.67
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4754.71
    Total L1 Elapsed Cycles          cycle       403764
    Average L2 Active Cycles         cycle      4314.29
    Total L2 Elapsed Cycles          cycle       173256
    Average SM Active Cycles         cycle      4754.71
    Total SM Elapsed Cycles          cycle       403764
    Average SMSP Active Cycles       cycle      4620.13
    Total SMSP Elapsed Cycles        cycle      1615056
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       826.41
    Elapsed Cycles                cycle         7126
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.01
    L2 Cache Throughput               %        29.50
    SM Active Cycles              cycle      4873.57
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.36
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.36
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.31%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s          146
    Mem Busy                               %        19.97
    Max Bandwidth                          %        49.41
    L1/TEX Hit Rate                        %         7.58
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.06
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.90
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.10
    Active Warps Per Scheduler          warp         8.77
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.77 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.98
    Warp Cycles Per Executed Instruction           cycle        55.89
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.59%                                                                                          
          On average, each warp of this workload spends 31.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.5% of the total average of 49.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.22
    Issued Instructions                             inst       196324
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.06
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25890.67
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4873.57
    Total L1 Elapsed Cycles          cycle       407974
    Average L2 Active Cycles         cycle      4263.29
    Total L2 Elapsed Cycles          cycle       167688
    Average SM Active Cycles         cycle      4873.57
    Total SM Elapsed Cycles          cycle       407974
    Average SMSP Active Cycles       cycle      4727.15
    Total SMSP Elapsed Cycles        cycle      1631896
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.49
    Elapsed Cycles                cycle         6873
    Memory Throughput                 %        47.77
    DRAM Throughput                   %        47.77
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.60
    L2 Cache Throughput               %        28.92
    SM Active Cycles              cycle      4716.93
    Compute (SM) Throughput           %        16.45
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.93
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.02%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.49
    Mem Busy                               %        19.56
    Max Bandwidth                          %        47.77
    L1/TEX Hit Rate                        %         7.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.37
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.63
    Active Warps Per Scheduler          warp         8.79
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.23%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.79 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.83
    Warp Cycles Per Executed Instruction           cycle        54.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.23%                                                                                          
          On average, each warp of this workload spends 29.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.3% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.96
    Issued Instructions                             inst       196263
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.88
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.12%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25845.33
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4716.93
    Total L1 Elapsed Cycles          cycle       398436
    Average L2 Active Cycles         cycle      4173.96
    Total L2 Elapsed Cycles          cycle       171120
    Average SM Active Cycles         cycle      4716.93
    Total SM Elapsed Cycles          cycle       398436
    Average SMSP Active Cycles       cycle      4605.33
    Total SMSP Elapsed Cycles        cycle      1593744
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       800.90
    Elapsed Cycles                cycle      7379438
    Memory Throughput                 %        45.93
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.85
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4354844.79
    Compute (SM) Throughput           %        45.93
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.21
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.48
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       412.58
    Mem Busy                               %        30.81
    Max Bandwidth                          %        45.93
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.69
    Mem Pipes Busy                         %        45.93
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680068 bank conflicts,  which represent   
          15.30% of the overall 95969284 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.7%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351463 bank conflicts,  which represent   
          68.56% of the overall 26765080 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.14
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.29%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78210.67
    Total DRAM Elapsed Cycles        cycle    340923392
    Average L1 Active Cycles         cycle   4354844.79
    Total L1 Elapsed Cycles          cycle    422662810
    Average L2 Active Cycles         cycle    349184.50
    Total L2 Elapsed Cycles          cycle    180158568
    Average SM Active Cycles         cycle   4354844.79
    Total SM Elapsed Cycles          cycle    422662810
    Average SMSP Active Cycles       cycle   4356815.34
    Total SMSP Elapsed Cycles        cycle   1690651240
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.93% above the average, while the minimum instance value is 8.17% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.88%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.16% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.17% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       792.02
    Elapsed Cycles                cycle         6840
    Memory Throughput                 %        45.49
    DRAM Throughput                   %        45.49
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        24.63
    L2 Cache Throughput               %        29.43
    SM Active Cycles              cycle      4587.19
    Compute (SM) Throughput           %        16.70
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.66
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.76%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.77
    Mem Busy                               %        20.38
    Max Bandwidth                          %        45.49
    L1/TEX Hit Rate                        %         7.89
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.70
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.23
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.77
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.80
    Warp Cycles Per Executed Instruction           cycle        52.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.51%                                                                                          
          On average, each warp of this workload spends 28.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.3% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst          810
    Issued Instructions                             inst       187920
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.29
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.71%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23989.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4587.19
    Total L1 Elapsed Cycles          cycle       392492
    Average L2 Active Cycles         cycle      4140.75
    Total L2 Elapsed Cycles          cycle       166656
    Average SM Active Cycles         cycle      4587.19
    Total SM Elapsed Cycles          cycle       392492
    Average SMSP Active Cycles       cycle      4444.02
    Total SMSP Elapsed Cycles        cycle      1569968
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       791.08
    Elapsed Cycles                cycle         6962
    Memory Throughput                 %        48.25
    DRAM Throughput                   %        48.25
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.55
    L2 Cache Throughput               %        29.14
    SM Active Cycles              cycle      4715.98
    Compute (SM) Throughput           %        16.41
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.94
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.94
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.02%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.60
    Mem Busy                               %        19.72
    Max Bandwidth                          %        48.25
    L1/TEX Hit Rate                        %         7.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.41
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.15
    Issued Warp Per Scheduler                        0.17
    No Eligible                            %        82.85
    Active Warps Per Scheduler          warp         8.41
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.41 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.05
    Warp Cycles Per Executed Instruction           cycle        55.97
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.75%                                                                                          
          On average, each warp of this workload spends 32.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 67.0% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.04
    Issued Instructions                             inst       196282
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.93
    Achieved Active Warps Per SM           warp        35.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.07%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25858.67
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4715.98
    Total L1 Elapsed Cycles          cycle       399368
    Average L2 Active Cycles         cycle      4269.75
    Total L2 Elapsed Cycles          cycle       169800
    Average SM Active Cycles         cycle      4715.98
    Total SM Elapsed Cycles          cycle       399368
    Average SMSP Active Cycles       cycle      4933.60
    Total SMSP Elapsed Cycles        cycle      1597472
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.01
    Elapsed Cycles                cycle         6896
    Memory Throughput                 %        48.21
    DRAM Throughput                   %        48.21
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.63
    L2 Cache Throughput               %        28.91
    SM Active Cycles              cycle      4688.91
    Compute (SM) Throughput           %        16.46
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.04
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.04
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.96%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.84
    Mem Busy                               %        19.58
    Max Bandwidth                          %        48.21
    L1/TEX Hit Rate                        %         7.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.46
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.34
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.66
    Active Warps Per Scheduler          warp         8.68
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.79%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.68 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.33
    Warp Cycles Per Executed Instruction           cycle        54.00
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.79%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.7% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.93
    Issued Instructions                             inst       196256
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.50
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25997.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4688.91
    Total L1 Elapsed Cycles          cycle       398262
    Average L2 Active Cycles         cycle      4204.67
    Total L2 Elapsed Cycles          cycle       170976
    Average SM Active Cycles         cycle      4688.91
    Total SM Elapsed Cycles          cycle       398262
    Average SMSP Active Cycles       cycle      4612.64
    Total SMSP Elapsed Cycles        cycle      1593048
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.29
    Elapsed Cycles                cycle         6803
    Memory Throughput                 %        48.63
    DRAM Throughput                   %        48.63
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.99
    L2 Cache Throughput               %        29.34
    SM Active Cycles              cycle      4626.67
    Compute (SM) Throughput           %        16.70
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.28
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.28
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.84%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.41
    Mem Busy                               %        19.87
    Max Bandwidth                          %        48.63
    L1/TEX Hit Rate                        %         7.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.70
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.18
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.82
    Active Warps Per Scheduler          warp         8.85
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.85 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.71
    Warp Cycles Per Executed Instruction           cycle        55.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.37%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 48.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.76
    Issued Instructions                             inst       196216
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.49
    Achieved Active Warps Per SM           warp        35.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.51%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25813.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4626.67
    Total L1 Elapsed Cycles          cycle       392528
    Average L2 Active Cycles         cycle      4273.08
    Total L2 Elapsed Cycles          cycle       168528
    Average SM Active Cycles         cycle      4626.67
    Total SM Elapsed Cycles          cycle       392528
    Average SMSP Active Cycles       cycle      4653.00
    Total SMSP Elapsed Cycles        cycle      1570112
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.14
    Elapsed Cycles                cycle      7381444
    Memory Throughput                 %        45.91
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.79
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4358461.81
    Compute (SM) Throughput           %        45.91
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.01
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.44
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.44
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       412.24
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.91
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.69
    Mem Pipes Busy                         %        45.91
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.65%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351172 bank conflicts,  which represent   
          68.56% of the overall 26764804 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.53
    Issued Instructions                             inst    509995747
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78157.33
    Total DRAM Elapsed Cycles        cycle    340974592
    Average L1 Active Cycles         cycle   4358461.81
    Total L1 Elapsed Cycles          cycle    422852338
    Average L2 Active Cycles         cycle    351079.12
    Total L2 Elapsed Cycles          cycle    180186240
    Average SM Active Cycles         cycle   4358461.81
    Total SM Elapsed Cycles          cycle    422852338
    Average SMSP Active Cycles       cycle   4356126.81
    Total SMSP Elapsed Cycles        cycle   1691409352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.93% above the average, while the minimum instance value is 8.16% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.15% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.16% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.63
    Elapsed Cycles                cycle         6880
    Memory Throughput                 %        45.08
    DRAM Throughput                   %        45.08
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.17
    L2 Cache Throughput               %        28.71
    SM Active Cycles              cycle      4675.07
    Compute (SM) Throughput           %        16.50
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.32
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.32
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.06
    Mem Busy                               %        20.01
    Max Bandwidth                          %        45.08
    L1/TEX Hit Rate                        %         6.85
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.50
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.22
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.78
    Active Warps Per Scheduler          warp         8.50
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.92%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.50 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.66
    Warp Cycles Per Executed Instruction           cycle        50.95
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.92%                                                                                          
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.2% of the total average of 46.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.53
    Issued Instructions                             inst       187812
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        68.75
    Achieved Active Warps Per SM           warp        33.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 31.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (68.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24309.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4675.07
    Total L1 Elapsed Cycles          cycle       397116
    Average L2 Active Cycles         cycle      4121.08
    Total L2 Elapsed Cycles          cycle       170592
    Average SM Active Cycles         cycle      4675.07
    Total SM Elapsed Cycles          cycle       397116
    Average SMSP Active Cycles       cycle      4442.36
    Total SMSP Elapsed Cycles        cycle      1588464
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.29
    Elapsed Cycles                cycle         6793
    Memory Throughput                 %        48.79
    DRAM Throughput                   %        48.79
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.99
    L2 Cache Throughput               %        29.38
    SM Active Cycles              cycle      4614.60
    Compute (SM) Throughput           %        16.72
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.34
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.34
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.82%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.42
    Mem Busy                               %        19.88
    Max Bandwidth                          %        48.79
    L1/TEX Hit Rate                        %         7.74
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.72
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.45
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.55
    Active Warps Per Scheduler          warp         8.88
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.21%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.88 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.12
    Warp Cycles Per Executed Instruction           cycle        54.91
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.21%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.11
    Issued Instructions                             inst       196297
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.26
    Achieved Active Warps Per SM           warp        36.60
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25898.67
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4614.60
    Total L1 Elapsed Cycles          cycle       392072
    Average L2 Active Cycles         cycle      4251.58
    Total L2 Elapsed Cycles          cycle       168408
    Average SM Active Cycles         cycle      4614.60
    Total SM Elapsed Cycles          cycle       392072
    Average SMSP Active Cycles       cycle      4585.23
    Total SMSP Elapsed Cycles        cycle      1568288
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.55
    Elapsed Cycles                cycle         6902
    Memory Throughput                 %        47.67
    DRAM Throughput                   %        47.67
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.61
    L2 Cache Throughput               %        28.92
    SM Active Cycles              cycle      4739.14
    Compute (SM) Throughput           %        16.45
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.85
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.85
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.20
    Mem Busy                               %        19.56
    Max Bandwidth                          %        47.67
    L1/TEX Hit Rate                        %         7.62
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.77
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.23
    Active Warps Per Scheduler          warp         8.53
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.33%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.53 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.02
    Warp Cycles Per Executed Instruction           cycle        54.79
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.33%                                                                                          
          On average, each warp of this workload spends 30.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.1% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.05
    Issued Instructions                             inst       196284
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.05
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.95%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25792
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4739.14
    Total L1 Elapsed Cycles          cycle       398476
    Average L2 Active Cycles         cycle      4306.58
    Total L2 Elapsed Cycles          cycle       171144
    Average SM Active Cycles         cycle      4739.14
    Total SM Elapsed Cycles          cycle       398476
    Average SMSP Active Cycles       cycle      4761.34
    Total SMSP Elapsed Cycles        cycle      1593904
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       827.34
    Elapsed Cycles                cycle         7158
    Memory Throughput                 %        49.04
    DRAM Throughput                   %        49.04
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        29.39
    SM Active Cycles              cycle      4903.21
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.60
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.26
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.26
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.36%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       145.29
    Mem Busy                               %        19.90
    Max Bandwidth                          %        49.04
    L1/TEX Hit Rate                        %         7.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.99
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.93
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.07
    Active Warps Per Scheduler          warp         8.93
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.96%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.93 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.78
    Warp Cycles Per Executed Instruction           cycle        56.83
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.96%                                                                                          
          On average, each warp of this workload spends 31.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.8% of the total average of 49.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.48
    Issued Instructions                             inst       196384
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.53
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25861.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4903.21
    Total L1 Elapsed Cycles          cycle       409974
    Average L2 Active Cycles         cycle      4158.21
    Total L2 Elapsed Cycles          cycle       168264
    Average SM Active Cycles         cycle      4903.21
    Total SM Elapsed Cycles          cycle       409974
    Average SMSP Active Cycles       cycle      4721.25
    Total SMSP Elapsed Cycles        cycle      1639896
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.42
    Elapsed Cycles                cycle      7380509
    Memory Throughput                 %        45.92
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.85
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355207.47
    Compute (SM) Throughput           %        45.92
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.16
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.92
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.88
    Mem Pipes Busy                         %        45.92
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680065 bank conflicts,  which represent   
          15.30% of the overall 95969281 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351400 bank conflicts,  which represent   
          68.56% of the overall 26764997 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78280
    Total DRAM Elapsed Cycles        cycle    340750336
    Average L1 Active Cycles         cycle   4355207.47
    Total L1 Elapsed Cycles          cycle    422725184
    Average L2 Active Cycles         cycle    349806.58
    Total L2 Elapsed Cycles          cycle    180067536
    Average SM Active Cycles         cycle   4355207.47
    Total SM Elapsed Cycles          cycle    422725184
    Average SMSP Active Cycles       cycle   4354864.76
    Total SMSP Elapsed Cycles        cycle   1690900736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.95% above the average, while the minimum instance value is 8.18% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.88%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.97% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.95% above the average, while the minimum instance value is 8.18% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.09
    Elapsed Cycles                cycle         6735
    Memory Throughput                 %        45.63
    DRAM Throughput                   %        45.63
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.00
    L2 Cache Throughput               %        29.33
    SM Active Cycles              cycle      4519.98
    Compute (SM) Throughput           %        16.85
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.66
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.91
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.63%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.69
    Mem Busy                               %        20.37
    Max Bandwidth                          %        45.63
    L1/TEX Hit Rate                        %         7.84
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.81
    Mem Pipes Busy                         %        16.85
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.25
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.75
    Active Warps Per Scheduler          warp         8.53
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.53 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.75
    Warp Cycles Per Executed Instruction           cycle        51.05
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.37%                                                                                          
          On average, each warp of this workload spends 27.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.2% of the total average of 46.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.72
    Issued Instructions                             inst       187855
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.56
    Achieved Active Warps Per SM           warp        34.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.44%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24064
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4519.98
    Total L1 Elapsed Cycles          cycle       389018
    Average L2 Active Cycles         cycle      4117.38
    Total L2 Elapsed Cycles          cycle       167064
    Average SM Active Cycles         cycle      4519.98
    Total SM Elapsed Cycles          cycle       389018
    Average SMSP Active Cycles       cycle      4436.67
    Total SMSP Elapsed Cycles        cycle      1556072
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.33
    Elapsed Cycles                cycle         6801
    Memory Throughput                 %        48.59
    DRAM Throughput                   %        48.59
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.97
    L2 Cache Throughput               %        29.37
    SM Active Cycles              cycle      4584.88
    Compute (SM) Throughput           %        16.69
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.45
    Issued Ipc Active     inst/cycle         0.74
    SM Busy                        %        18.45
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.76%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.73
    Mem Busy                               %        19.87
    Max Bandwidth                          %        48.59
    L1/TEX Hit Rate                        %         7.36
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.69
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.22
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.78
    Active Warps Per Scheduler          warp         8.77
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.77 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.17
    Warp Cycles Per Executed Instruction           cycle        54.93
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.41%                                                                                          
          On average, each warp of this workload spends 29.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.8% of the total average of 48.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.69
    Issued Instructions                             inst       196199
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.07
    Achieved Active Warps Per SM           warp        35.56
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.93%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25872
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4584.88
    Total L1 Elapsed Cycles          cycle       392550
    Average L2 Active Cycles         cycle      4190.33
    Total L2 Elapsed Cycles          cycle       168480
    Average SM Active Cycles         cycle      4584.88
    Total SM Elapsed Cycles          cycle       392550
    Average SMSP Active Cycles       cycle      4642.65
    Total SMSP Elapsed Cycles        cycle      1570200
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       799.84
    Elapsed Cycles                cycle         6922
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        24.72
    L2 Cache Throughput               %        29.63
    SM Active Cycles              cycle      4667.59
    Compute (SM) Throughput           %        16.53
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.13
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.13
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.92%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       145.44
    Mem Busy                               %        20.04
    Max Bandwidth                          %        48.93
    L1/TEX Hit Rate                        %         7.07
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.92
    Mem Pipes Busy                         %        16.53
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.99
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.01
    Active Warps Per Scheduler          warp         8.86
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.07%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.86 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.22
    Warp Cycles Per Executed Instruction           cycle        56.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.07%                                                                                          
          On average, each warp of this workload spends 30.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.1% of the total average of 49.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.06
    Issued Instructions                             inst       196285
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        78.62
    Achieved Active Warps Per SM           warp        37.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 21.38%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (78.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25888
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4667.59
    Total L1 Elapsed Cycles          cycle       396360
    Average L2 Active Cycles         cycle      4251.54
    Total L2 Elapsed Cycles          cycle       167088
    Average SM Active Cycles         cycle      4667.59
    Total SM Elapsed Cycles          cycle       396360
    Average SMSP Active Cycles       cycle      4702.07
    Total SMSP Elapsed Cycles        cycle      1585440
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       793.47
    Elapsed Cycles                cycle         6991
    Memory Throughput                 %        48.23
    DRAM Throughput                   %        48.23
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.48
    L2 Cache Throughput               %        29.12
    SM Active Cycles              cycle      4740.03
    Compute (SM) Throughput           %        16.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.84
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s          143
    Mem Busy                               %        19.69
    Max Bandwidth                          %        48.23
    L1/TEX Hit Rate                        %         7.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.90
    Mem Pipes Busy                         %        16.36
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.96
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.04
    Active Warps Per Scheduler          warp         8.59
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.77%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.59 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.84
    Warp Cycles Per Executed Instruction           cycle        54.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.77%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.3% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.43
    Issued Instructions                             inst       196140
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.49
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.51%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25930.67
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4740.03
    Total L1 Elapsed Cycles          cycle       400582
    Average L2 Active Cycles         cycle      4234.42
    Total L2 Elapsed Cycles          cycle       170040
    Average SM Active Cycles         cycle      4740.03
    Total SM Elapsed Cycles          cycle       400582
    Average SMSP Active Cycles       cycle      4708.07
    Total SMSP Elapsed Cycles        cycle      1602328
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.53
    Elapsed Cycles                cycle      7386218
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.87
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4353773.53
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.49
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.49
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       418.52
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.49
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680074 bank conflicts,  which represent   
          15.30% of the overall 95969290 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.71%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351374 bank conflicts,  which represent   
          68.56% of the overall 26764993 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995749
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        79328
    Total DRAM Elapsed Cycles        cycle    340891648
    Average L1 Active Cycles         cycle   4353773.53
    Total L1 Elapsed Cycles          cycle    422959114
    Average L2 Active Cycles         cycle    350256.17
    Total L2 Elapsed Cycles          cycle    180141696
    Average SM Active Cycles         cycle   4353773.53
    Total SM Elapsed Cycles          cycle    422959114
    Average SMSP Active Cycles       cycle   4354881.96
    Total SMSP Elapsed Cycles        cycle   1691836456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.88%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.99% above the average, while the minimum instance value is 8.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.91% above the average, while the minimum instance value is 8.18% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.88%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.99% above the average, while the minimum instance value is 8.19% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       791.89
    Elapsed Cycles                cycle         6793
    Memory Throughput                 %        46.45
    DRAM Throughput                   %        46.45
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        24.84
    L2 Cache Throughput               %        29.67
    SM Active Cycles              cycle      4549.14
    Compute (SM) Throughput           %        16.83
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.80
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.80
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.69%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       136.86
    Mem Busy                               %        20.72
    Max Bandwidth                          %        46.45
    L1/TEX Hit Rate                        %         7.10
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.83
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.94
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.06
    Active Warps Per Scheduler          warp         8.37
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.37 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.67
    Warp Cycles Per Executed Instruction           cycle        50.95
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.55%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.9% of the total average of 46.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.59
    Issued Instructions                             inst       187824
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.22
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24178.67
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4549.14
    Total L1 Elapsed Cycles          cycle       389484
    Average L2 Active Cycles         cycle      4123.58
    Total L2 Elapsed Cycles          cycle       165144
    Average SM Active Cycles         cycle      4549.14
    Total SM Elapsed Cycles          cycle       389484
    Average SMSP Active Cycles       cycle      4511.54
    Total SMSP Elapsed Cycles        cycle      1557936
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.57
    Elapsed Cycles                cycle         6834
    Memory Throughput                 %        48.43
    DRAM Throughput                   %        48.43
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.82
    L2 Cache Throughput               %        29.22
    SM Active Cycles              cycle      4676.17
    Compute (SM) Throughput           %        16.61
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.10
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.10
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.94%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.20
    Mem Busy                               %        19.76
    Max Bandwidth                          %        48.43
    L1/TEX Hit Rate                        %         7.58
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.61
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.53
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.47
    Active Warps Per Scheduler          warp         9.08
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.57%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.08 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.99
    Warp Cycles Per Executed Instruction           cycle        55.90
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.57%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.1% of the total average of 49.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.23
    Issued Instructions                             inst       196325
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.50
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25872
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4676.17
    Total L1 Elapsed Cycles          cycle       394624
    Average L2 Active Cycles         cycle      4304.12
    Total L2 Elapsed Cycles          cycle       169440
    Average SM Active Cycles         cycle      4676.17
    Total SM Elapsed Cycles          cycle       394624
    Average SMSP Active Cycles       cycle      4566.60
    Total SMSP Elapsed Cycles        cycle      1578496
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.09
    Elapsed Cycles                cycle         6929
    Memory Throughput                 %        47.96
    DRAM Throughput                   %        47.96
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.50
    L2 Cache Throughput               %        28.80
    SM Active Cycles              cycle      4756.05
    Compute (SM) Throughput           %        16.38
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.80
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.80
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.53
    Mem Busy                               %        19.51
    Max Bandwidth                          %        47.96
    L1/TEX Hit Rate                        %         7.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.86
    Mem Pipes Busy                         %        16.38
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.77
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.23
    Active Warps Per Scheduler          warp         8.80
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.04%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.80 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.50
    Warp Cycles Per Executed Instruction           cycle        56.51
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.04%                                                                                          
          On average, each warp of this workload spends 31.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.1% of the total average of 49.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.50
    Issued Instructions                             inst       196387
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.86
    Achieved Active Warps Per SM           warp        35.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.14%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25946.67
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4756.05
    Total L1 Elapsed Cycles          cycle       400202
    Average L2 Active Cycles         cycle      4267.62
    Total L2 Elapsed Cycles          cycle       171792
    Average SM Active Cycles         cycle      4756.05
    Total SM Elapsed Cycles          cycle       400202
    Average SMSP Active Cycles       cycle      4762.42
    Total SMSP Elapsed Cycles        cycle      1600808
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       791.98
    Elapsed Cycles                cycle         6969
    Memory Throughput                 %        48.11
    DRAM Throughput                   %        48.11
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.50
    L2 Cache Throughput               %        29.14
    SM Active Cycles              cycle      4716.16
    Compute (SM) Throughput           %        16.39
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.95
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.95
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.02%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.62
    Mem Busy                               %        19.73
    Max Bandwidth                          %        48.11
    L1/TEX Hit Rate                        %         7.80
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.39
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.05
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.95
    Active Warps Per Scheduler          warp         8.95
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.95 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.59
    Warp Cycles Per Executed Instruction           cycle        56.62
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.89%                                                                                          
          On average, each warp of this workload spends 30.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.8% of the total average of 49.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.55
    Issued Instructions                             inst       196399
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.74
    Achieved Active Warps Per SM           warp        36.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.26%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25861.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4716.16
    Total L1 Elapsed Cycles          cycle       399826
    Average L2 Active Cycles         cycle      4186.79
    Total L2 Elapsed Cycles          cycle       169704
    Average SM Active Cycles         cycle      4716.16
    Total SM Elapsed Cycles          cycle       399826
    Average SMSP Active Cycles       cycle      4689.47
    Total SMSP Elapsed Cycles        cycle      1599304
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.75
    Elapsed Cycles                cycle      7383779
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.10
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.18
    SM Active Cycles              cycle   4355863.53
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       412.54
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.68
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680066 bank conflicts,  which represent   
          15.30% of the overall 95969282 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.68%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351406 bank conflicts,  which represent   
          68.56% of the overall 26765019 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995749
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78170.67
    Total DRAM Elapsed Cycles        cycle    340781056
    Average L1 Active Cycles         cycle   4355863.53
    Total L1 Elapsed Cycles          cycle    422928724
    Average L2 Active Cycles         cycle    350789.12
    Total L2 Elapsed Cycles          cycle    180083616
    Average SM Active Cycles         cycle   4355863.53
    Total SM Elapsed Cycles          cycle    422928724
    Average SMSP Active Cycles       cycle   4356080.78
    Total SMSP Elapsed Cycles        cycle   1691714896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.92% above the average, while the minimum instance value is 8.18% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.19% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.18% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       792.32
    Elapsed Cycles                cycle         6819
    Memory Throughput                 %        45.73
    DRAM Throughput                   %        45.73
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.84
    L2 Cache Throughput               %        29.53
    SM Active Cycles              cycle      4548.31
    Compute (SM) Throughput           %        16.75
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.80
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.80
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.68%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.11
    Mem Busy                               %        20.49
    Max Bandwidth                          %        45.73
    L1/TEX Hit Rate                        %         7.11
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.75
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.54
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.46
    Active Warps Per Scheduler          warp         8.42
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.42 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.02
    Warp Cycles Per Executed Instruction           cycle        52.43
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.27%                                                                                          
          On average, each warp of this workload spends 27.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 57.4% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.73
    Issued Instructions                             inst       187858
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.01
    Achieved Active Warps Per SM           warp        35.52
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.99%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23960
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4548.31
    Total L1 Elapsed Cycles          cycle       391166
    Average L2 Active Cycles         cycle      4076.92
    Total L2 Elapsed Cycles          cycle       165960
    Average SM Active Cycles         cycle      4548.31
    Total SM Elapsed Cycles          cycle       391166
    Average SMSP Active Cycles       cycle      4615.54
    Total SMSP Elapsed Cycles        cycle      1564664
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       792.75
    Elapsed Cycles                cycle         6924
    Memory Throughput                 %        48.24
    DRAM Throughput                   %        48.24
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.66
    L2 Cache Throughput               %        29.37
    SM Active Cycles              cycle      4711.36
    Compute (SM) Throughput           %        16.50
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.96
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.96
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.17
    Mem Busy                               %        19.86
    Max Bandwidth                          %        48.24
    L1/TEX Hit Rate                        %         7.07
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        51.01
    Mem Pipes Busy                         %        16.50
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.11
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.89
    Active Warps Per Scheduler          warp         8.69
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.76%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.69 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.99
    Warp Cycles Per Executed Instruction           cycle        54.76
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.76%                                                                                          
          On average, each warp of this workload spends 30.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.12
    Issued Instructions                             inst       196300
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.11
    Achieved Active Warps Per SM           warp        35.57
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.89%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25770.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4711.36
    Total L1 Elapsed Cycles          cycle       397272
    Average L2 Active Cycles         cycle      4234.54
    Total L2 Elapsed Cycles          cycle       168600
    Average SM Active Cycles         cycle      4711.36
    Total SM Elapsed Cycles          cycle       397272
    Average SMSP Active Cycles       cycle      4672.62
    Total SMSP Elapsed Cycles        cycle      1589088
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       815.79
    Elapsed Cycles                cycle         7136
    Memory Throughput                 %        48.58
    DRAM Throughput                   %        48.58
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        29.29
    SM Active Cycles              cycle      4875.60
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.35
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.35
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.31%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.70
    Mem Busy                               %        19.82
    Max Bandwidth                          %        48.58
    L1/TEX Hit Rate                        %         8.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.69
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.31
    Active Warps Per Scheduler          warp         8.75
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.75 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.47
    Warp Cycles Per Executed Instruction           cycle        56.44
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.42%                                                                                          
          On average, each warp of this workload spends 31.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.3% of the total average of 49.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.06
    Issued Instructions                             inst       196286
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.18
    Achieved Active Warps Per SM           warp        33.69
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25866.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4875.60
    Total L1 Elapsed Cycles          cycle       408788
    Average L2 Active Cycles         cycle      4239.88
    Total L2 Elapsed Cycles          cycle       168936
    Average SM Active Cycles         cycle      4875.60
    Total SM Elapsed Cycles          cycle       408788
    Average SMSP Active Cycles       cycle      4783.75
    Total SMSP Elapsed Cycles        cycle      1635152
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       793.70
    Elapsed Cycles                cycle         6910
    Memory Throughput                 %        48.60
    DRAM Throughput                   %        48.60
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        29.45
    SM Active Cycles              cycle      4658.48
    Compute (SM) Throughput           %        16.54
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.16
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.16
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.84
    Mem Busy                               %        19.91
    Max Bandwidth                          %        48.60
    L1/TEX Hit Rate                        %         7.93
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.54
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.14
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.86
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.02
    Warp Cycles Per Executed Instruction           cycle        54.79
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.4%                                                                                           
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.2% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.08
    Issued Instructions                             inst       196291
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.79
    Achieved Active Warps Per SM           warp        35.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25794.67
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4658.48
    Total L1 Elapsed Cycles          cycle       396274
    Average L2 Active Cycles         cycle      4344.71
    Total L2 Elapsed Cycles          cycle       168144
    Average SM Active Cycles         cycle      4658.48
    Total SM Elapsed Cycles          cycle       396274
    Average SMSP Active Cycles       cycle      4664.07
    Total SMSP Elapsed Cycles        cycle      1585096
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       801.81
    Elapsed Cycles                cycle      7383695
    Memory Throughput                 %        45.91
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.89
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4352784.66
    Compute (SM) Throughput           %        45.91
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.50
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.50
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       415.43
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.91
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.50
    Mem Pipes Busy                         %        45.91
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680068 bank conflicts,  which represent   
          15.30% of the overall 95969284 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.72%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351352 bank conflicts,  which represent   
          68.56% of the overall 26764975 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995751
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78696
    Total DRAM Elapsed Cycles        cycle    340684800
    Average L1 Active Cycles         cycle   4352784.66
    Total L1 Elapsed Cycles          cycle    422840224
    Average L2 Active Cycles         cycle    351353.21
    Total L2 Elapsed Cycles          cycle    180032808
    Average SM Active Cycles         cycle   4352784.66
    Total SM Elapsed Cycles          cycle    422840224
    Average SMSP Active Cycles       cycle   4354588.77
    Total SMSP Elapsed Cycles        cycle   1691360896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.97% above the average, while the minimum instance value is 8.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.97% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       794.49
    Elapsed Cycles                cycle         6722
    Memory Throughput                 %        47.12
    DRAM Throughput                   %        47.12
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.25
    L2 Cache Throughput               %        30.15
    SM Active Cycles              cycle      4474.59
    Compute (SM) Throughput           %        17.03
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.66
    Executed Ipc Elapsed  inst/cycle         0.45
    Issue Slots Busy               %        18.09
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.09
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.53%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       139.11
    Mem Busy                               %        20.98
    Max Bandwidth                          %        47.12
    L1/TEX Hit Rate                        %         6.92
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        17.03
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.88
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.12
    Active Warps Per Scheduler          warp         8.41
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.88%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.41 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.04
    Warp Cycles Per Executed Instruction           cycle        51.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.88%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.6% of the total average of 47.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.47
    Issued Instructions                             inst       187798
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.62
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.38%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24205.33
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4474.59
    Total L1 Elapsed Cycles          cycle       384858
    Average L2 Active Cycles         cycle      4113.79
    Total L2 Elapsed Cycles          cycle       162528
    Average SM Active Cycles         cycle      4474.59
    Total SM Elapsed Cycles          cycle       384858
    Average SMSP Active Cycles       cycle      4528.49
    Total SMSP Elapsed Cycles        cycle      1539432
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       821.05
    Elapsed Cycles                cycle         7161
    Memory Throughput                 %        48.74
    DRAM Throughput                   %        48.74
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        29.39
    SM Active Cycles              cycle      4891.09
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.31
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.34%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.25
    Mem Busy                               %        19.87
    Max Bandwidth                          %        48.74
    L1/TEX Hit Rate                        %         6.98
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.99
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.02
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.98
    Active Warps Per Scheduler          warp         8.77
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.77 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.71
    Warp Cycles Per Executed Instruction           cycle        55.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.26%                                                                                          
          On average, each warp of this workload spends 32.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 66.3% of the total average of 48.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.50
    Issued Instructions                             inst       196388
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.86
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.14%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25869.33
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4891.09
    Total L1 Elapsed Cycles          cycle       409902
    Average L2 Active Cycles         cycle      4271.75
    Total L2 Elapsed Cycles          cycle       168528
    Average SM Active Cycles         cycle      4891.09
    Total SM Elapsed Cycles          cycle       409902
    Average SMSP Active Cycles       cycle      4698.50
    Total SMSP Elapsed Cycles        cycle      1639608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.42
    Elapsed Cycles                cycle         6931
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.37
    L2 Cache Throughput               %        28.67
    SM Active Cycles              cycle      4787.45
    Compute (SM) Throughput           %        16.31
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.66
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.15%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.09
    Mem Busy                               %        19.39
    Max Bandwidth                          %        47.68
    L1/TEX Hit Rate                        %         7.73
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.31
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.89
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.11
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.65
    Warp Cycles Per Executed Instruction           cycle        55.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.32%                                                                                          
          On average, each warp of this workload spends 31.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 64.5% of the total average of 48.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.66
    Issued Instructions                             inst       196193
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.26
    Achieved Active Warps Per SM           warp        35.64
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25960
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4787.45
    Total L1 Elapsed Cycles          cycle       401818
    Average L2 Active Cycles         cycle      4213.88
    Total L2 Elapsed Cycles          cycle       172632
    Average SM Active Cycles         cycle      4787.45
    Total SM Elapsed Cycles          cycle       401818
    Average SMSP Active Cycles       cycle      4726.05
    Total SMSP Elapsed Cycles        cycle      1607272
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.85
    Elapsed Cycles                cycle         6977
    Memory Throughput                 %        47.58
    DRAM Throughput                   %        47.58
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.35
    L2 Cache Throughput               %        28.67
    SM Active Cycles              cycle      4771.43
    Compute (SM) Throughput           %        16.28
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.73
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.73
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.12%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.35
    Mem Busy                               %        19.39
    Max Bandwidth                          %        47.58
    L1/TEX Hit Rate                        %         7.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.28
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.06
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.94
    Active Warps Per Scheduler          warp         8.86
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.86 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.07
    Warp Cycles Per Executed Instruction           cycle        55.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.42%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.7% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.17
    Issued Instructions                             inst       196312
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.61
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.8%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25824
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4771.43
    Total L1 Elapsed Cycles          cycle       402552
    Average L2 Active Cycles         cycle      4307.67
    Total L2 Elapsed Cycles          cycle       172656
    Average SM Active Cycles         cycle      4771.43
    Total SM Elapsed Cycles          cycle       402552
    Average SMSP Active Cycles       cycle      4685.69
    Total SMSP Elapsed Cycles        cycle      1610208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.23
    Elapsed Cycles                cycle      7385957
    Memory Throughput                 %        45.89
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355526.24
    Compute (SM) Throughput           %        45.89
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       412.59
    Mem Busy                               %        30.78
    Max Bandwidth                          %        45.89
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.90
    Mem Pipes Busy                         %        45.89
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680094 bank conflicts,  which represent   
          15.30% of the overall 95969310 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351287 bank conflicts,  which represent   
          68.56% of the overall 26764900 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995753
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.34%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78144
    Total DRAM Elapsed Cycles        cycle    340631552
    Average L1 Active Cycles         cycle   4355526.24
    Total L1 Elapsed Cycles          cycle    422995332
    Average L2 Active Cycles         cycle    350801.33
    Total L2 Elapsed Cycles          cycle    180004344
    Average SM Active Cycles         cycle   4355526.24
    Total SM Elapsed Cycles          cycle    422995332
    Average SMSP Active Cycles       cycle   4355693.95
    Total SMSP Elapsed Cycles        cycle   1691981328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.82%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.89% above the average, while the minimum instance value is 8.20% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.82%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.89% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.39
    Elapsed Cycles                cycle         6824
    Memory Throughput                 %        44.83
    DRAM Throughput                   %        44.83
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.44
    L2 Cache Throughput               %        28.97
    SM Active Cycles              cycle      4623.98
    Compute (SM) Throughput           %        16.63
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.51
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.51
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.84%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       132.56
    Mem Busy                               %        20.13
    Max Bandwidth                          %        44.83
    L1/TEX Hit Rate                        %         7.48
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.63
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.85
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.15
    Active Warps Per Scheduler          warp         8.42
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 55.17%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.42 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.21
    Warp Cycles Per Executed Instruction           cycle        51.54
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 55.17%                                                                                          
          On average, each warp of this workload spends 28.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.8% of the total average of 47.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.62
    Issued Instructions                             inst       187832
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.26
    Achieved Active Warps Per SM           warp        33.73
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23949.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4623.98
    Total L1 Elapsed Cycles          cycle       394030
    Average L2 Active Cycles         cycle      4142.29
    Total L2 Elapsed Cycles          cycle       169176
    Average SM Active Cycles         cycle      4623.98
    Total SM Elapsed Cycles          cycle       394030
    Average SMSP Active Cycles       cycle      4536.56
    Total SMSP Elapsed Cycles        cycle      1576120
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.61
    Elapsed Cycles                cycle         6933
    Memory Throughput                 %        47.74
    DRAM Throughput                   %        47.74
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.48
    L2 Cache Throughput               %        28.78
    SM Active Cycles              cycle      4756.07
    Compute (SM) Throughput           %        16.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.79
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.79
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.09%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.32
    Mem Busy                               %        19.47
    Max Bandwidth                          %        47.74
    L1/TEX Hit Rate                        %         7.15
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.36
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.13
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.87
    Active Warps Per Scheduler          warp         8.76
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.76 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.34
    Warp Cycles Per Executed Instruction           cycle        55.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.26%                                                                                          
          On average, each warp of this workload spends 30.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.4% of the total average of 48.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.95
    Issued Instructions                             inst       196260
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.78
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.22%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25909.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4756.07
    Total L1 Elapsed Cycles          cycle       400468
    Average L2 Active Cycles         cycle      4150.62
    Total L2 Elapsed Cycles          cycle       171984
    Average SM Active Cycles         cycle      4756.07
    Total SM Elapsed Cycles          cycle       400468
    Average SMSP Active Cycles       cycle      4665.34
    Total SMSP Elapsed Cycles        cycle      1601872
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.25
    Elapsed Cycles                cycle         6950
    Memory Throughput                 %        47.69
    DRAM Throughput                   %        47.69
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.42
    L2 Cache Throughput               %        28.77
    SM Active Cycles              cycle      4731.36
    Compute (SM) Throughput           %        16.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.88
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.18
    Mem Busy                               %        19.45
    Max Bandwidth                          %        47.69
    L1/TEX Hit Rate                        %         7.93
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.90
    Mem Pipes Busy                         %        16.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.31
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.69
    Active Warps Per Scheduler          warp         8.74
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.31%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.74 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.75
    Warp Cycles Per Executed Instruction           cycle        54.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.31%                                                                                          
          On average, each warp of this workload spends 29.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.98
    Issued Instructions                             inst       196268
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.26
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25882.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4731.36
    Total L1 Elapsed Cycles          cycle       401302
    Average L2 Active Cycles         cycle      4181.96
    Total L2 Elapsed Cycles          cycle       172176
    Average SM Active Cycles         cycle      4731.36
    Total SM Elapsed Cycles          cycle       401302
    Average SMSP Active Cycles       cycle      4621.01
    Total SMSP Elapsed Cycles        cycle      1605208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       795.86
    Elapsed Cycles                cycle         7114
    Memory Throughput                 %        47.41
    DRAM Throughput                   %        47.41
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.05
    L2 Cache Throughput               %        28.63
    SM Active Cycles              cycle      4834.41
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.50
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.50
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.24%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.72
    Mem Busy                               %        19.38
    Max Bandwidth                          %        47.41
    L1/TEX Hit Rate                        %         7.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.08
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.90
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.10
    Active Warps Per Scheduler          warp         8.67
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.67 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.40
    Warp Cycles Per Executed Instruction           cycle        55.22
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.59%                                                                                          
          On average, each warp of this workload spends 30.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.0% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.99
    Issued Instructions                             inst       196269
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.77%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25893.33
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4834.41
    Total L1 Elapsed Cycles          cycle       407676
    Average L2 Active Cycles         cycle      4187.17
    Total L2 Elapsed Cycles          cycle       172752
    Average SM Active Cycles         cycle      4834.41
    Total SM Elapsed Cycles          cycle       407676
    Average SMSP Active Cycles       cycle         4725
    Total SMSP Elapsed Cycles        cycle      1630704
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.28
    Elapsed Cycles                cycle      7387057
    Memory Throughput                 %        45.88
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.83
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4356025.24
    Compute (SM) Throughput           %        45.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.46
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       426.06
    Mem Busy                               %        30.77
    Max Bandwidth                          %        45.88
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.55
    Mem Pipes Busy                         %        45.88
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680067 bank conflicts,  which represent   
          15.30% of the overall 95969283 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.68%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351400 bank conflicts,  which represent   
          68.56% of the overall 26765019 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995749
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80714.67
    Total DRAM Elapsed Cycles        cycle    340710400
    Average L1 Active Cycles         cycle   4356025.24
    Total L1 Elapsed Cycles          cycle    423120860
    Average L2 Active Cycles         cycle    355668.46
    Total L2 Elapsed Cycles          cycle    180046800
    Average SM Active Cycles         cycle   4356025.24
    Total SM Elapsed Cycles          cycle    423120860
    Average SMSP Active Cycles       cycle   4355743.25
    Total SMSP Elapsed Cycles        cycle   1692483440
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.82%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.89% above the average, while the minimum instance value is 8.21% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.82%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.89% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.02
    Elapsed Cycles                cycle         6709
    Memory Throughput                 %        46.31
    DRAM Throughput                   %        46.31
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.15
    L2 Cache Throughput               %        29.52
    SM Active Cycles              cycle      4492.24
    Compute (SM) Throughput           %        16.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.66
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.02
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.02
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.57%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       136.83
    Mem Busy                               %        20.56
    Max Bandwidth                          %        46.31
    L1/TEX Hit Rate                        %         7.90
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.52
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.48
    Active Warps Per Scheduler          warp         8.29
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.69%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.29 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.30
    Warp Cycles Per Executed Instruction           cycle        51.64
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.69%                                                                                          
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.4% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.50
    Issued Instructions                             inst       187804
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.06
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24264
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4492.24
    Total L1 Elapsed Cycles          cycle       387070
    Average L2 Active Cycles         cycle      4122.83
    Total L2 Elapsed Cycles          cycle       166008
    Average SM Active Cycles         cycle      4492.24
    Total SM Elapsed Cycles          cycle       387070
    Average SMSP Active Cycles       cycle      4619.78
    Total SMSP Elapsed Cycles        cycle      1548280
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.74
    Elapsed Cycles                cycle         6828
    Memory Throughput                 %        48.59
    DRAM Throughput                   %        48.59
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.85
    L2 Cache Throughput               %        29.19
    SM Active Cycles              cycle      4636.36
    Compute (SM) Throughput           %        16.62
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.25
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.86%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.19
    Mem Busy                               %        19.77
    Max Bandwidth                          %        48.59
    L1/TEX Hit Rate                        %         8.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.62
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.30
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.70
    Active Warps Per Scheduler          warp         8.65
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.65 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.30
    Warp Cycles Per Executed Instruction           cycle        53.98
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.41%                                                                                          
          On average, each warp of this workload spends 30.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 65.1% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.17
    Issued Instructions                             inst       196312
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.18
    Achieved Active Warps Per SM           warp        36.08
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25954.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4636.36
    Total L1 Elapsed Cycles          cycle       394254
    Average L2 Active Cycles         cycle      4254.67
    Total L2 Elapsed Cycles          cycle       169344
    Average SM Active Cycles         cycle      4636.36
    Total SM Elapsed Cycles          cycle       394254
    Average SMSP Active Cycles       cycle      4624.72
    Total SMSP Elapsed Cycles        cycle      1577016
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.22
    Elapsed Cycles                cycle         6928
    Memory Throughput                 %        47.84
    DRAM Throughput                   %        47.84
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.51
    L2 Cache Throughput               %        28.88
    SM Active Cycles              cycle      4747.83
    Compute (SM) Throughput           %        16.39
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.82
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.82
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.08%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.26
    Mem Busy                               %        19.53
    Max Bandwidth                          %        47.84
    L1/TEX Hit Rate                        %         7.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.39
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.77
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.23
    Active Warps Per Scheduler          warp         8.42
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.16%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.42 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.39
    Warp Cycles Per Executed Instruction           cycle        54.07
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.16%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.8% of the total average of 47.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.05
    Issued Instructions                             inst       196284
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.28%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25802.67
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4747.83
    Total L1 Elapsed Cycles          cycle       399830
    Average L2 Active Cycles         cycle      4267.88
    Total L2 Elapsed Cycles          cycle       171456
    Average SM Active Cycles         cycle      4747.83
    Total SM Elapsed Cycles          cycle       399830
    Average SMSP Active Cycles       cycle      4760.85
    Total SMSP Elapsed Cycles        cycle      1599320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.24
    Elapsed Cycles                cycle         6959
    Memory Throughput                 %        47.68
    DRAM Throughput                   %        47.68
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.42
    L2 Cache Throughput               %        28.71
    SM Active Cycles              cycle      4769.48
    Compute (SM) Throughput           %        16.31
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.74
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.12%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.62
    Mem Busy                               %        19.43
    Max Bandwidth                          %        47.68
    L1/TEX Hit Rate                        %         7.96
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.31
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.97
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.03
    Active Warps Per Scheduler          warp         8.69
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.32%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.69 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.35
    Warp Cycles Per Executed Instruction           cycle        55.16
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.32%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.9% of the total average of 48.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.03
    Issued Instructions                             inst       196279
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.62
    Achieved Active Warps Per SM           warp        34.86
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.38%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25874.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4769.48
    Total L1 Elapsed Cycles          cycle       401726
    Average L2 Active Cycles         cycle      4249.42
    Total L2 Elapsed Cycles          cycle       172296
    Average SM Active Cycles         cycle      4769.48
    Total SM Elapsed Cycles          cycle       401726
    Average SMSP Active Cycles       cycle      4707.00
    Total SMSP Elapsed Cycles        cycle      1606904
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.40
    Elapsed Cycles                cycle      7384858
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.85
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355329.52
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.67
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.67
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680068 bank conflicts,  which represent   
          15.30% of the overall 95969284 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351357 bank conflicts,  which represent   
          68.56% of the overall 26764977 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78320
    Total DRAM Elapsed Cycles        cycle    340502528
    Average L1 Active Cycles         cycle   4355329.52
    Total L1 Elapsed Cycles          cycle    422929282
    Average L2 Active Cycles         cycle    349769.67
    Total L2 Elapsed Cycles          cycle    179936568
    Average SM Active Cycles         cycle   4355329.52
    Total SM Elapsed Cycles          cycle    422929282
    Average SMSP Active Cycles       cycle   4354531.02
    Total SMSP Elapsed Cycles        cycle   1691717128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.96% above the average, while the minimum instance value is 8.21% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.05
    Elapsed Cycles                cycle         6769
    Memory Throughput                 %        45.45
    DRAM Throughput                   %        45.45
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.76
    L2 Cache Throughput               %        29.25
    SM Active Cycles              cycle      4563.95
    Compute (SM) Throughput           %        16.78
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.74
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.72%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.16
    Mem Busy                               %        20.31
    Max Bandwidth                          %        45.45
    L1/TEX Hit Rate                        %         7.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.78
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.45
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.55
    Active Warps Per Scheduler          warp         8.69
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.69 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.08
    Warp Cycles Per Executed Instruction           cycle        51.41
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.55%                                                                                          
          On average, each warp of this workload spends 28.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 47.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.75
    Issued Instructions                             inst       187862
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.15
    Achieved Active Warps Per SM           warp        34.63
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.85%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23970.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4563.95
    Total L1 Elapsed Cycles          cycle       390494
    Average L2 Active Cycles         cycle      4179.21
    Total L2 Elapsed Cycles          cycle       167520
    Average SM Active Cycles         cycle      4563.95
    Total SM Elapsed Cycles          cycle       390494
    Average SMSP Active Cycles       cycle      4387.97
    Total SMSP Elapsed Cycles        cycle      1561976
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.65
    Elapsed Cycles                cycle         6918
    Memory Throughput                 %        47.85
    DRAM Throughput                   %        47.85
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.55
    L2 Cache Throughput               %        28.83
    SM Active Cycles              cycle      4713.93
    Compute (SM) Throughput           %        16.40
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.95
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.95
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.72
    Mem Busy                               %        19.51
    Max Bandwidth                          %        47.85
    L1/TEX Hit Rate                        %         8.25
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.40
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.26
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.74
    Active Warps Per Scheduler          warp         8.79
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.79 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.15
    Warp Cycles Per Executed Instruction           cycle        54.93
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.15%                                                                                          
          On average, each warp of this workload spends 28.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.7% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.00
    Issued Instructions                             inst       196273
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.02
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.98%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25888
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4713.93
    Total L1 Elapsed Cycles          cycle       399538
    Average L2 Active Cycles         cycle      4322.04
    Total L2 Elapsed Cycles          cycle       171600
    Average SM Active Cycles         cycle      4713.93
    Total SM Elapsed Cycles          cycle       399538
    Average SMSP Active Cycles       cycle      4633.62
    Total SMSP Elapsed Cycles        cycle      1598152
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.13
    Elapsed Cycles                cycle         6920
    Memory Throughput                 %        47.94
    DRAM Throughput                   %        47.94
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.55
    L2 Cache Throughput               %        28.91
    SM Active Cycles              cycle      4660.91
    Compute (SM) Throughput           %        16.41
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.15
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.91%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.99
    Mem Busy                               %        19.55
    Max Bandwidth                          %        47.94
    L1/TEX Hit Rate                        %         7.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.41
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.91
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.09
    Active Warps Per Scheduler          warp         8.90
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.90 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.68
    Warp Cycles Per Executed Instruction           cycle        56.69
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.06%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 49.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       846.10
    Issued Instructions                             inst       196296
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.75
    Achieved Active Warps Per SM           warp        35.88
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.25%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25936
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4660.91
    Total L1 Elapsed Cycles          cycle       399272
    Average L2 Active Cycles         cycle      4209.46
    Total L2 Elapsed Cycles          cycle       171240
    Average SM Active Cycles         cycle      4660.91
    Total SM Elapsed Cycles          cycle       399272
    Average SMSP Active Cycles       cycle      4724.17
    Total SMSP Elapsed Cycles        cycle      1597088
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.76
    Elapsed Cycles                cycle         6956
    Memory Throughput                 %        47.75
    DRAM Throughput                   %        47.75
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.44
    L2 Cache Throughput               %        28.72
    SM Active Cycles              cycle      4712.24
    Compute (SM) Throughput           %        16.32
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.95
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.95
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.86
    Mem Busy                               %        19.43
    Max Bandwidth                          %        47.75
    L1/TEX Hit Rate                        %         7.68
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.32
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.91
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.09
    Active Warps Per Scheduler          warp         8.76
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.25%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.76 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.89
    Warp Cycles Per Executed Instruction           cycle        55.78
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.25%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.0% of the total average of 48.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       845.86
    Issued Instructions                             inst       196240
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.02
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.98%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25917.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4712.24
    Total L1 Elapsed Cycles          cycle       401488
    Average L2 Active Cycles         cycle      4228.75
    Total L2 Elapsed Cycles          cycle       172272
    Average SM Active Cycles         cycle      4712.24
    Total SM Elapsed Cycles          cycle       401488
    Average SMSP Active Cycles       cycle      4723.91
    Total SMSP Elapsed Cycles        cycle      1605952
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.57
    Elapsed Cycles                cycle      7387787
    Memory Throughput                 %        45.88
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.85
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355261.81
    Compute (SM) Throughput           %        45.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.73
    Mem Busy                               %        30.78
    Max Bandwidth                          %        45.88
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.50
    Mem Pipes Busy                         %        45.88
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351369 bank conflicts,  which represent   
          68.56% of the overall 26764999 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.46
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.54
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78349.33
    Total DRAM Elapsed Cycles        cycle    340579328
    Average L1 Active Cycles         cycle   4355261.81
    Total L1 Elapsed Cycles          cycle    423115914
    Average L2 Active Cycles         cycle    353350.46
    Total L2 Elapsed Cycles          cycle    179977008
    Average SM Active Cycles         cycle   4355261.81
    Total SM Elapsed Cycles          cycle    423115914
    Average SMSP Active Cycles       cycle   4356342.02
    Total SMSP Elapsed Cycles        cycle   1692463656
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.82%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.90% above the average, while the minimum instance value is 8.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.82%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.90% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.52
    Elapsed Cycles                cycle         6649
    Memory Throughput                 %        46.66
    DRAM Throughput                   %        46.66
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.34
    L2 Cache Throughput               %        29.72
    SM Active Cycles              cycle      4458.67
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.67
    Executed Ipc Elapsed  inst/cycle         0.45
    Issue Slots Busy               %        18.15
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.5%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       137.55
    Mem Busy                               %        20.74
    Max Bandwidth                          %        46.66
    L1/TEX Hit Rate                        %         6.82
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        17.07
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.38
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.62
    Active Warps Per Scheduler          warp         8.70
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.34%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.70 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.33
    Warp Cycles Per Executed Instruction           cycle        51.66
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.34%                                                                                          
          On average, each warp of this workload spends 28.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.9% of the total average of 47.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.38
    Issued Instructions                             inst       187776
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.15
    Achieved Active Warps Per SM           warp        34.63
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.85%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24208
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4458.67
    Total L1 Elapsed Cycles          cycle       383912
    Average L2 Active Cycles         cycle      4195.50
    Total L2 Elapsed Cycles          cycle       164784
    Average SM Active Cycles         cycle      4458.67
    Total SM Elapsed Cycles          cycle       383912
    Average SMSP Active Cycles       cycle      4403.22
    Total SMSP Elapsed Cycles        cycle      1535648
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.98
    Elapsed Cycles                cycle         6792
    Memory Throughput                 %        48.60
    DRAM Throughput                   %        48.60
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.94
    L2 Cache Throughput               %        29.29
    SM Active Cycles              cycle      4645.02
    Compute (SM) Throughput           %        16.64
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.25
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.88%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.23
    Mem Busy                               %        19.80
    Max Bandwidth                          %        48.60
    L1/TEX Hit Rate                        %         7.85
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.64
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.07
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.93
    Active Warps Per Scheduler          warp         8.83
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.4%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.83 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.88
    Warp Cycles Per Executed Instruction           cycle        55.88
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.4%                                                                                           
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.9% of the total average of 48.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.78
    Issued Instructions                             inst       196684
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.53
    Achieved Active Warps Per SM           warp        36.26
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.47%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25877.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4645.02
    Total L1 Elapsed Cycles          cycle       393816
    Average L2 Active Cycles         cycle      4295.67
    Total L2 Elapsed Cycles          cycle       169128
    Average SM Active Cycles         cycle      4645.02
    Total SM Elapsed Cycles          cycle       393816
    Average SMSP Active Cycles       cycle      4691.87
    Total SMSP Elapsed Cycles        cycle      1575264
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.87
    Elapsed Cycles                cycle         6838
    Memory Throughput                 %        48.63
    DRAM Throughput                   %        48.63
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.88
    L2 Cache Throughput               %        29.25
    SM Active Cycles              cycle      4712.09
    Compute (SM) Throughput           %        16.60
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.00
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.00
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.34
    Mem Busy                               %        19.76
    Max Bandwidth                          %        48.63
    L1/TEX Hit Rate                        %         7.35
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.60
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.16
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.84
    Active Warps Per Scheduler          warp         8.88
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.88 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.92
    Warp Cycles Per Executed Instruction           cycle        55.94
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.37%                                                                                          
          On average, each warp of this workload spends 28.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.0% of the total average of 48.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.95
    Issued Instructions                             inst       196725
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.13
    Achieved Active Warps Per SM           warp        34.62
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.87%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25896
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4712.09
    Total L1 Elapsed Cycles          cycle       394762
    Average L2 Active Cycles         cycle      4270.08
    Total L2 Elapsed Cycles          cycle       169440
    Average SM Active Cycles         cycle      4712.09
    Total SM Elapsed Cycles          cycle       394762
    Average SMSP Active Cycles       cycle      4670.50
    Total SMSP Elapsed Cycles        cycle      1579048
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.50
    Elapsed Cycles                cycle         6869
    Memory Throughput                 %        48.47
    DRAM Throughput                   %        48.47
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.76
    L2 Cache Throughput               %        29.14
    SM Active Cycles              cycle      4686.81
    Compute (SM) Throughput           %        16.53
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.10
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.10
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.96%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.25
    Mem Busy                               %        19.68
    Max Bandwidth                          %        48.47
    L1/TEX Hit Rate                        %         7.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.53
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.98
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.02
    Active Warps Per Scheduler          warp         8.57
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.57 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.63
    Warp Cycles Per Executed Instruction           cycle        54.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.53%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 47.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.10
    Issued Instructions                             inst       196759
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.27
    Achieved Active Warps Per SM           warp        35.65
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25976
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4686.81
    Total L1 Elapsed Cycles          cycle       396546
    Average L2 Active Cycles         cycle      4263.50
    Total L2 Elapsed Cycles          cycle       170136
    Average SM Active Cycles         cycle      4686.81
    Total SM Elapsed Cycles          cycle       396546
    Average SMSP Active Cycles       cycle      4715.63
    Total SMSP Elapsed Cycles        cycle      1586184
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.56
    Elapsed Cycles                cycle      7383173
    Memory Throughput                 %        45.91
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.82
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4356616.26
    Compute (SM) Throughput           %        45.91
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.46
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       414.16
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.91
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.65
    Mem Pipes Busy                         %        45.91
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680073 bank conflicts,  which represent   
          15.30% of the overall 95969289 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351378 bank conflicts,  which represent   
          68.56% of the overall 26764989 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.59
    Issued Instructions                             inst    509995760
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.10
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.35%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78384
    Total DRAM Elapsed Cycles        cycle    340381696
    Average L1 Active Cycles         cycle   4356616.26
    Total L1 Elapsed Cycles          cycle    422864246
    Average L2 Active Cycles         cycle    354702.71
    Total L2 Elapsed Cycles          cycle    179872704
    Average SM Active Cycles         cycle   4356616.26
    Total SM Elapsed Cycles          cycle    422864246
    Average SMSP Active Cycles       cycle   4354464.77
    Total SMSP Elapsed Cycles        cycle   1691456984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.94% above the average, while the minimum instance value is 8.19% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.19% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       797.25
    Elapsed Cycles                cycle         6868
    Memory Throughput                 %        45.82
    DRAM Throughput                   %        45.82
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.51
    L2 Cache Throughput               %        29.63
    SM Active Cycles              cycle      4610.95
    Compute (SM) Throughput           %        16.65
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.56
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.56
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.81%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.95
    Mem Busy                               %        20.53
    Max Bandwidth                          %        45.82
    L1/TEX Hit Rate                        %         7.41
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.65
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.90
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.10
    Active Warps Per Scheduler          warp         8.39
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.18%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.39 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.88
    Warp Cycles Per Executed Instruction           cycle        51.19
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.18%                                                                                          
          On average, each warp of this workload spends 28.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.1% of the total average of 46.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.62
    Issued Instructions                             inst       187833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.98
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.02%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23930.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4610.95
    Total L1 Elapsed Cycles          cycle       393598
    Average L2 Active Cycles         cycle      4081.29
    Total L2 Elapsed Cycles          cycle       165504
    Average SM Active Cycles         cycle      4610.95
    Total SM Elapsed Cycles          cycle       393598
    Average SMSP Active Cycles       cycle      4523.82
    Total SMSP Elapsed Cycles        cycle      1574392
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.32
    Elapsed Cycles                cycle         6854
    Memory Throughput                 %        48.44
    DRAM Throughput                   %        48.44
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.83
    L2 Cache Throughput               %        29.24
    SM Active Cycles              cycle      4656.72
    Compute (SM) Throughput           %        16.57
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.21
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.21
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.15
    Mem Busy                               %        19.73
    Max Bandwidth                          %        48.44
    L1/TEX Hit Rate                        %         7.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.57
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.34
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.66
    Active Warps Per Scheduler          warp         8.79
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.56%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.79 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.91
    Warp Cycles Per Executed Instruction           cycle        54.79
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.56%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.97
    Issued Instructions                             inst       196730
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.64
    Achieved Active Warps Per SM           warp        36.31
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.36%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25957.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4656.72
    Total L1 Elapsed Cycles          cycle       395440
    Average L2 Active Cycles         cycle      4227.33
    Total L2 Elapsed Cycles          cycle       169656
    Average SM Active Cycles         cycle      4656.72
    Total SM Elapsed Cycles          cycle       395440
    Average SMSP Active Cycles       cycle      4622.41
    Total SMSP Elapsed Cycles        cycle      1581760
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.11
    Elapsed Cycles                cycle         6830
    Memory Throughput                 %        48.56
    DRAM Throughput                   %        48.56
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.90
    L2 Cache Throughput               %        29.30
    SM Active Cycles              cycle      4604.62
    Compute (SM) Throughput           %        16.62
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.42
    Issued Ipc Active     inst/cycle         0.74
    SM Busy                        %        18.42
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.8%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.57
    Mem Busy                               %        19.78
    Max Bandwidth                          %        48.56
    L1/TEX Hit Rate                        %         7.09
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.62
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.56
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.44
    Active Warps Per Scheduler          warp         8.93
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.93 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.12
    Warp Cycles Per Executed Instruction           cycle        55.03
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.44%                                                                                          
          On average, each warp of this workload spends 30.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.9% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.05
    Issued Instructions                             inst       196747
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.00
    Achieved Active Warps Per SM           warp        36.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25%                                                                                             
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25938.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4604.62
    Total L1 Elapsed Cycles          cycle       394382
    Average L2 Active Cycles         cycle      4240.29
    Total L2 Elapsed Cycles          cycle       169224
    Average SM Active Cycles         cycle      4604.62
    Total SM Elapsed Cycles          cycle       394382
    Average SMSP Active Cycles       cycle      4569.57
    Total SMSP Elapsed Cycles        cycle      1577528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.95
    Elapsed Cycles                cycle         6850
    Memory Throughput                 %        48.05
    DRAM Throughput                   %        48.05
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.85
    L2 Cache Throughput               %        29.27
    SM Active Cycles              cycle      4613.66
    Compute (SM) Throughput           %        16.58
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.39
    Issued Ipc Active     inst/cycle         0.74
    SM Busy                        %        18.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.82%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.98
    Mem Busy                               %        19.75
    Max Bandwidth                          %        48.05
    L1/TEX Hit Rate                        %         7.37
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.58
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.26
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.74
    Active Warps Per Scheduler          warp         8.87
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.87 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.57
    Warp Cycles Per Executed Instruction           cycle        55.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.95%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 48.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.38
    Issued Instructions                             inst       196825
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.68
    Achieved Active Warps Per SM           warp        36.33
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.32%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25832
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4613.66
    Total L1 Elapsed Cycles          cycle       395312
    Average L2 Active Cycles         cycle      4271.21
    Total L2 Elapsed Cycles          cycle       169512
    Average SM Active Cycles         cycle      4613.66
    Total SM Elapsed Cycles          cycle       395312
    Average SMSP Active Cycles       cycle      4645.79
    Total SMSP Elapsed Cycles        cycle      1581248
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.67
    Elapsed Cycles                cycle      7385290
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355402.60
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.11
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.68
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680084 bank conflicts,  which represent   
          15.30% of the overall 95969300 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351301 bank conflicts,  which represent   
          68.56% of the overall 26764888 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.49
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.51
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.83
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78184
    Total DRAM Elapsed Cycles        cycle    340374528
    Average L1 Active Cycles         cycle   4355402.60
    Total L1 Elapsed Cycles          cycle    422915166
    Average L2 Active Cycles         cycle    355061.67
    Total L2 Elapsed Cycles          cycle    179868744
    Average SM Active Cycles         cycle   4355402.60
    Total SM Elapsed Cycles          cycle    422915166
    Average SMSP Active Cycles       cycle   4353692.57
    Total SMSP Elapsed Cycles        cycle   1691660664
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.95% above the average, while the minimum instance value is 8.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.95% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       800.77
    Elapsed Cycles                cycle         6949
    Memory Throughput                 %        45.79
    DRAM Throughput                   %        45.79
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        29.20
    SM Active Cycles              cycle      4644.19
    Compute (SM) Throughput           %        16.45
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.44
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.44
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.88%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.60
    Mem Busy                               %        20.35
    Max Bandwidth                          %        45.79
    L1/TEX Hit Rate                        %         7.63
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.82
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.18
    Active Warps Per Scheduler          warp         8.58
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.21%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.58 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.14
    Warp Cycles Per Executed Instruction           cycle        52.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.21%                                                                                          
          On average, each warp of this workload spends 27.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 57.8% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.81
    Issued Instructions                             inst       187875
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.22
    Achieved Active Warps Per SM           warp        33.71
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24226.67
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4644.19
    Total L1 Elapsed Cycles          cycle       398292
    Average L2 Active Cycles         cycle      4077.96
    Total L2 Elapsed Cycles          cycle       167736
    Average SM Active Cycles         cycle      4644.19
    Total SM Elapsed Cycles          cycle       398292
    Average SMSP Active Cycles       cycle      4544.85
    Total SMSP Elapsed Cycles        cycle      1593168
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       792.83
    Elapsed Cycles                cycle         6930
    Memory Throughput                 %        48.58
    DRAM Throughput                   %        48.58
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.63
    L2 Cache Throughput               %        29.23
    SM Active Cycles              cycle      4803.33
    Compute (SM) Throughput           %        16.43
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.65
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.18%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.63
    Mem Busy                               %        19.76
    Max Bandwidth                          %        48.58
    L1/TEX Hit Rate                        %         7.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.43
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.32
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.68
    Active Warps Per Scheduler          warp         9.08
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.42%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.08 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.59
    Warp Cycles Per Executed Instruction           cycle        56.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.42%                                                                                          
          On average, each warp of this workload spends 31.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.2% of the total average of 49.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.92
    Issued Instructions                             inst       196717
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.19
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.81%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25949.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4803.33
    Total L1 Elapsed Cycles          cycle       398776
    Average L2 Active Cycles         cycle      4151.88
    Total L2 Elapsed Cycles          cycle       169464
    Average SM Active Cycles         cycle      4803.33
    Total SM Elapsed Cycles          cycle       398776
    Average SMSP Active Cycles       cycle      4629.56
    Total SMSP Elapsed Cycles        cycle      1595104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.54
    Elapsed Cycles                cycle         6952
    Memory Throughput                 %        47.77
    DRAM Throughput                   %        47.77
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.47
    L2 Cache Throughput               %        28.79
    SM Active Cycles              cycle      4734.52
    Compute (SM) Throughput           %        16.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.91
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.05%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.90
    Mem Busy                               %        19.45
    Max Bandwidth                          %        47.77
    L1/TEX Hit Rate                        %         8.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.31
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.69
    Active Warps Per Scheduler          warp         8.91
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.23%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.91 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.64
    Warp Cycles Per Executed Instruction           cycle        55.64
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.23%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.4% of the total average of 48.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.09
    Issued Instructions                             inst       196756
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.1%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25925.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4734.52
    Total L1 Elapsed Cycles          cycle       401370
    Average L2 Active Cycles         cycle      4225.42
    Total L2 Elapsed Cycles          cycle       172152
    Average SM Active Cycles         cycle      4734.52
    Total SM Elapsed Cycles          cycle       401370
    Average SMSP Active Cycles       cycle      4631.57
    Total SMSP Elapsed Cycles        cycle      1605480
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.08
    Elapsed Cycles                cycle         6723
    Memory Throughput                 %        49.14
    DRAM Throughput                   %        49.14
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.31
    L2 Cache Throughput               %        29.79
    SM Active Cycles              cycle      4600.52
    Compute (SM) Throughput           %        16.89
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.43
    Issued Ipc Active     inst/cycle         0.74
    SM Busy                        %        18.43
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.79%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       145.12
    Mem Busy                               %        20.12
    Max Bandwidth                          %        49.14
    L1/TEX Hit Rate                        %         7.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.89
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.38
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.62
    Active Warps Per Scheduler          warp         8.76
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.86%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.76 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.65
    Warp Cycles Per Executed Instruction           cycle        54.49
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.86%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.3% of the total average of 47.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.94
    Issued Instructions                             inst       196721
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.21
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25832
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4600.52
    Total L1 Elapsed Cycles          cycle       388054
    Average L2 Active Cycles         cycle      4183.04
    Total L2 Elapsed Cycles          cycle       166392
    Average SM Active Cycles         cycle      4600.52
    Total SM Elapsed Cycles          cycle       388054
    Average SMSP Active Cycles       cycle      4614.11
    Total SMSP Elapsed Cycles        cycle      1552216
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.89
    Elapsed Cycles                cycle      7386400
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355441.26
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       418.10
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.48
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680070 bank conflicts,  which represent   
          15.30% of the overall 95969286 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351417 bank conflicts,  which represent   
          68.56% of the overall 26765007 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.58
    Issued Instructions                             inst    509995758
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.12
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.32%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79117.33
    Total DRAM Elapsed Cycles        cycle    340321280
    Average L1 Active Cycles         cycle   4355441.26
    Total L1 Elapsed Cycles          cycle    422963516
    Average L2 Active Cycles         cycle       355379
    Total L2 Elapsed Cycles          cycle    179840160
    Average SM Active Cycles         cycle   4355441.26
    Total SM Elapsed Cycles          cycle    422963516
    Average SMSP Active Cycles       cycle   4355389.92
    Total SMSP Elapsed Cycles        cycle   1691854064
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.82%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.88% above the average, while the minimum instance value is 8.20% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.82%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.88% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.55
    Elapsed Cycles                cycle         6768
    Memory Throughput                 %        45.33
    DRAM Throughput                   %        45.33
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.78
    L2 Cache Throughput               %        29.21
    SM Active Cycles              cycle      4560.45
    Compute (SM) Throughput           %        16.77
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.75
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.75
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.71%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.72
    Mem Busy                               %        20.25
    Max Bandwidth                          %        45.33
    L1/TEX Hit Rate                        %         7.03
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.77
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.88
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.12
    Active Warps Per Scheduler          warp         8.57
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.67%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.57 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.95
    Warp Cycles Per Executed Instruction           cycle        52.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.67%                                                                                          
          On average, each warp of this workload spends 27.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 56.7% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.55
    Issued Instructions                             inst       187816
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.16
    Achieved Active Warps Per SM           warp        34.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.84%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23981.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4560.45
    Total L1 Elapsed Cycles          cycle       390698
    Average L2 Active Cycles         cycle      4168.79
    Total L2 Elapsed Cycles          cycle       167640
    Average SM Active Cycles         cycle      4560.45
    Total SM Elapsed Cycles          cycle       390698
    Average SMSP Active Cycles       cycle      4528.63
    Total SMSP Elapsed Cycles        cycle      1562792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       795.81
    Elapsed Cycles                cycle         7107
    Memory Throughput                 %        47.73
    DRAM Throughput                   %        47.73
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        28.84
    SM Active Cycles              cycle      4825.22
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.58
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.58
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.22%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.80
    Mem Busy                               %        19.50
    Max Bandwidth                          %        47.73
    L1/TEX Hit Rate                        %         7.14
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.08
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.18
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.82
    Active Warps Per Scheduler          warp         8.68
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.68 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.72
    Warp Cycles Per Executed Instruction           cycle        54.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.27%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 47.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.41
    Issued Instructions                             inst       196832
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.84
    Achieved Active Warps Per SM           warp        34.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.16%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25906.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4825.22
    Total L1 Elapsed Cycles          cycle       407666
    Average L2 Active Cycles         cycle      4263.92
    Total L2 Elapsed Cycles          cycle       171720
    Average SM Active Cycles         cycle      4825.22
    Total SM Elapsed Cycles          cycle       407666
    Average SMSP Active Cycles       cycle      4667.18
    Total SMSP Elapsed Cycles        cycle      1630664
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       791.12
    Elapsed Cycles                cycle         7039
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        28.79
    SM Active Cycles              cycle      4798.83
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.68
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.17%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.71
    Mem Busy                               %        19.45
    Max Bandwidth                          %        47.56
    L1/TEX Hit Rate                        %         6.96
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.17
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.81
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.19
    Active Warps Per Scheduler          warp         8.90
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.90 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.96
    Warp Cycles Per Executed Instruction           cycle        57.15
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.44%                                                                                          
          On average, each warp of this workload spends 30.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.2% of the total average of 50.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.33
    Issued Instructions                             inst       196813
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.71
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.29%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25890.67
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4798.83
    Total L1 Elapsed Cycles          cycle       405254
    Average L2 Active Cycles         cycle      4308.58
    Total L2 Elapsed Cycles          cycle       172152
    Average SM Active Cycles         cycle      4798.83
    Total SM Elapsed Cycles          cycle       405254
    Average SMSP Active Cycles       cycle      4762.83
    Total SMSP Elapsed Cycles        cycle      1621016
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.30
    Elapsed Cycles                cycle         6893
    Memory Throughput                 %        48.25
    DRAM Throughput                   %        48.25
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.68
    L2 Cache Throughput               %        29.08
    SM Active Cycles              cycle      4685.12
    Compute (SM) Throughput           %        16.47
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.10
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.10
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.96%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.53
    Mem Busy                               %        19.63
    Max Bandwidth                          %        48.25
    L1/TEX Hit Rate                        %         7.66
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.47
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.26
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.74
    Active Warps Per Scheduler          warp         9.07
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.07 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.65
    Warp Cycles Per Executed Instruction           cycle        56.77
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.75%                                                                                          
          On average, each warp of this workload spends 30.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 49.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.97
    Issued Instructions                             inst       196729
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.85
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.15%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25941.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4685.12
    Total L1 Elapsed Cycles          cycle       397902
    Average L2 Active Cycles         cycle      4245.38
    Total L2 Elapsed Cycles          cycle       170568
    Average SM Active Cycles         cycle      4685.12
    Total SM Elapsed Cycles          cycle       397902
    Average SMSP Active Cycles       cycle      4642.81
    Total SMSP Elapsed Cycles        cycle      1591608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.88
    Elapsed Cycles                cycle      7388971
    Memory Throughput                 %        45.87
    DRAM Throughput                   %         0.14
    Duration                         ms         9.09
    L1/TEX Cache Throughput           %        76.83
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4356521.21
    Compute (SM) Throughput           %        45.87
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.46
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.46
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       417.07
    Mem Busy                               %        30.77
    Max Bandwidth                          %        45.87
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.62
    Mem Pipes Busy                         %        45.87
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351264 bank conflicts,  which represent   
          68.56% of the overall 26764881 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.83
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.49%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.53
    Issued Instructions                             inst    509995747
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.34%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78968
    Total DRAM Elapsed Cycles        cycle    340524032
    Average L1 Active Cycles         cycle   4356521.21
    Total L1 Elapsed Cycles          cycle    423208020
    Average L2 Active Cycles         cycle    355458.92
    Total L2 Elapsed Cycles          cycle    179947512
    Average SM Active Cycles         cycle   4356521.21
    Total SM Elapsed Cycles          cycle    423208020
    Average SMSP Active Cycles       cycle   4355853.39
    Total SMSP Elapsed Cycles        cycle   1692832080
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.82%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.89% above the average, while the minimum instance value is 8.21% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.82%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.89% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.60
    Elapsed Cycles                cycle         6776
    Memory Throughput                 %        45.81
    DRAM Throughput                   %        45.81
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.04
    L2 Cache Throughput               %        29.23
    SM Active Cycles              cycle      4512.16
    Compute (SM) Throughput           %        16.75
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.66
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.94
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.94
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.61%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.36
    Mem Busy                               %        20.31
    Max Bandwidth                          %        45.81
    L1/TEX Hit Rate                        %         7.24
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.81
    Mem Pipes Busy                         %        16.75
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.31
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.69
    Active Warps Per Scheduler          warp         8.85
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.19%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.85 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.32
    Warp Cycles Per Executed Instruction           cycle        52.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.19%                                                                                          
          On average, each warp of this workload spends 28.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.3% of the total average of 48.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.55
    Issued Instructions                             inst       187815
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.67
    Achieved Active Warps Per SM           warp        34.40
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.33%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24274.67
    Total DRAM Elapsed Cycles        cycle       317952
    Average L1 Active Cycles         cycle      4512.16
    Total L1 Elapsed Cycles          cycle       391218
    Average L2 Active Cycles         cycle      4119.75
    Total L2 Elapsed Cycles          cycle       167664
    Average SM Active Cycles         cycle      4512.16
    Total SM Elapsed Cycles          cycle       391218
    Average SMSP Active Cycles       cycle      4421.97
    Total SMSP Elapsed Cycles        cycle      1564872
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.93
    Elapsed Cycles                cycle         6977
    Memory Throughput                 %        47.64
    DRAM Throughput                   %        47.64
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.37
    L2 Cache Throughput               %        28.64
    SM Active Cycles              cycle      4814.29
    Compute (SM) Throughput           %        16.26
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.61
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.61
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.2%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.96
    Mem Busy                               %        19.35
    Max Bandwidth                          %        47.64
    L1/TEX Hit Rate                        %         7.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.26
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.81
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.19
    Active Warps Per Scheduler          warp         8.76
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.76 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.21
    Warp Cycles Per Executed Instruction           cycle        56.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.36%                                                                                          
          On average, each warp of this workload spends 29.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.0% of the total average of 49.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.86
    Issued Instructions                             inst       196704
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.46
    Achieved Active Warps Per SM           warp        35.26
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.54%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25936
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4814.29
    Total L1 Elapsed Cycles          cycle       403108
    Average L2 Active Cycles         cycle      4278.29
    Total L2 Elapsed Cycles          cycle       173064
    Average SM Active Cycles         cycle      4814.29
    Total SM Elapsed Cycles          cycle       403108
    Average SMSP Active Cycles       cycle      4760.50
    Total SMSP Elapsed Cycles        cycle      1612432
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.75
    Elapsed Cycles                cycle         6869
    Memory Throughput                 %        48.11
    DRAM Throughput                   %        48.11
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        29.11
    SM Active Cycles              cycle      4697.88
    Compute (SM) Throughput           %        16.52
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.05
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.05
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.98%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.63
    Mem Busy                               %        19.66
    Max Bandwidth                          %        48.11
    L1/TEX Hit Rate                        %         7.14
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.52
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.34
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.66
    Active Warps Per Scheduler          warp         8.78
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.78 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.86
    Warp Cycles Per Executed Instruction           cycle        54.73
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.89%                                                                                          
          On average, each warp of this workload spends 28.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.3% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.92
    Issued Instructions                             inst       196718
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.65
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.35%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25864
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4697.88
    Total L1 Elapsed Cycles          cycle       396670
    Average L2 Active Cycles         cycle      4287.33
    Total L2 Elapsed Cycles          cycle       170304
    Average SM Active Cycles         cycle      4697.88
    Total SM Elapsed Cycles          cycle       396670
    Average SMSP Active Cycles       cycle      4623.65
    Total SMSP Elapsed Cycles        cycle      1586680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.61
    Elapsed Cycles                cycle         6953
    Memory Throughput                 %        47.71
    DRAM Throughput                   %        47.71
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.47
    L2 Cache Throughput               %        28.79
    SM Active Cycles              cycle      4742.81
    Compute (SM) Throughput           %        16.32
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.88
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.07%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.24
    Mem Busy                               %        19.44
    Max Bandwidth                          %        47.71
    L1/TEX Hit Rate                        %         7.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.32
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.61
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.39
    Active Warps Per Scheduler          warp         8.53
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.29%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.53 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.40
    Warp Cycles Per Executed Instruction           cycle        55.36
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.29%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.6% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.18
    Issued Instructions                             inst       196778
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.47
    Achieved Active Warps Per SM           warp        35.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.53%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25893.33
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4742.81
    Total L1 Elapsed Cycles          cycle       401488
    Average L2 Active Cycles         cycle      4181.08
    Total L2 Elapsed Cycles          cycle       172224
    Average SM Active Cycles         cycle      4742.81
    Total SM Elapsed Cycles          cycle       401488
    Average SMSP Active Cycles       cycle      4815.38
    Total SMSP Elapsed Cycles        cycle      1605952
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       802.86
    Elapsed Cycles                cycle      7385878
    Memory Throughput                 %        45.89
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.87
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4354251.57
    Compute (SM) Throughput           %        45.89
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.49
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.49
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       417.07
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.89
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.52
    Mem Pipes Busy                         %        45.89
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680081 bank conflicts,  which represent   
          15.30% of the overall 95969297 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.7%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351394 bank conflicts,  which represent   
          68.56% of the overall 26765001 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.54
    Issued Instructions                             inst    509995749
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78925.33
    Total DRAM Elapsed Cycles        cycle    340337664
    Average L1 Active Cycles         cycle   4354251.57
    Total L1 Elapsed Cycles          cycle    422967972
    Average L2 Active Cycles         cycle    352148.12
    Total L2 Elapsed Cycles          cycle    179849208
    Average SM Active Cycles         cycle   4354251.57
    Total SM Elapsed Cycles          cycle    422967972
    Average SMSP Active Cycles       cycle   4354517.21
    Total SMSP Elapsed Cycles        cycle   1691871888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.97% above the average, while the minimum instance value is 8.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.91% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.97% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       786.06
    Elapsed Cycles                cycle         6801
    Memory Throughput                 %        45.28
    DRAM Throughput                   %        45.28
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.68
    L2 Cache Throughput               %        29.12
    SM Active Cycles              cycle      4579.10
    Compute (SM) Throughput           %        16.70
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.68
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.68
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.75%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.58
    Mem Busy                               %        20.26
    Max Bandwidth                          %        45.28
    L1/TEX Hit Rate                        %         7.83
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.70
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.81
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.19
    Active Warps Per Scheduler          warp         8.36
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.36 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.95
    Warp Cycles Per Executed Instruction           cycle        51.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.72%                                                                                          
          On average, each warp of this workload spends 27.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.9% of the total average of 47.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.52
    Issued Instructions                             inst       187808
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.18
    Achieved Active Warps Per SM           warp        34.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23954.67
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4579.10
    Total L1 Elapsed Cycles          cycle       392452
    Average L2 Active Cycles         cycle      4101.83
    Total L2 Elapsed Cycles          cycle       168288
    Average SM Active Cycles         cycle      4579.10
    Total SM Elapsed Cycles          cycle       392452
    Average SMSP Active Cycles       cycle      4545.56
    Total SMSP Elapsed Cycles        cycle      1569808
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.01
    Elapsed Cycles                cycle         6965
    Memory Throughput                 %        47.41
    DRAM Throughput                   %        47.41
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.42
    L2 Cache Throughput               %        28.73
    SM Active Cycles              cycle      4739.57
    Compute (SM) Throughput           %        16.30
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.89
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.29
    Mem Busy                               %        19.40
    Max Bandwidth                          %        47.41
    L1/TEX Hit Rate                        %         7.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.30
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.33
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.67
    Active Warps Per Scheduler          warp         9.06
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.59%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.06 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.43
    Warp Cycles Per Executed Instruction           cycle        56.51
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.59%                                                                                          
          On average, each warp of this workload spends 30.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.9% of the total average of 49.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.77
    Issued Instructions                             inst       196683
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.14
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25813.33
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4739.57
    Total L1 Elapsed Cycles          cycle       402124
    Average L2 Active Cycles         cycle      4227.33
    Total L2 Elapsed Cycles          cycle       172536
    Average SM Active Cycles         cycle      4739.57
    Total SM Elapsed Cycles          cycle       402124
    Average SMSP Active Cycles       cycle      4624.92
    Total SMSP Elapsed Cycles        cycle      1608496
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       787.12
    Elapsed Cycles                cycle         7061
    Memory Throughput                 %        47.07
    DRAM Throughput                   %        47.07
    Duration                         us         8.93
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        28.35
    SM Active Cycles              cycle      4833.03
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.54
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.54
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.23%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       139.50
    Mem Busy                               %        19.15
    Max Bandwidth                          %        47.07
    L1/TEX Hit Rate                        %         7.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.08
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.48
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.52
    Active Warps Per Scheduler          warp         9.07
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.07 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.09
    Warp Cycles Per Executed Instruction           cycle        56.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.93%                                                                                          
          On average, each warp of this workload spends 29.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.6% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.63
    Issued Instructions                             inst       196650
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.06
    Achieved Active Warps Per SM           warp        34.59
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25946.67
    Total DRAM Elapsed Cycles        cycle       330752
    Average L1 Active Cycles         cycle      4833.03
    Total L1 Elapsed Cycles          cycle       407588
    Average L2 Active Cycles         cycle      4253.50
    Total L2 Elapsed Cycles          cycle       174864
    Average SM Active Cycles         cycle      4833.03
    Total SM Elapsed Cycles          cycle       407588
    Average SMSP Active Cycles       cycle      4587.75
    Total SMSP Elapsed Cycles        cycle      1630352
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.49
    Elapsed Cycles                cycle         7011
    Memory Throughput                 %        47.15
    DRAM Throughput                   %        47.15
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        24.27
    L2 Cache Throughput               %        28.53
    SM Active Cycles              cycle      4733.09
    Compute (SM) Throughput           %        16.19
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.91
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.05%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       139.37
    Mem Busy                               %        19.27
    Max Bandwidth                          %        47.15
    L1/TEX Hit Rate                        %         7.46
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.19
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.92
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.08
    Active Warps Per Scheduler          warp         8.73
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.85%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.73 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.72
    Warp Cycles Per Executed Instruction           cycle        55.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.85%                                                                                          
          On average, each warp of this workload spends 30.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.0% of the total average of 48.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.79
    Issued Instructions                             inst       196688
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        77.00
    Achieved Active Warps Per SM           warp        36.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23%                                                                                             
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (77.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25829.33
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4733.09
    Total L1 Elapsed Cycles          cycle       404770
    Average L2 Active Cycles         cycle      4303.67
    Total L2 Elapsed Cycles          cycle       173712
    Average SM Active Cycles         cycle      4733.09
    Total SM Elapsed Cycles          cycle       404770
    Average SMSP Active Cycles       cycle      4730.67
    Total SMSP Elapsed Cycles        cycle      1619080
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.09
    Elapsed Cycles                cycle      7385466
    Memory Throughput                 %        45.93
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.90
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4352455.95
    Compute (SM) Throughput           %        45.93
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.51
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.51
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       433.01
    Mem Busy                               %        30.81
    Max Bandwidth                          %        45.93
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.51
    Mem Pipes Busy                         %        45.93
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680067 bank conflicts,  which represent   
          15.30% of the overall 95969283 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.72%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351365 bank conflicts,  which represent   
          68.56% of the overall 26764983 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.49
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.51
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.83
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.14
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        81864
    Total DRAM Elapsed Cycles        cycle    340012032
    Average L1 Active Cycles         cycle   4352455.95
    Total L1 Elapsed Cycles          cycle    422687112
    Average L2 Active Cycles         cycle    356576.46
    Total L2 Elapsed Cycles          cycle    179677272
    Average SM Active Cycles         cycle   4352455.95
    Total SM Elapsed Cycles          cycle    422687112
    Average SMSP Active Cycles       cycle   4354110.09
    Total SMSP Elapsed Cycles        cycle   1690748448
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.97% above the average, while the minimum instance value is 8.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.88%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.97% above the average, while the minimum instance value is 8.24% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.00
    Elapsed Cycles                cycle         6891
    Memory Throughput                 %        45.14
    DRAM Throughput                   %        45.14
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        28.74
    SM Active Cycles              cycle      4658.57
    Compute (SM) Throughput           %        16.48
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.38
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.38
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.76
    Mem Busy                               %        19.96
    Max Bandwidth                          %        45.14
    L1/TEX Hit Rate                        %         6.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.84
    Mem Pipes Busy                         %        16.48
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.65
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.35
    Active Warps Per Scheduler          warp         8.79
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.86%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.79 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.78
    Warp Cycles Per Executed Instruction           cycle        54.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.86%                                                                                          
          On average, each warp of this workload spends 27.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 56.1% of the total average of 49.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.66
    Issued Instructions                             inst       187842
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.22
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.78%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24344
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4658.57
    Total L1 Elapsed Cycles          cycle       397752
    Average L2 Active Cycles         cycle      4045.33
    Total L2 Elapsed Cycles          cycle       170640
    Average SM Active Cycles         cycle      4658.57
    Total SM Elapsed Cycles          cycle       397752
    Average SMSP Active Cycles       cycle      4587.73
    Total SMSP Elapsed Cycles        cycle      1591008
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.63
    Elapsed Cycles                cycle         6856
    Memory Throughput                 %        48.24
    DRAM Throughput                   %        48.24
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.80
    L2 Cache Throughput               %        29.13
    SM Active Cycles              cycle      4690.47
    Compute (SM) Throughput           %        16.55
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.07
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.07
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.97%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.57
    Mem Busy                               %        19.68
    Max Bandwidth                          %        48.24
    L1/TEX Hit Rate                        %         7.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.55
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.80
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.20
    Active Warps Per Scheduler          warp         8.51
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.76%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.51 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.77
    Warp Cycles Per Executed Instruction           cycle        54.61
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.76%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 47.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.69
    Issued Instructions                             inst       196665
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.26
    Achieved Active Warps Per SM           warp        36.12
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.74%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25853.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4690.47
    Total L1 Elapsed Cycles          cycle       396102
    Average L2 Active Cycles         cycle      4256.08
    Total L2 Elapsed Cycles          cycle       170112
    Average SM Active Cycles         cycle      4690.47
    Total SM Elapsed Cycles          cycle       396102
    Average SMSP Active Cycles       cycle      4761.00
    Total SMSP Elapsed Cycles        cycle      1584408
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.54
    Elapsed Cycles                cycle         6849
    Memory Throughput                 %        48.25
    DRAM Throughput                   %        48.25
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.74
    L2 Cache Throughput               %        29.09
    SM Active Cycles              cycle      4672.91
    Compute (SM) Throughput           %        16.51
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.15
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.93%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.97
    Mem Busy                               %        19.64
    Max Bandwidth                          %        48.25
    L1/TEX Hit Rate                        %         7.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.51
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.21
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.79
    Active Warps Per Scheduler          warp         8.64
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.75%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.64 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.43
    Warp Cycles Per Executed Instruction           cycle        54.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.75%                                                                                          
          On average, each warp of this workload spends 29.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.3% of the total average of 47.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.31
    Issued Instructions                             inst       196809
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.27
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.73%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     26021.33
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4672.91
    Total L1 Elapsed Cycles          cycle       397002
    Average L2 Active Cycles         cycle      4286.46
    Total L2 Elapsed Cycles          cycle       170472
    Average SM Active Cycles         cycle      4672.91
    Total SM Elapsed Cycles          cycle       397002
    Average SMSP Active Cycles       cycle      4659.38
    Total SMSP Elapsed Cycles        cycle      1588008
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.74
    Elapsed Cycles                cycle         6772
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.14
    L2 Cache Throughput               %        29.58
    SM Active Cycles              cycle      4653.55
    Compute (SM) Throughput           %        16.77
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.23
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.89%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.61
    Mem Busy                               %        19.97
    Max Bandwidth                          %        49.18
    L1/TEX Hit Rate                        %         7.74
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.77
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.30
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.70
    Active Warps Per Scheduler          warp         8.96
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.82%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.96 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.95
    Warp Cycles Per Executed Instruction           cycle        55.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.82%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.3% of the total average of 48.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.13
    Issued Instructions                             inst       196767
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.21
    Achieved Active Warps Per SM           warp        36.10
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25933.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4653.55
    Total L1 Elapsed Cycles          cycle       390786
    Average L2 Active Cycles         cycle      4249.96
    Total L2 Elapsed Cycles          cycle       167616
    Average SM Active Cycles         cycle      4653.55
    Total SM Elapsed Cycles          cycle       390786
    Average SMSP Active Cycles       cycle      4635.83
    Total SMSP Elapsed Cycles        cycle      1563144
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.20
    Elapsed Cycles                cycle      7385981
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355861.36
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       413.58
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.59
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680067 bank conflicts,  which represent   
          15.30% of the overall 95969283 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.68%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351380 bank conflicts,  which represent   
          68.56% of the overall 26764993 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78221.33
    Total DRAM Elapsed Cycles        cycle    340150272
    Average L1 Active Cycles         cycle   4355861.36
    Total L1 Elapsed Cycles          cycle    422914096
    Average L2 Active Cycles         cycle    351181.38
    Total L2 Elapsed Cycles          cycle    179751072
    Average SM Active Cycles         cycle   4355861.36
    Total SM Elapsed Cycles          cycle    422914096
    Average SMSP Active Cycles       cycle   4354712.34
    Total SMSP Elapsed Cycles        cycle   1691656384
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.94% above the average, while the minimum instance value is 8.21% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.94% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.91
    Elapsed Cycles                cycle         6701
    Memory Throughput                 %        45.71
    DRAM Throughput                   %        45.71
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.60
    L2 Cache Throughput               %        29.46
    SM Active Cycles              cycle      4592.34
    Compute (SM) Throughput           %        16.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.63
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.77%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.50
    Mem Busy                               %        20.48
    Max Bandwidth                          %        45.71
    L1/TEX Hit Rate                        %         7.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.94
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.06
    Active Warps Per Scheduler          warp         8.59
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.29%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.59 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.88
    Warp Cycles Per Executed Instruction           cycle        52.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.29%                                                                                          
          On average, each warp of this workload spends 27.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 57.7% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.49
    Issued Instructions                             inst       187801
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.77
    Achieved Active Warps Per SM           warp        33.97
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.23%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4592.34
    Total L1 Elapsed Cycles          cycle       387006
    Average L2 Active Cycles         cycle      4142.79
    Total L2 Elapsed Cycles          cycle       166152
    Average SM Active Cycles         cycle      4592.34
    Total SM Elapsed Cycles          cycle       387006
    Average SMSP Active Cycles       cycle      4512.12
    Total SMSP Elapsed Cycles        cycle      1548024
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.75
    Elapsed Cycles                cycle         7115
    Memory Throughput                 %        46.59
    DRAM Throughput                   %        46.59
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        28.16
    SM Active Cycles              cycle      4867.03
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.43
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.43
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.29%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       137.46
    Mem Busy                               %        19.01
    Max Bandwidth                          %        46.59
    L1/TEX Hit Rate                        %         7.39
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.96
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.86
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.14
    Active Warps Per Scheduler          warp         8.78
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.41%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.78 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.18
    Warp Cycles Per Executed Instruction           cycle        56.25
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.41%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.0% of the total average of 49.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.21
    Issued Instructions                             inst       196784
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.69
    Achieved Active Warps Per SM           warp        34.41
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.31%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25842.67
    Total DRAM Elapsed Cycles        cycle       332800
    Average L1 Active Cycles         cycle      4867.03
    Total L1 Elapsed Cycles          cycle       410730
    Average L2 Active Cycles         cycle      4321.25
    Total L2 Elapsed Cycles          cycle       176112
    Average SM Active Cycles         cycle      4867.03
    Total SM Elapsed Cycles          cycle       410730
    Average SMSP Active Cycles       cycle      4749.99
    Total SMSP Elapsed Cycles        cycle      1642920
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.29
    Elapsed Cycles                cycle         6991
    Memory Throughput                 %        47.39
    DRAM Throughput                   %        47.39
    Duration                         us         8.86
    L1/TEX Cache Throughput           %        24.31
    L2 Cache Throughput               %        28.64
    SM Active Cycles              cycle      4735.84
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.90
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.90
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.05%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.16
    Mem Busy                               %        19.35
    Max Bandwidth                          %        47.39
    L1/TEX Hit Rate                        %         7.75
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.23
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.45
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.55
    Active Warps Per Scheduler          warp         9.07
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.61%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.07 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.19
    Warp Cycles Per Executed Instruction           cycle        56.24
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.61%                                                                                          
          On average, each warp of this workload spends 29.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 49.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.81
    Issued Instructions                             inst       196691
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        35.37
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.3%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25882.67
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4735.84
    Total L1 Elapsed Cycles          cycle       403724
    Average L2 Active Cycles         cycle      4283.79
    Total L2 Elapsed Cycles          cycle       173064
    Average SM Active Cycles         cycle      4735.84
    Total SM Elapsed Cycles          cycle       403724
    Average SMSP Active Cycles       cycle      4595.50
    Total SMSP Elapsed Cycles        cycle      1614896
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.20
    Elapsed Cycles                cycle         6932
    Memory Throughput                 %        47.79
    DRAM Throughput                   %        47.79
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.54
    L2 Cache Throughput               %        28.88
    SM Active Cycles              cycle      4702.02
    Compute (SM) Throughput           %        16.37
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.03
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.03
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.99%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.48
    Mem Busy                               %        19.49
    Max Bandwidth                          %        47.79
    L1/TEX Hit Rate                        %         7.08
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.37
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.43
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.57
    Active Warps Per Scheduler          warp         8.87
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.21%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.87 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.15
    Warp Cycles Per Executed Instruction           cycle        55.06
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.21%                                                                                          
          On average, each warp of this workload spends 30.3 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.0% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.94
    Issued Instructions                             inst       196722
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.47
    Achieved Active Warps Per SM           warp        35.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.53%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25938.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4702.02
    Total L1 Elapsed Cycles          cycle       400252
    Average L2 Active Cycles         cycle      4285.79
    Total L2 Elapsed Cycles          cycle       171792
    Average SM Active Cycles         cycle      4702.02
    Total SM Elapsed Cycles          cycle       400252
    Average SMSP Active Cycles       cycle      4600.44
    Total SMSP Elapsed Cycles        cycle      1601008
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.21
    Elapsed Cycles                cycle      7384191
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.86
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4354564.26
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.48
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       431.32
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.55
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680066 bank conflicts,  which represent   
          15.30% of the overall 95969282 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.7%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351398 bank conflicts,  which represent   
          68.56% of the overall 26765034 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.55
    Issued Instructions                             inst    509995752
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.31%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        81576
    Total DRAM Elapsed Cycles        cycle    340143104
    Average L1 Active Cycles         cycle   4354564.26
    Total L1 Elapsed Cycles          cycle    422908888
    Average L2 Active Cycles         cycle    353921.12
    Total L2 Elapsed Cycles          cycle    179746104
    Average SM Active Cycles         cycle   4354564.26
    Total SM Elapsed Cycles          cycle    422908888
    Average SMSP Active Cycles       cycle   4355102.93
    Total SMSP Elapsed Cycles        cycle   1691635552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.91% above the average, while the minimum instance value is 8.21% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.91% above the average, while the minimum instance value is 8.21% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.72
    Elapsed Cycles                cycle         6690
    Memory Throughput                 %        45.94
    DRAM Throughput                   %        45.94
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.22
    L2 Cache Throughput               %        29.38
    SM Active Cycles              cycle      4479.97
    Compute (SM) Throughput           %        16.90
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.66
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.07
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.07
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.54%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.66
    Mem Busy                               %        20.41
    Max Bandwidth                          %        45.94
    L1/TEX Hit Rate                        %         6.79
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        51.01
    Mem Pipes Busy                         %        16.90
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.42
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.58
    Active Warps Per Scheduler          warp         8.83
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.83 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.96
    Warp Cycles Per Executed Instruction           cycle        52.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.06%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 58.3% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.34
    Issued Instructions                             inst       187766
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.41%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24146.67
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4479.97
    Total L1 Elapsed Cycles          cycle       387876
    Average L2 Active Cycles         cycle      4116.92
    Total L2 Elapsed Cycles          cycle       166608
    Average SM Active Cycles         cycle      4479.97
    Total SM Elapsed Cycles          cycle       387876
    Average SMSP Active Cycles       cycle      4394.60
    Total SMSP Elapsed Cycles        cycle      1551504
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       794.33
    Elapsed Cycles                cycle         6930
    Memory Throughput                 %        48.82
    DRAM Throughput                   %        48.82
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.68
    L2 Cache Throughput               %        29.43
    SM Active Cycles              cycle      4745.81
    Compute (SM) Throughput           %        16.46
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.86
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.86
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.07%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.97
    Mem Busy                               %        19.88
    Max Bandwidth                          %        48.82
    L1/TEX Hit Rate                        %         7.14
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.46
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.25
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.75
    Active Warps Per Scheduler          warp         8.74
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.18%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.74 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.89
    Warp Cycles Per Executed Instruction           cycle        54.76
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.18%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.83
    Issued Instructions                             inst       196696
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.62
    Achieved Active Warps Per SM           warp        35.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.38%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25914.67
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4745.81
    Total L1 Elapsed Cycles          cycle       398052
    Average L2 Active Cycles         cycle      4233.62
    Total L2 Elapsed Cycles          cycle       168408
    Average SM Active Cycles         cycle      4745.81
    Total SM Elapsed Cycles          cycle       398052
    Average SMSP Active Cycles       cycle      4645.12
    Total SMSP Elapsed Cycles        cycle      1592208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       793.29
    Elapsed Cycles                cycle         6879
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        24.79
    L2 Cache Throughput               %        29.50
    SM Active Cycles              cycle      4735.90
    Compute (SM) Throughput           %        16.55
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.91
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.91
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.05%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       144.52
    Mem Busy                               %        19.93
    Max Bandwidth                          %        48.99
    L1/TEX Hit Rate                        %         7.93
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.55
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.80
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.20
    Active Warps Per Scheduler          warp         8.76
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.76 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.20
    Warp Cycles Per Executed Instruction           cycle        56.26
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.01%                                                                                          
          On average, each warp of this workload spends 29.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 49.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.98
    Issued Instructions                             inst       196731
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.89
    Achieved Active Warps Per SM           warp        35.95
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.11%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25917.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4735.90
    Total L1 Elapsed Cycles          cycle       396048
    Average L2 Active Cycles         cycle      4217.83
    Total L2 Elapsed Cycles          cycle       167976
    Average SM Active Cycles         cycle      4735.90
    Total SM Elapsed Cycles          cycle       396048
    Average SMSP Active Cycles       cycle      4764.05
    Total SMSP Elapsed Cycles        cycle      1584192
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       814.89
    Elapsed Cycles                cycle         7182
    Memory Throughput                 %        48.03
    DRAM Throughput                   %        48.03
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        29.12
    SM Active Cycles              cycle      4919.52
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.60
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.23
    Issued Ipc Active     inst/cycle         0.69
    SM Busy                        %        17.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.39%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.40
    Mem Busy                               %        19.65
    Max Bandwidth                          %        48.03
    L1/TEX Hit Rate                        %         7.11
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.79
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.21
    Active Warps Per Scheduler          warp         8.49
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.49 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.73
    Warp Cycles Per Executed Instruction           cycle        54.57
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.97%                                                                                          
          On average, each warp of this workload spends 30.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.7% of the total average of 47.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.81
    Issued Instructions                             inst       196693
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.90
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.1%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25821.33
    Total DRAM Elapsed Cycles        cycle       322560
    Average L1 Active Cycles         cycle      4919.52
    Total L1 Elapsed Cycles          cycle       411370
    Average L2 Active Cycles         cycle      4308.33
    Total L2 Elapsed Cycles          cycle       170352
    Average SM Active Cycles         cycle      4919.52
    Total SM Elapsed Cycles          cycle       411370
    Average SMSP Active Cycles       cycle      4764.90
    Total SMSP Elapsed Cycles        cycle      1645480
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.35
    Elapsed Cycles                cycle      7383075
    Memory Throughput                 %        45.91
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.88
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4353585.93
    Compute (SM) Throughput           %        45.91
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.49
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.49
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       426.60
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.91
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.47
    Mem Pipes Busy                         %        45.91
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.71%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351217 bank conflicts,  which represent   
          68.56% of the overall 26764837 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.59
    Issued Instructions                             inst    509995761
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.15
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.27%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80645.33
    Total DRAM Elapsed Cycles        cycle    339982336
    Average L1 Active Cycles         cycle   4353585.93
    Total L1 Elapsed Cycles          cycle    422789832
    Average L2 Active Cycles         cycle    356658.17
    Total L2 Elapsed Cycles          cycle    179661984
    Average SM Active Cycles         cycle   4353585.93
    Total SM Elapsed Cycles          cycle    422789832
    Average SMSP Active Cycles       cycle   4354618.34
    Total SMSP Elapsed Cycles        cycle   1691159328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.95% above the average, while the minimum instance value is 8.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.96% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.95% above the average, while the minimum instance value is 8.24% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.07%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       793.19
    Elapsed Cycles                cycle         6597
    Memory Throughput                 %        46.98
    DRAM Throughput                   %        46.98
    Duration                         us         8.26
    L1/TEX Cache Throughput           %        25.57
    L2 Cache Throughput               %        30.39
    SM Active Cycles              cycle      4419.69
    Compute (SM) Throughput           %        17.25
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.67
    Executed Ipc Elapsed  inst/cycle         0.45
    Issue Slots Busy               %        18.32
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.32
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.41%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       138.91
    Mem Busy                               %        21.13
    Max Bandwidth                          %        46.98
    L1/TEX Hit Rate                        %         7.04
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        17.25
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.67
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.33
    Active Warps Per Scheduler          warp         8.31
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.02%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.31 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.00
    Warp Cycles Per Executed Instruction           cycle        51.32
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.02%                                                                                          
          On average, each warp of this workload spends 27.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.2% of the total average of 47.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.59
    Issued Instructions                             inst       187824
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.40
    Achieved Active Warps Per SM           warp        35.71
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.6%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23893.33
    Total DRAM Elapsed Cycles        cycle       305152
    Average L1 Active Cycles         cycle      4419.69
    Total L1 Elapsed Cycles          cycle       379816
    Average L2 Active Cycles         cycle      4050.21
    Total L2 Elapsed Cycles          cycle       161304
    Average SM Active Cycles         cycle      4419.69
    Total SM Elapsed Cycles          cycle       379816
    Average SMSP Active Cycles       cycle      4581.62
    Total SMSP Elapsed Cycles        cycle      1519264
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       786.28
    Elapsed Cycles                cycle         6977
    Memory Throughput                 %        47.56
    DRAM Throughput                   %        47.56
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.38
    L2 Cache Throughput               %        28.71
    SM Active Cycles              cycle      4645.84
    Compute (SM) Throughput           %        16.27
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.25
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.88%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.72
    Mem Busy                               %        19.39
    Max Bandwidth                          %        47.56
    L1/TEX Hit Rate                        %         7.61
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.27
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.92
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.08
    Active Warps Per Scheduler          warp         8.67
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.67 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.40
    Warp Cycles Per Executed Instruction           cycle        55.34
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.44%                                                                                          
          On average, each warp of this workload spends 29.5 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.8% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.84
    Issued Instructions                             inst       196698
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.88
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.12%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25893.33
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4645.84
    Total L1 Elapsed Cycles          cycle       402774
    Average L2 Active Cycles         cycle      4231.88
    Total L2 Elapsed Cycles          cycle       172632
    Average SM Active Cycles         cycle      4645.84
    Total SM Elapsed Cycles          cycle       402774
    Average SMSP Active Cycles       cycle      4730.94
    Total SMSP Elapsed Cycles        cycle      1611096
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.07
    Elapsed Cycles                cycle         7017
    Memory Throughput                 %        47.36
    DRAM Throughput                   %        47.36
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        28.52
    SM Active Cycles              cycle      4778.83
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.74
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.74
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.13%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s          140
    Mem Busy                               %        19.26
    Max Bandwidth                          %        47.36
    L1/TEX Hit Rate                        %         6.83
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.98
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.02
    Active Warps Per Scheduler          warp         8.82
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.64%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.82 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.05
    Warp Cycles Per Executed Instruction           cycle        56.09
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.64%                                                                                          
          On average, each warp of this workload spends 30.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.1% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.99
    Issued Instructions                             inst       196734
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.40
    Achieved Active Warps Per SM           warp        34.75
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.6%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25946.67
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4778.83
    Total L1 Elapsed Cycles          cycle       405070
    Average L2 Active Cycles         cycle      4272.58
    Total L2 Elapsed Cycles          cycle       173832
    Average SM Active Cycles         cycle      4778.83
    Total SM Elapsed Cycles          cycle       405070
    Average SMSP Active Cycles       cycle      4716.66
    Total SMSP Elapsed Cycles        cycle      1620280
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.00
    Elapsed Cycles                cycle         7016
    Memory Throughput                 %        47.30
    DRAM Throughput                   %        47.30
    Duration                         us         8.90
    L1/TEX Cache Throughput           %        24.25
    L2 Cache Throughput               %        28.56
    SM Active Cycles              cycle      4719.26
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.97
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.97
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.02%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       139.81
    Mem Busy                               %        19.28
    Max Bandwidth                          %        47.30
    L1/TEX Hit Rate                        %         7.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.35
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.65
    Active Warps Per Scheduler          warp         8.82
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.7%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.82 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.08
    Warp Cycles Per Executed Instruction           cycle        54.98
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.7%                                                                                           
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.1% of the total average of 48.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.98
    Issued Instructions                             inst       196732
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25912
    Total DRAM Elapsed Cycles        cycle       328704
    Average L1 Active Cycles         cycle      4719.26
    Total L1 Elapsed Cycles          cycle       405034
    Average L2 Active Cycles         cycle      4222.88
    Total L2 Elapsed Cycles          cycle       173664
    Average SM Active Cycles         cycle      4719.26
    Total SM Elapsed Cycles          cycle       405034
    Average SMSP Active Cycles       cycle      4620.90
    Total SMSP Elapsed Cycles        cycle      1620136
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.53
    Elapsed Cycles                cycle      7392756
    Memory Throughput                 %        45.87
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.90
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4352430.97
    Compute (SM) Throughput           %        45.87
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.51
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.51
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       431.32
    Mem Busy                               %        30.77
    Max Bandwidth                          %        45.87
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.51
    Mem Pipes Busy                         %        45.87
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680067 bank conflicts,  which represent   
          15.30% of the overall 95969283 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.72%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351267 bank conflicts,  which represent   
          68.56% of the overall 26764889 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995754
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.14
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.29%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        81592
    Total DRAM Elapsed Cycles        cycle    340216832
    Average L1 Active Cycles         cycle   4352430.97
    Total L1 Elapsed Cycles          cycle    423167498
    Average L2 Active Cycles         cycle    355121.54
    Total L2 Elapsed Cycles          cycle    179785248
    Average SM Active Cycles         cycle   4352430.97
    Total SM Elapsed Cycles          cycle    423167498
    Average SMSP Active Cycles       cycle   4355677.35
    Total SMSP Elapsed Cycles        cycle   1692669992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 40.00% above the average, while the minimum instance value is 8.26% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 40.00% above the average, while the minimum instance value is 8.26% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.05%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.86
    Elapsed Cycles                cycle         6829
    Memory Throughput                 %        45.24
    DRAM Throughput                   %        45.24
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.55
    L2 Cache Throughput               %        29.01
    SM Active Cycles              cycle      4602.47
    Compute (SM) Throughput           %        16.62
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.59
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.59
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.79%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.76
    Mem Busy                               %        20.17
    Max Bandwidth                          %        45.24
    L1/TEX Hit Rate                        %         7.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.62
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.85
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.15
    Active Warps Per Scheduler          warp         8.43
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.76%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.43 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.21
    Warp Cycles Per Executed Instruction           cycle        51.56
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.76%                                                                                          
          On average, each warp of this workload spends 28.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.7% of the total average of 47.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.72
    Issued Instructions                             inst       187854
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.84
    Achieved Active Warps Per SM           warp        34.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.16%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24165.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4602.47
    Total L1 Elapsed Cycles          cycle       394258
    Average L2 Active Cycles         cycle      4119.12
    Total L2 Elapsed Cycles          cycle       169056
    Average SM Active Cycles         cycle      4602.47
    Total SM Elapsed Cycles          cycle       394258
    Average SMSP Active Cycles       cycle      4536.59
    Total SMSP Elapsed Cycles        cycle      1577032
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       791.08
    Elapsed Cycles                cycle         6759
    Memory Throughput                 %        49.61
    DRAM Throughput                   %        49.61
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.24
    L2 Cache Throughput               %        29.98
    SM Active Cycles              cycle      4625.10
    Compute (SM) Throughput           %        16.84
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.33
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.33
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.84%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       146.64
    Mem Busy                               %        20.24
    Max Bandwidth                          %        49.61
    L1/TEX Hit Rate                        %         7.68
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.84
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.73
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.27
    Active Warps Per Scheduler          warp         8.44
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.44 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.59
    Warp Cycles Per Executed Instruction           cycle        54.42
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.39%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 47.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.91
    Issued Instructions                             inst       196716
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.01
    Achieved Active Warps Per SM           warp        36.48
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.99%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25906.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4625.10
    Total L1 Elapsed Cycles          cycle       389080
    Average L2 Active Cycles         cycle      4153.29
    Total L2 Elapsed Cycles          cycle       165384
    Average SM Active Cycles         cycle      4625.10
    Total SM Elapsed Cycles          cycle       389080
    Average SMSP Active Cycles       cycle      4782.50
    Total SMSP Elapsed Cycles        cycle      1556320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.15
    Elapsed Cycles                cycle         6897
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.66
    L2 Cache Throughput               %        29.02
    SM Active Cycles              cycle      4729.55
    Compute (SM) Throughput           %        16.45
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.93
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.30
    Mem Busy                               %        19.60
    Max Bandwidth                          %        48.02
    L1/TEX Hit Rate                        %         7.54
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.52
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.48
    Active Warps Per Scheduler          warp         8.51
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.98%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.51 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.54
    Warp Cycles Per Executed Instruction           cycle        55.51
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.98%                                                                                          
          On average, each warp of this workload spends 30.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 62.6% of the total average of 48.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.97
    Issued Instructions                             inst       196730
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.81
    Achieved Active Warps Per SM           warp        35.91
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.19%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25898.67
    Total DRAM Elapsed Cycles        cycle       323584
    Average L1 Active Cycles         cycle      4729.55
    Total L1 Elapsed Cycles          cycle       398330
    Average L2 Active Cycles         cycle      4286.62
    Total L2 Elapsed Cycles          cycle       170808
    Average SM Active Cycles         cycle      4729.55
    Total SM Elapsed Cycles          cycle       398330
    Average SMSP Active Cycles       cycle      4838.99
    Total SMSP Elapsed Cycles        cycle      1593320
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.39
    Elapsed Cycles                cycle         6885
    Memory Throughput                 %        47.94
    DRAM Throughput                   %        47.94
    Duration                         us         8.74
    L1/TEX Cache Throughput           %        24.69
    L2 Cache Throughput               %        29.05
    SM Active Cycles              cycle      4741.88
    Compute (SM) Throughput           %        16.47
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.88
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.51
    Mem Busy                               %        19.62
    Max Bandwidth                          %        47.94
    L1/TEX Hit Rate                        %         6.91
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.47
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.12
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.88
    Active Warps Per Scheduler          warp         8.78
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.78 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.47
    Warp Cycles Per Executed Instruction           cycle        55.42
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.06%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.6% of the total average of 48.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.85
    Issued Instructions                             inst       196702
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.18
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25936
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4741.88
    Total L1 Elapsed Cycles          cycle       397946
    Average L2 Active Cycles         cycle      4242.17
    Total L2 Elapsed Cycles          cycle       170616
    Average SM Active Cycles         cycle      4741.88
    Total SM Elapsed Cycles          cycle       397946
    Average SMSP Active Cycles       cycle      4680.27
    Total SMSP Elapsed Cycles        cycle      1591784
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.50
    Elapsed Cycles                cycle      7386305
    Memory Throughput                 %        45.90
    DRAM Throughput                   %         0.14
    Duration                         ms         9.08
    L1/TEX Cache Throughput           %        76.89
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4352804.93
    Compute (SM) Throughput           %        45.90
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.50
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.50
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       414.04
    Mem Busy                               %        30.79
    Max Bandwidth                          %        45.90
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.71
    Mem Pipes Busy                         %        45.90
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680072 bank conflicts,  which represent   
          15.30% of the overall 95969288 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.72%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351465 bank conflicts,  which represent   
          68.56% of the overall 26765073 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.51%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.57
    Issued Instructions                             inst    509995757
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.14
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.29%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78282.67
    Total DRAM Elapsed Cycles        cycle    340034560
    Average L1 Active Cycles         cycle   4352804.93
    Total L1 Elapsed Cycles          cycle    422927940
    Average L2 Active Cycles         cycle    352358.17
    Total L2 Elapsed Cycles          cycle    179689176
    Average SM Active Cycles         cycle   4352804.93
    Total SM Elapsed Cycles          cycle    422927940
    Average SMSP Active Cycles       cycle   4354859.43
    Total SMSP Elapsed Cycles        cycle   1691711760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.98% above the average, while the minimum instance value is 8.24% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.98% above the average, while the minimum instance value is 8.24% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       799.48
    Elapsed Cycles                cycle         6815
    Memory Throughput                 %        46.27
    DRAM Throughput                   %        46.27
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        24.52
    L2 Cache Throughput               %        29.94
    SM Active Cycles              cycle      4608.69
    Compute (SM) Throughput           %        16.79
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.57
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.57
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.81%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       136.46
    Mem Busy                               %        20.79
    Max Bandwidth                          %        46.27
    L1/TEX Hit Rate                        %         7.50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.79
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.83
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.17
    Active Warps Per Scheduler          warp         8.49
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.73%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.49 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.64
    Warp Cycles Per Executed Instruction           cycle        52.01
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.73%                                                                                          
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.0% of the total average of 47.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.60
    Issued Instructions                             inst       187828
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.25
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23925.33
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4608.69
    Total L1 Elapsed Cycles          cycle       390254
    Average L2 Active Cycles         cycle      4084.38
    Total L2 Elapsed Cycles          cycle       163728
    Average SM Active Cycles         cycle      4608.69
    Total SM Elapsed Cycles          cycle       390254
    Average SMSP Active Cycles       cycle      4541.55
    Total SMSP Elapsed Cycles        cycle      1561016
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       793.66
    Elapsed Cycles                cycle         7037
    Memory Throughput                 %        47.74
    DRAM Throughput                   %        47.74
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.24
    L2 Cache Throughput               %        28.83
    SM Active Cycles              cycle      4807.05
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.65
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.19%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.77
    Mem Busy                               %        19.45
    Max Bandwidth                          %        47.74
    L1/TEX Hit Rate                        %         7.20
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.18
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.10
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.90
    Active Warps Per Scheduler          warp         9.01
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.01 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.79
    Warp Cycles Per Executed Instruction           cycle        56.96
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.26%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.9% of the total average of 49.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.24
    Issued Instructions                             inst       196792
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.12%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25992
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4807.05
    Total L1 Elapsed Cycles          cycle       405084
    Average L2 Active Cycles         cycle      4230.12
    Total L2 Elapsed Cycles          cycle       172104
    Average SM Active Cycles         cycle      4807.05
    Total SM Elapsed Cycles          cycle       405084
    Average SMSP Active Cycles       cycle      4685.93
    Total SMSP Elapsed Cycles        cycle      1620336
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       795.39
    Elapsed Cycles                cycle         7040
    Memory Throughput                 %        47.75
    DRAM Throughput                   %        47.75
    Duration                         us         8.77
    L1/TEX Cache Throughput           %        24.27
    L2 Cache Throughput               %        28.96
    SM Active Cycles              cycle      4761.14
    Compute (SM) Throughput           %        16.20
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.81
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.81
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.1%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.43
    Mem Busy                               %        19.56
    Max Bandwidth                          %        47.75
    L1/TEX Hit Rate                        %         7.56
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.20
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.28
    Issued Warp Per Scheduler                        0.17
    No Eligible                            %        82.72
    Active Warps Per Scheduler          warp         8.46
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.25%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.46 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.97
    Warp Cycles Per Executed Instruction           cycle        55.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.25%                                                                                          
          On average, each warp of this workload spends 31.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.4% of the total average of 49.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.84
    Issued Instructions                             inst       196699
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.96
    Achieved Active Warps Per SM           warp        35.50
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.04%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25834.67
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4761.14
    Total L1 Elapsed Cycles          cycle       404496
    Average L2 Active Cycles         cycle      4282.08
    Total L2 Elapsed Cycles          cycle       171168
    Average SM Active Cycles         cycle      4761.14
    Total SM Elapsed Cycles          cycle       404496
    Average SMSP Active Cycles       cycle      4906.86
    Total SMSP Elapsed Cycles        cycle      1617984
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.23
    Elapsed Cycles                cycle         6943
    Memory Throughput                 %        47.63
    DRAM Throughput                   %        47.63
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.50
    L2 Cache Throughput               %        28.83
    SM Active Cycles              cycle      4696.07
    Compute (SM) Throughput           %        16.35
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.06
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.06
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.98%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.00
    Mem Busy                               %        19.47
    Max Bandwidth                          %        47.63
    L1/TEX Hit Rate                        %         7.45
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.35
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.94
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.06
    Active Warps Per Scheduler          warp         8.81
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.37%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.81 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.13
    Warp Cycles Per Executed Instruction           cycle        56.20
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.37%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.1% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.23
    Issued Instructions                             inst       196789
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.98
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.11%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25850.67
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4696.07
    Total L1 Elapsed Cycles          cycle       400782
    Average L2 Active Cycles         cycle      4283.58
    Total L2 Elapsed Cycles          cycle       171912
    Average SM Active Cycles         cycle      4696.07
    Total SM Elapsed Cycles          cycle       400782
    Average SMSP Active Cycles       cycle      4728.59
    Total SMSP Elapsed Cycles        cycle      1603128
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.65
    Elapsed Cycles                cycle      7382464
    Memory Throughput                 %        45.91
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.84
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4355480.29
    Compute (SM) Throughput           %        45.91
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.90
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.47
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.47
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       432.31
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.91
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.51
    Mem Pipes Busy                         %        45.91
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680070 bank conflicts,  which represent   
          15.30% of the overall 95969286 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.69%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351259 bank conflicts,  which represent   
          68.56% of the overall 26764880 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.53
    Issued Instructions                             inst    509995747
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.11
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.33%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     81701.33
    Total DRAM Elapsed Cycles        cycle    339887104
    Average L1 Active Cycles         cycle   4355480.29
    Total L1 Elapsed Cycles          cycle    422817606
    Average L2 Active Cycles         cycle    357746.42
    Total L2 Elapsed Cycles          cycle    179611824
    Average SM Active Cycles         cycle   4355480.29
    Total SM Elapsed Cycles          cycle    422817606
    Average SMSP Active Cycles       cycle   4354792.78
    Total SMSP Elapsed Cycles        cycle   1691270424
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.87%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.95% above the average, while the minimum instance value is 8.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.84%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.90% above the average, while the minimum instance value is 8.26% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.87%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.95% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.08%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.82
    Elapsed Cycles                cycle         6852
    Memory Throughput                 %        45.18
    DRAM Throughput                   %        45.18
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.62
    L2 Cache Throughput               %        28.88
    SM Active Cycles              cycle      4589.78
    Compute (SM) Throughput           %        16.56
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.64
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.64
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.77%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.53
    Mem Busy                               %        20.14
    Max Bandwidth                          %        45.18
    L1/TEX Hit Rate                        %         7.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.56
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.34
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.66
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.35
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.82%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.35 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.48
    Warp Cycles Per Executed Instruction           cycle        51.84
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.82%                                                                                          
          On average, each warp of this workload spends 28.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.1% of the total average of 47.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.59
    Issued Instructions                             inst       187825
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.35
    Achieved Active Warps Per SM           warp        34.25
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.65%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24213.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4589.78
    Total L1 Elapsed Cycles          cycle       395698
    Average L2 Active Cycles         cycle      4142.71
    Total L2 Elapsed Cycles          cycle       169704
    Average SM Active Cycles         cycle      4589.78
    Total SM Elapsed Cycles          cycle       395698
    Average SMSP Active Cycles       cycle      4414.78
    Total SMSP Elapsed Cycles        cycle      1582792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.59
    Elapsed Cycles                cycle         6869
    Memory Throughput                 %        48.48
    DRAM Throughput                   %        48.48
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.77
    L2 Cache Throughput               %        29.11
    SM Active Cycles              cycle      4752.93
    Compute (SM) Throughput           %        16.53
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.84
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.08%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.82
    Mem Busy                               %        19.68
    Max Bandwidth                          %        48.48
    L1/TEX Hit Rate                        %         7.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.53
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.46
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.54
    Active Warps Per Scheduler          warp         9.09
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.09 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.26
    Warp Cycles Per Executed Instruction           cycle        56.34
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.52%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.8% of the total average of 49.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.15
    Issued Instructions                             inst       196771
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.81
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.19%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25898.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4752.93
    Total L1 Elapsed Cycles          cycle       396586
    Average L2 Active Cycles         cycle      4228.67
    Total L2 Elapsed Cycles          cycle       170136
    Average SM Active Cycles         cycle      4752.93
    Total SM Elapsed Cycles          cycle       396586
    Average SMSP Active Cycles       cycle      4595.72
    Total SMSP Elapsed Cycles        cycle      1586344
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.36
    Elapsed Cycles                cycle         6732
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.26
    L2 Cache Throughput               %        29.76
    SM Active Cycles              cycle      4609.48
    Compute (SM) Throughput           %        16.86
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.39
    Issued Ipc Active     inst/cycle         0.74
    SM Busy                        %        18.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.81%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       146.33
    Mem Busy                               %        20.09
    Max Bandwidth                          %        49.37
    L1/TEX Hit Rate                        %         7.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.86
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.08
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.92
    Active Warps Per Scheduler          warp         8.66
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 50.63%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.66 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.88
    Warp Cycles Per Executed Instruction           cycle        54.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 50.63%                                                                                          
          On average, each warp of this workload spends 29.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.6% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.85
    Issued Instructions                             inst       196702
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.29
    Achieved Active Warps Per SM           warp        35.66
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.71%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25949.33
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4609.48
    Total L1 Elapsed Cycles          cycle       388714
    Average L2 Active Cycles         cycle         4308
    Total L2 Elapsed Cycles          cycle       166680
    Average SM Active Cycles         cycle      4609.48
    Total SM Elapsed Cycles          cycle       388714
    Average SMSP Active Cycles       cycle      4688.44
    Total SMSP Elapsed Cycles        cycle      1554856
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       800.97
    Elapsed Cycles                cycle         7059
    Memory Throughput                 %        48.17
    DRAM Throughput                   %        48.17
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        24.29
    L2 Cache Throughput               %        29.20
    SM Active Cycles              cycle      4811.36
    Compute (SM) Throughput           %        16.21
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.62
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.62
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.19%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.37
    Mem Busy                               %        19.72
    Max Bandwidth                          %        48.17
    L1/TEX Hit Rate                        %         7.13
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.21
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.12
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.88
    Active Warps Per Scheduler          warp         8.81
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.83%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.81 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.64
    Warp Cycles Per Executed Instruction           cycle        55.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.83%                                                                                          
          On average, each warp of this workload spends 29.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.5% of the total average of 48.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.64
    Issued Instructions                             inst       196652
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.85
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.15%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25816
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4811.36
    Total L1 Elapsed Cycles          cycle       404348
    Average L2 Active Cycles         cycle      4277.88
    Total L2 Elapsed Cycles          cycle       169752
    Average SM Active Cycles         cycle      4811.36
    Total SM Elapsed Cycles          cycle       404348
    Average SMSP Active Cycles       cycle      4677.49
    Total SMSP Elapsed Cycles        cycle      1617392
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.71
    Elapsed Cycles                cycle      7381785
    Memory Throughput                 %        45.92
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.81
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4357120.95
    Compute (SM) Throughput           %        45.92
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.45
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.45
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       418.94
    Mem Busy                               %        30.80
    Max Bandwidth                          %        45.92
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.50
    Mem Pipes Busy                         %        45.92
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.75%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.67%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351384 bank conflicts,  which represent   
          68.56% of the overall 26764999 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.52%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.10
    Achieved Active Warps Per SM           warp        17.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.35%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        79160
    Total DRAM Elapsed Cycles        cycle    339827712
    Average L1 Active Cycles         cycle   4357120.95
    Total L1 Elapsed Cycles          cycle    422776246
    Average L2 Active Cycles         cycle    350737.58
    Total L2 Elapsed Cycles          cycle    179579808
    Average SM Active Cycles         cycle   4357120.95
    Total SM Elapsed Cycles          cycle    422776246
    Average SMSP Active Cycles       cycle   4354852.63
    Total SMSP Elapsed Cycles        cycle   1691104984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.90% above the average, while the minimum instance value is 8.20% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.90% above the average, while the minimum instance value is 8.20% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.09%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.14
    Elapsed Cycles                cycle         6806
    Memory Throughput                 %        45.05
    DRAM Throughput                   %        45.05
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.58
    L2 Cache Throughput               %        29.07
    SM Active Cycles              cycle      4596.62
    Compute (SM) Throughput           %        16.68
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.61
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.61
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.78%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       133.27
    Mem Busy                               %        20.12
    Max Bandwidth                          %        45.05
    L1/TEX Hit Rate                        %         7.51
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.68
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.97
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.03
    Active Warps Per Scheduler          warp         8.72
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.72 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.50
    Warp Cycles Per Executed Instruction           cycle        52.94
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.95%                                                                                          
          On average, each warp of this workload spends 27.2 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 56.2% of the total average of 48.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.47
    Issued Instructions                             inst       187796
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.87
    Achieved Active Warps Per SM           warp        34.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.13%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23989.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4596.62
    Total L1 Elapsed Cycles          cycle       392946
    Average L2 Active Cycles         cycle      4140.54
    Total L2 Elapsed Cycles          cycle       168456
    Average SM Active Cycles         cycle      4596.62
    Total SM Elapsed Cycles          cycle       392946
    Average SMSP Active Cycles       cycle      4503.91
    Total SMSP Elapsed Cycles        cycle      1571784
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       787.64
    Elapsed Cycles                cycle         6863
    Memory Throughput                 %        48.16
    DRAM Throughput                   %        48.16
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.79
    L2 Cache Throughput               %        29.22
    SM Active Cycles              cycle      4648.59
    Compute (SM) Throughput           %        16.54
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.25
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.25
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.88%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       142.86
    Mem Busy                               %        19.71
    Max Bandwidth                          %        48.16
    L1/TEX Hit Rate                        %         8.09
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.54
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.18
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.82
    Active Warps Per Scheduler          warp         8.72
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.84%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.72 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.95
    Warp Cycles Per Executed Instruction           cycle        54.85
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.84%                                                                                          
          On average, each warp of this workload spends 28.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 59.9% of the total average of 48.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.28
    Issued Instructions                             inst       196801
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.11
    Achieved Active Warps Per SM           warp        36.53
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.89%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25810.67
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4648.59
    Total L1 Elapsed Cycles          cycle       396158
    Average L2 Active Cycles         cycle      4235.71
    Total L2 Elapsed Cycles          cycle       169824
    Average SM Active Cycles         cycle      4648.59
    Total SM Elapsed Cycles          cycle       396158
    Average SMSP Active Cycles       cycle      4665.59
    Total SMSP Elapsed Cycles        cycle      1584632
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       796.73
    Elapsed Cycles                cycle         6970
    Memory Throughput                 %        48.45
    DRAM Throughput                   %        48.45
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.51
    L2 Cache Throughput               %        29.22
    SM Active Cycles              cycle      4711.09
    Compute (SM) Throughput           %        16.35
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.00
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.00
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.26
    Mem Busy                               %        19.75
    Max Bandwidth                          %        48.45
    L1/TEX Hit Rate                        %         7.48
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.35
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.18
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.82
    Active Warps Per Scheduler          warp         8.79
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.79 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.37
    Warp Cycles Per Executed Instruction           cycle        55.30
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.55%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.7% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.78
    Issued Instructions                             inst       196686
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.05
    Achieved Active Warps Per SM           warp        36.51
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.95%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25882.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4711.09
    Total L1 Elapsed Cycles          cycle       400730
    Average L2 Active Cycles         cycle      4172.50
    Total L2 Elapsed Cycles          cycle       169512
    Average SM Active Cycles         cycle      4711.09
    Total SM Elapsed Cycles          cycle       400730
    Average SMSP Active Cycles       cycle      4664.24
    Total SMSP Elapsed Cycles        cycle      1602920
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       786.71
    Elapsed Cycles                cycle         6980
    Memory Throughput                 %        47.51
    DRAM Throughput                   %        47.51
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.37
    L2 Cache Throughput               %        28.69
    SM Active Cycles              cycle      4741.33
    Compute (SM) Throughput           %        16.26
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.89
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.01
    Mem Busy                               %        19.37
    Max Bandwidth                          %        47.51
    L1/TEX Hit Rate                        %         7.58
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.26
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.08
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.92
    Active Warps Per Scheduler          warp         8.95
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.95 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.49
    Warp Cycles Per Executed Instruction           cycle        56.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.49%                                                                                          
          On average, each warp of this workload spends 29.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 49.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.06
    Issued Instructions                             inst       196751
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.16
    Achieved Active Warps Per SM           warp        35.60
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.84%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25946.67
    Total DRAM Elapsed Cycles        cycle       327680
    Average L1 Active Cycles         cycle      4741.33
    Total L1 Elapsed Cycles          cycle       402990
    Average L2 Active Cycles         cycle      4309.62
    Total L2 Elapsed Cycles          cycle       172872
    Average SM Active Cycles         cycle      4741.33
    Total SM Elapsed Cycles          cycle       402990
    Average SMSP Active Cycles       cycle      4690.10
    Total SMSP Elapsed Cycles        cycle      1611960
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.75
    Elapsed Cycles                cycle      7388666
    Memory Throughput                 %        45.89
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.90
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4352536.07
    Compute (SM) Throughput           %        45.89
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.51
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.51
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       423.64
    Mem Busy                               %        30.78
    Max Bandwidth                          %        45.89
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.45
    Mem Pipes Busy                         %        45.89
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680067 bank conflicts,  which represent   
          15.30% of the overall 95969283 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.72%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351264 bank conflicts,  which represent   
          68.56% of the overall 26764877 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995753
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.17
    Achieved Active Warps Per SM           warp        17.84
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.25%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80093.33
    Total DRAM Elapsed Cycles        cycle    340016128
    Average L1 Active Cycles         cycle   4352536.07
    Total L1 Elapsed Cycles          cycle    423026278
    Average L2 Active Cycles         cycle    351846.75
    Total L2 Elapsed Cycles          cycle    179679720
    Average SM Active Cycles         cycle   4352536.07
    Total SM Elapsed Cycles          cycle    423026278
    Average SMSP Active Cycles       cycle   4355247.73
    Total SMSP Elapsed Cycles        cycle   1692105112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.97% above the average, while the minimum instance value is 8.26% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.85%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.95% above the average, while the minimum instance value is 8.24% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.97% above the average, while the minimum instance value is 8.26% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       792.69
    Elapsed Cycles                cycle         6925
    Memory Throughput                 %        45.45
    DRAM Throughput                   %        45.45
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.26
    L2 Cache Throughput               %        28.97
    SM Active Cycles              cycle      4657.29
    Compute (SM) Throughput           %        16.44
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.39
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.9%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       134.39
    Mem Busy                               %        20.07
    Max Bandwidth                          %        45.45
    L1/TEX Hit Rate                        %         6.90
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.44
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.19
    Issued Warp Per Scheduler                        0.17
    No Eligible                            %        82.81
    Active Warps Per Scheduler          warp         8.14
    Eligible Warps Per Scheduler        warp         0.33
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 54.55%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.14 active warps per scheduler, but only an average of 0.33 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.37
    Warp Cycles Per Executed Instruction           cycle        51.73
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 54.55%                                                                                          
          On average, each warp of this workload spends 28.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.1% of the total average of 47.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.82
    Issued Instructions                             inst       187879
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.16
    Achieved Active Warps Per SM           warp        34.16
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.84%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24280
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4657.29
    Total L1 Elapsed Cycles          cycle       398698
    Average L2 Active Cycles         cycle      4026.75
    Total L2 Elapsed Cycles          cycle       169200
    Average SM Active Cycles         cycle      4657.29
    Total SM Elapsed Cycles          cycle       398698
    Average SMSP Active Cycles       cycle      4710.83
    Total SMSP Elapsed Cycles        cycle      1594792
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       791.53
    Elapsed Cycles                cycle         6893
    Memory Throughput                 %        48.61
    DRAM Throughput                   %        48.61
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        24.77
    L2 Cache Throughput               %        29.42
    SM Active Cycles              cycle      4714.71
    Compute (SM) Throughput           %        16.52
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.98
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.98
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.81
    Mem Busy                               %        19.89
    Max Bandwidth                          %        48.61
    L1/TEX Hit Rate                        %         7.57
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.87
    Mem Pipes Busy                         %        16.52
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.19
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.81
    Active Warps Per Scheduler          warp         9.00
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.39%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.00 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.51
    Warp Cycles Per Executed Instruction           cycle        56.61
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.39%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.6% of the total average of 49.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.82
    Issued Instructions                             inst       196694
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.49
    Achieved Active Warps Per SM           warp        35.76
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.51%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25885.33
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4714.71
    Total L1 Elapsed Cycles          cycle       396644
    Average L2 Active Cycles         cycle         4188
    Total L2 Elapsed Cycles          cycle       168432
    Average SM Active Cycles         cycle      4714.71
    Total SM Elapsed Cycles          cycle       396644
    Average SMSP Active Cycles       cycle      4661.64
    Total SMSP Elapsed Cycles        cycle      1586576
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.42
    Elapsed Cycles                cycle         6922
    Memory Throughput                 %        47.46
    DRAM Throughput                   %        47.46
    Duration                         us         8.83
    L1/TEX Cache Throughput           %        24.45
    L2 Cache Throughput               %        28.77
    SM Active Cycles              cycle      4730.24
    Compute (SM) Throughput           %        16.33
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        17.93
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        17.93
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.43
    Mem Busy                               %        19.43
    Max Bandwidth                          %        47.46
    L1/TEX Hit Rate                        %         6.98
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.33
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.50
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.50
    Active Warps Per Scheduler          warp         8.92
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.54%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.92 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.19
    Warp Cycles Per Executed Instruction           cycle        55.11
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.54%                                                                                          
          On average, each warp of this workload spends 30.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 63.6% of the total average of 48.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.91
    Issued Instructions                             inst       196716
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.82
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.18%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25840
    Total DRAM Elapsed Cycles        cycle       326656
    Average L1 Active Cycles         cycle      4730.24
    Total L1 Elapsed Cycles          cycle       401308
    Average L2 Active Cycles         cycle      4289.83
    Total L2 Elapsed Cycles          cycle       172320
    Average SM Active Cycles         cycle      4730.24
    Total SM Elapsed Cycles          cycle       401308
    Average SMSP Active Cycles       cycle      4582.51
    Total SMSP Elapsed Cycles        cycle      1605232
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       815.46
    Elapsed Cycles                cycle         6925
    Memory Throughput                 %        50.07
    DRAM Throughput                   %        50.07
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        24.76
    L2 Cache Throughput               %        30.20
    SM Active Cycles              cycle      4710.14
    Compute (SM) Throughput           %        16.53
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.63
    Executed Ipc Elapsed  inst/cycle         0.43
    Issue Slots Busy               %        18.00
    Issued Ipc Active     inst/cycle         0.72
    SM Busy                        %        18.00
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91%                                                                                       
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       147.76
    Mem Busy                               %        20.41
    Max Bandwidth                          %        50.07
    L1/TEX Hit Rate                        %         7.67
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.53
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.00
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.00
    Active Warps Per Scheduler          warp         8.71
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.71 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.39
    Warp Cycles Per Executed Instruction           cycle        55.32
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 49.93%                                                                                          
          On average, each warp of this workload spends 29.9 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.8% of the total average of 48.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       847.75
    Issued Instructions                             inst       196678
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.36
    Achieved Active Warps Per SM           warp        36.17
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.64%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25808
    Total DRAM Elapsed Cycles        cycle       309248
    Average L1 Active Cycles         cycle      4710.14
    Total L1 Elapsed Cycles          cycle       396524
    Average L2 Active Cycles         cycle      4257.08
    Total L2 Elapsed Cycles          cycle       164064
    Average SM Active Cycles         cycle      4710.14
    Total SM Elapsed Cycles          cycle       396524
    Average SMSP Active Cycles       cycle      4710.13
    Total SMSP Elapsed Cycles        cycle      1586096
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.94
    Elapsed Cycles                cycle      7389512
    Memory Throughput                 %        45.88
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.86
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4354467.24
    Compute (SM) Throughput           %        45.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.48
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.48
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       433.12
    Mem Busy                               %        30.77
    Max Bandwidth                          %        45.88
    L1/TEX Hit Rate                        %         0.06
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.52
    Mem Pipes Busy                         %        45.88
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680080 bank conflicts,  which represent   
          15.30% of the overall 95969296 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.7%                                                                                           
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351435 bank conflicts,  which represent   
          68.56% of the overall 26765056 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.47
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.53
    Active Warps Per Scheduler          warp         4.45
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.45 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.82
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995753
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.14
    Achieved Active Warps Per SM           warp        17.83
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.29%                                                                                          
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     81885.33
    Total DRAM Elapsed Cycles        cycle    340014080
    Average L1 Active Cycles         cycle   4354467.24
    Total L1 Elapsed Cycles          cycle    423125236
    Average L2 Active Cycles         cycle    354804.79
    Total L2 Elapsed Cycles          cycle    179678448
    Average SM Active Cycles         cycle   4354467.24
    Total SM Elapsed Cycles          cycle    423125236
    Average SMSP Active Cycles       cycle   4355513.99
    Total SMSP Elapsed Cycles        cycle   1692500944
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.93% above the average, while the minimum instance value is 8.26% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.23% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.93% above the average, while the minimum instance value is 8.26% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 255, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       792.94
    Elapsed Cycles                cycle         6727
    Memory Throughput                 %        46.28
    DRAM Throughput                   %        46.28
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        24.69
    L2 Cache Throughput               %        29.86
    SM Active Cycles              cycle      4576.14
    Compute (SM) Throughput           %        16.93
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.69
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.69
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.74%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       136.96
    Mem Busy                               %        20.76
    Max Bandwidth                          %        46.28
    L1/TEX Hit Rate                        %         7.32
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.81
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.19
    Active Warps Per Scheduler          warp         8.32
    Eligible Warps Per Scheduler        warp         0.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.32 active warps per scheduler, but only an average of 0.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        46.69
    Warp Cycles Per Executed Instruction           cycle        50.98
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.72%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.0% of the total average of 46.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.62
    Issued Instructions                             inst       187833
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.79
    Achieved Active Warps Per SM           warp        33.98
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.21%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24013.33
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4576.14
    Total L1 Elapsed Cycles          cycle       387046
    Average L2 Active Cycles         cycle      4103.71
    Total L2 Elapsed Cycles          cycle       164232
    Average SM Active Cycles         cycle      4576.14
    Total SM Elapsed Cycles          cycle       387046
    Average SMSP Active Cycles       cycle      4544.82
    Total SMSP Elapsed Cycles        cycle      1548184
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       799.20
    Elapsed Cycles                cycle         7119
    Memory Throughput                 %        47.92
    DRAM Throughput                   %        47.92
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        28.92
    SM Active Cycles              cycle      4809.43
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.64
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.64
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.19%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       141.41
    Mem Busy                               %        19.52
    Max Bandwidth                          %        47.92
    L1/TEX Hit Rate                        %         7.53
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.07
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.17
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.83
    Active Warps Per Scheduler          warp         9.01
    Eligible Warps Per Scheduler        warp         0.38
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.08%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.01 active warps per scheduler, but only an average of 0.38 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.55
    Warp Cycles Per Executed Instruction           cycle        56.71
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.08%                                                                                          
          On average, each warp of this workload spends 30.4 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.4% of the total average of 49.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.60
    Issued Instructions                             inst       196875
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.65
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 26.35%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25925.33
    Total DRAM Elapsed Cycles        cycle       324608
    Average L1 Active Cycles         cycle      4809.43
    Total L1 Elapsed Cycles          cycle       407910
    Average L2 Active Cycles         cycle      4269.17
    Total L2 Elapsed Cycles          cycle       171528
    Average SM Active Cycles         cycle      4809.43
    Total SM Elapsed Cycles          cycle       407910
    Average SMSP Active Cycles       cycle      4669.07
    Total SMSP Elapsed Cycles        cycle      1631640
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       805.11
    Elapsed Cycles                cycle         7177
    Memory Throughput                 %        47.51
    DRAM Throughput                   %        47.51
    Duration                         us         8.80
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        28.88
    SM Active Cycles              cycle      4833.28
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        17.55
    Issued Ipc Active     inst/cycle         0.70
    SM Busy                        %        17.55
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.23%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       140.64
    Mem Busy                               %        19.50
    Max Bandwidth                          %        47.51
    L1/TEX Hit Rate                        %         7.19
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        15.95
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.13
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        81.87
    Active Warps Per Scheduler          warp         8.84
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.84 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        48.74
    Warp Cycles Per Executed Instruction           cycle        55.75
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 52.49%                                                                                          
          On average, each warp of this workload spends 30.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 61.6% of the total average of 48.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.23
    Issued Instructions                             inst       196790
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.94
    Achieved Active Warps Per SM           warp        34.53
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 28.06%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        25784
    Total DRAM Elapsed Cycles        cycle       325632
    Average L1 Active Cycles         cycle      4833.28
    Total L1 Elapsed Cycles          cycle       410924
    Average L2 Active Cycles         cycle      4284.12
    Total L2 Elapsed Cycles          cycle       171648
    Average SM Active Cycles         cycle      4833.28
    Total SM Elapsed Cycles          cycle       410924
    Average SMSP Active Cycles       cycle      4677.53
    Total SMSP Elapsed Cycles        cycle      1643696
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.86
    Elapsed Cycles                cycle         6848
    Memory Throughput                 %        48.51
    DRAM Throughput                   %        48.51
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.85
    L2 Cache Throughput               %        29.30
    SM Active Cycles              cycle      4633.79
    Compute (SM) Throughput           %        16.58
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.64
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        18.30
    Issued Ipc Active     inst/cycle         0.73
    SM Busy                        %        18.30
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.86%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       143.44
    Mem Busy                               %        19.74
    Max Bandwidth                          %        48.51
    L1/TEX Hit Rate                        %         7.17
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.58
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.68
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.32
    Active Warps Per Scheduler          warp         9.17
    Eligible Warps Per Scheduler        warp         0.39
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.49%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.17 active warps per scheduler, but only an average of 0.39 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        49.08
    Warp Cycles Per Executed Instruction           cycle        56.13
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 51.49%                                                                                          
          On average, each warp of this workload spends 29.8 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.7% of the total average of 49.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       848.08
    Issued Instructions                             inst       196755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.21
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     25914.67
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4633.79
    Total L1 Elapsed Cycles          cycle       395268
    Average L2 Active Cycles         cycle      4266.33
    Total L2 Elapsed Cycles          cycle       169560
    Average SM Active Cycles         cycle      4633.79
    Total SM Elapsed Cycles          cycle       395268
    Average SMSP Active Cycles       cycle      4540.01
    Total SMSP Elapsed Cycles        cycle      1581072
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void fa_kernel<64, 32>(const float *, const float *, const float *, float *, int, int, float) (64, 1, 1)x(512, 1, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       803.96
    Elapsed Cycles                cycle      7389404
    Memory Throughput                 %        45.88
    DRAM Throughput                   %         0.14
    Duration                         ms         9.07
    L1/TEX Cache Throughput           %        76.87
    L2 Cache Throughput               %         1.19
    SM Active Cycles              cycle   4353733.55
    Compute (SM) Throughput           %        45.88
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.6 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 5%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        28.84
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            4
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.02
    Executed Ipc Elapsed  inst/cycle         1.20
    Issue Slots Busy               %        50.49
    Issued Ipc Active     inst/cycle         2.02
    SM Busy                        %        50.49
    -------------------- ----------- ------------

    INF   FMA is the highest-utilized pipeline (27.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes 32-bit floating point (FADD, FMUL, FMAD, ...) and integer (IMUL, IMAD)    
          operations. It is well-utilized, but should not be a bottleneck.                                              

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Mbyte/s       416.51
    Mem Busy                               %        30.78
    Max Bandwidth                          %        45.88
    L1/TEX Hit Rate                        %         0.05
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        97.65
    Mem Pipes Busy                         %        45.88
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 11.76%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.2 - way bank        
          conflict across all 81289216 shared load requests.This results in 14680064 bank conflicts,  which represent   
          15.30% of the overall 95969280 wavefronts for shared loads. Check the Source Counters section for             
          uncoalesced shared loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 52.71%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 3.2 - way bank       
          conflict across all 8413440 shared store requests.This results in 18351335 bank conflicts,  which represent   
          68.56% of the overall 26764943 wavefronts for shared stores. Check the Source Counters section for            
          uncoalesced shared stores.                                                                                    

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        50.48
    Issued Warp Per Scheduler                        0.50
    No Eligible                            %        49.52
    Active Warps Per Scheduler          warp         4.46
    Eligible Warps Per Scheduler        warp         1.36
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 49.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 4.46 active warps per scheduler, but only an average of 1.36 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         8.83
    Warp Cycles Per Executed Instruction           cycle         8.84
    Avg. Active Threads Per Warp                                16.04
    Avg. Not Predicated Off Threads Per Warp                    14.91
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.5%                                                                                           
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 16.0 threads being active per cycle. This is further reduced  
          to 14.9 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2195377.66
    Executed Instructions                           inst    509327616
    Avg. Issued Instructions Per Scheduler          inst   2198257.56
    Issued Instructions                             inst    509995755
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   512
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     64
    Registers Per Thread             register/thread              64
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           33.54
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread           32768
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.55
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have at least two  
          blocks per multiprocessor (compared to the currently executed 1.1 blocks) This way, blocks that aren't        
          waiting for __syncthreads() can keep the hardware busy.                                                       

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            3
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        37.13
    Achieved Active Warps Per SM           warp        17.82
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.3%                                                                                           
          The difference between calculated theoretical (66.7%) and measured achieved occupancy (37.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 33.33%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of required      
          registers, and the required amount of shared memory.                                                          

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78736
    Total DRAM Elapsed Cycles        cycle    339979264
    Average L1 Active Cycles         cycle   4353733.55
    Total L1 Elapsed Cycles          cycle    423096870
    Average L2 Active Cycles         cycle    356106.33
    Total L2 Elapsed Cycles          cycle    179659944
    Average SM Active Cycles         cycle   4353733.55
    Total SM Elapsed Cycles          cycle    423096870
    Average SMSP Active Cycles       cycle   4354909.20
    Total SMSP Elapsed Cycles        cycle   1692387480
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 23.86%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 39.99% above the average, while the minimum instance value is 8.27% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.83%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 39.92% above the average, while the minimum instance value is 8.22% below the       
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 23.86%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 39.99% above the average, while the minimum instance value is 8.27% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst     26462720
    Branch Efficiency                   %        98.67
    Avg. Divergent Branches                    1129.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 16.06%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 33030144 excessive wavefronts (27% of the 
          total 122732800 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source            
          locations. The CUDA Best Practices Guide                                                                      
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (4, 256, 1)x(16, 16, 1), Context 1, Stream 256, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       791.09
    Elapsed Cycles                cycle         6838
    Memory Throughput                 %        46.05
    DRAM Throughput                   %        46.05
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.61
    L2 Cache Throughput               %        29.34
    SM Active Cycles              cycle      4591.78
    Compute (SM) Throughput           %        16.65
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.65
    Executed Ipc Elapsed  inst/cycle         0.44
    Issue Slots Busy               %        17.63
    Issued Ipc Active     inst/cycle         0.71
    SM Busy                        %        17.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.77%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       135.93
    Mem Busy                               %        20.36
    Max Bandwidth                          %        46.05
    L1/TEX Hit Rate                        %         8.43
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.91
    Mem Pipes Busy                         %        16.65
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        17.77
    Issued Warp Per Scheduler                        0.18
    No Eligible                            %        82.23
    Active Warps Per Scheduler          warp         8.47
    Eligible Warps Per Scheduler        warp         0.34
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 53.95%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 5.6 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.47 active warps per scheduler, but only an average of 0.34 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.66
    Warp Cycles Per Executed Instruction           cycle        52.04
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 53.95%                                                                                          
          On average, each warp of this workload spends 28.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 60.3% of the total average of 47.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       741.52
    Executed Instructions                           inst       172032
    Avg. Issued Instructions Per Scheduler          inst       809.68
    Issued Instructions                             inst       187846
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        29
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.28
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 27.72%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24285.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4591.78
    Total L1 Elapsed Cycles          cycle       393494
    Average L2 Active Cycles         cycle      4100.08
    Total L2 Elapsed Cycles          cycle       167088
    Average SM Active Cycles         cycle      4591.78
    Total SM Elapsed Cycles          cycle       393494
    Average SMSP Active Cycles       cycle      4556.08
    Total SMSP Elapsed Cycles        cycle      1573976
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst        16384
    Branch Efficiency                   %            0
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

