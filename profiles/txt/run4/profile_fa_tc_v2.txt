==PROF== Connected to process 77240 (/teamspace/studios/this_studio/QuantizedMHA/bin/profile_fa_tc_v2)
==PROF== Profiling "extract_mat" - 0: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 1: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 2: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 3: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 4: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 5: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 6: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 7: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 8: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 9: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 10: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 11: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 12: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 13: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 14: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 15: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 16: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 17: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 18: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 19: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 20: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 21: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 22: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 23: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 24: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 25: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 26: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 27: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 28: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 29: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 30: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 31: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 32: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 33: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 34: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 35: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 36: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 37: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 38: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 39: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 40: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 41: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 42: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 43: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 44: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 45: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 46: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 47: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 48: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 49: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 50: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 51: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 52: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 53: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 54: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 55: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 56: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 57: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 58: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 59: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 60: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 61: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 62: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 63: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 64: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 65: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 66: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 67: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 68: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 69: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 70: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 71: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 72: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 73: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 74: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 75: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 76: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 77: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 78: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 79: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 80: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 81: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 82: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 83: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 84: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 85: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 86: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 87: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 88: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 89: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 90: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 91: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 92: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 93: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 94: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 95: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 96: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 97: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 98: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 99: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 100: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 101: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 102: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 103: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 104: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 105: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 106: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 107: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 108: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 109: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 110: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 111: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 112: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 113: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 114: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 115: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 116: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 117: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 118: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 119: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 120: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 121: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 122: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 123: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 124: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 125: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 126: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 127: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 128: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 129: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 130: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 131: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 132: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 133: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 134: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 135: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 136: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 137: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 138: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 139: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 140: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 141: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 142: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 143: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 144: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 145: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 146: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 147: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 148: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 149: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 150: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 151: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 152: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 153: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 154: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 155: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 156: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 157: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 158: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 159: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 160: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 161: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 162: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 163: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 164: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 165: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 166: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 167: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 168: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 169: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 170: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 171: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 172: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 173: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 174: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 175: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 176: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 177: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 178: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 179: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 180: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 181: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 182: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 183: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 184: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 185: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 186: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 187: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 188: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 189: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 190: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 191: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 192: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 193: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 194: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 195: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 196: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 197: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 198: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 199: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 200: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 201: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 202: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 203: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 204: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 205: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 206: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 207: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 208: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 209: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 210: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 211: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 212: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 213: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 214: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 215: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 216: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 217: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 218: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 219: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 220: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 221: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 222: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 223: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 224: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 225: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 226: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 227: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 228: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 229: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 230: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 231: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 232: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 233: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 234: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 235: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 236: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 237: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 238: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 239: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 240: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 241: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 242: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 243: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 244: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 245: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 246: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 247: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 248: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 249: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 250: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 251: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 252: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 253: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 254: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 255: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 256: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 257: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 258: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 259: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 260: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 261: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 262: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 263: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 264: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 265: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 266: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 267: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 268: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 269: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 270: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 271: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 272: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 273: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 274: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 275: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 276: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 277: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 278: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 279: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 280: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 281: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 282: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 283: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 284: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 285: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 286: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 287: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 288: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 289: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 290: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 291: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 292: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 293: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 294: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 295: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 296: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 297: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 298: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 299: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 300: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 301: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 302: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 303: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 304: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 305: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 306: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 307: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 308: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 309: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 310: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 311: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 312: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 313: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 314: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 315: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 316: 0%....50%....100% - 8 passes
==PROF== Profiling "extract_mat" - 317: 0%....50%....100% - 8 passes
==PROF== Profiling "fa_kernel" - 318: 0%....50%....100% - 8 passes
==PROF== Profiling "concat_mat" - 319: 0%....50%....100% - 8 passes
Initializing host data (constant values for correctness check)...
Running correctness check 
Loaded reference output from .cache/ref_N8192_d1024.bin
Correctness check PASSED.
Loaded input matrices from .cache/input_random_N8192_d1024.bin
Running 0 warmup iterations...
Running 1 profiling iterations...
Profiling complete.
==PROF== Disconnected from process 77240
[77240] profile_fa_tc_v2@127.0.0.1
  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.43
    Elapsed Cycles                cycle         7068
    Memory Throughput                 %        50.05
    DRAM Throughput                   %        50.05
    Duration                         us         8.99
    L1/TEX Cache Throughput           %        24.39
    L2 Cache Throughput               %        28.17
    SM Active Cycles              cycle      4953.40
    Compute (SM) Throughput           %        16.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.28
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27674.67
    Total DRAM Elapsed Cycles        cycle       331776
    Average L1 Active Cycles         cycle      4953.40
    Total L1 Elapsed Cycles          cycle       402588
    Average L2 Active Cycles         cycle      4414.58
    Total L2 Elapsed Cycles          cycle       175968
    Average SM Active Cycles         cycle      4953.40
    Total SM Elapsed Cycles          cycle       402588
    Average SMSP Active Cycles       cycle      4838.29
    Total SMSP Elapsed Cycles        cycle      1610352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.35
    Elapsed Cycles                cycle         7140
    Memory Throughput                 %        49.45
    DRAM Throughput                   %        49.45
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.74
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      4950.10
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4950.10
    Total L1 Elapsed Cycles          cycle       413544
    Average L2 Active Cycles         cycle         4443
    Total L2 Elapsed Cycles          cycle       177720
    Average SM Active Cycles         cycle      4950.10
    Total SM Elapsed Cycles          cycle       413544
    Average SMSP Active Cycles       cycle      4862.58
    Total SMSP Elapsed Cycles        cycle      1654176
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.57
    Elapsed Cycles                cycle         7183
    Memory Throughput                 %        48.94
    DRAM Throughput                   %        48.94
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      5017.95
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.29
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5017.95
    Total L1 Elapsed Cycles          cycle       415390
    Average L2 Active Cycles         cycle      4528.88
    Total L2 Elapsed Cycles          cycle       178872
    Average SM Active Cycles         cycle      5017.95
    Total SM Elapsed Cycles          cycle       415390
    Average SMSP Active Cycles       cycle      4964.19
    Total SMSP Elapsed Cycles        cycle      1661560
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4440268
    Memory Throughput                 %        51.00
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204948.38
    Compute (SM) Throughput           %        51.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77280
    Total DRAM Elapsed Cycles        cycle    209270784
    Average L1 Active Cycles         cycle   3204948.38
    Total L1 Elapsed Cycles          cycle    257332662
    Average L2 Active Cycles         cycle   1020624.29
    Total L2 Elapsed Cycles          cycle    110596776
    Average SM Active Cycles         cycle   3204948.38
    Total SM Elapsed Cycles          cycle    257332662
    Average SMSP Active Cycles       cycle   3204762.39
    Total SMSP Elapsed Cycles        cycle   1029330648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.78% above the average, while the minimum instance value is 10.55% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.67% above the average, while the minimum instance value is 10.42% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.55% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       779.41
    Elapsed Cycles                cycle         6511
    Memory Throughput                 %        46.32
    DRAM Throughput                   %        46.32
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.75
    L2 Cache Throughput               %        25.86
    SM Active Cycles              cycle      4387.67
    Compute (SM) Throughput           %        17.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.11
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23794.67
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4387.67
    Total L1 Elapsed Cycles          cycle       377452
    Average L2 Active Cycles         cycle      4044.21
    Total L2 Elapsed Cycles          cycle       162120
    Average SM Active Cycles         cycle      4387.67
    Total SM Elapsed Cycles          cycle       377452
    Average SMSP Active Cycles       cycle      4486.44
    Total SMSP Elapsed Cycles        cycle      1509808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.842%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 11.43% above the average, while the minimum instance value is 2.68% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         7195
    Memory Throughput                 %        49.06
    DRAM Throughput                   %        49.06
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.85
    L2 Cache Throughput               %        27.64
    SM Active Cycles              cycle      4961.59
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.43
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27714.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4961.59
    Total L1 Elapsed Cycles          cycle       411890
    Average L2 Active Cycles         cycle      4479.04
    Total L2 Elapsed Cycles          cycle       179424
    Average SM Active Cycles         cycle      4961.59
    Total SM Elapsed Cycles          cycle       411890
    Average SMSP Active Cycles       cycle      4883.94
    Total SMSP Elapsed Cycles        cycle      1647560
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.00
    Elapsed Cycles                cycle         7254
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        27.49
    SM Active Cycles              cycle      4936.19
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.39
    Achieved Active Warps Per SM           warp        35.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4936.19
    Total L1 Elapsed Cycles          cycle       411734
    Average L2 Active Cycles         cycle      4392.29
    Total L2 Elapsed Cycles          cycle       180552
    Average SM Active Cycles         cycle      4936.19
    Total SM Elapsed Cycles          cycle       411734
    Average SMSP Active Cycles       cycle      4811.48
    Total SMSP Elapsed Cycles        cycle      1646936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.50
    Elapsed Cycles                cycle         7165
    Memory Throughput                 %        49.47
    DRAM Throughput                   %        49.47
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        27.78
    SM Active Cycles              cycle      4997.28
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.70
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4997.28
    Total L1 Elapsed Cycles          cycle       406004
    Average L2 Active Cycles         cycle      4494.42
    Total L2 Elapsed Cycles          cycle       178416
    Average SM Active Cycles         cycle      4997.28
    Total SM Elapsed Cycles          cycle       406004
    Average SMSP Active Cycles       cycle      4922.43
    Total SMSP Elapsed Cycles        cycle      1624016
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4437760
    Memory Throughput                 %        50.98
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205328.36
    Compute (SM) Throughput           %        50.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80202.67
    Total DRAM Elapsed Cycles        cycle    209152000
    Average L1 Active Cycles         cycle   3205328.36
    Total L1 Elapsed Cycles          cycle    257410754
    Average L2 Active Cycles         cycle   1028550.33
    Total L2 Elapsed Cycles          cycle    110525400
    Average SM Active Cycles         cycle   3205328.36
    Total SM Elapsed Cycles          cycle    257410754
    Average SMSP Active Cycles       cycle   3204724.07
    Total SMSP Elapsed Cycles        cycle   1029643016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.2%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.2%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.32
    Elapsed Cycles                cycle         6594
    Memory Throughput                 %        46.50
    DRAM Throughput                   %        46.50
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.53
    L2 Cache Throughput               %        25.53
    SM Active Cycles              cycle      4426.53
    Compute (SM) Throughput           %        17.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.09
    Achieved Active Warps Per SM           warp        34.60
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24045.33
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4426.53
    Total L1 Elapsed Cycles          cycle       380250
    Average L2 Active Cycles         cycle      4102.58
    Total L2 Elapsed Cycles          cycle       164160
    Average SM Active Cycles         cycle      4426.53
    Total SM Elapsed Cycles          cycle       380250
    Average SMSP Active Cycles       cycle      4431.77
    Total SMSP Elapsed Cycles        cycle      1521000
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.19
    Elapsed Cycles                cycle         7203
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4883.10
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.22
    Achieved Active Warps Per SM           warp        36.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4883.10
    Total L1 Elapsed Cycles          cycle       411680
    Average L2 Active Cycles         cycle      4376.46
    Total L2 Elapsed Cycles          cycle       179376
    Average SM Active Cycles         cycle      4883.10
    Total SM Elapsed Cycles          cycle       411680
    Average SMSP Active Cycles       cycle      4814.97
    Total SMSP Elapsed Cycles        cycle      1646720
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.04
    Elapsed Cycles                cycle         7093
    Memory Throughput                 %        49.65
    DRAM Throughput                   %        49.65
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        28.01
    SM Active Cycles              cycle      4952.10
    Compute (SM) Throughput           %        16.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.40
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27709.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4952.10
    Total L1 Elapsed Cycles          cycle       402738
    Average L2 Active Cycles         cycle      4496.21
    Total L2 Elapsed Cycles          cycle       176664
    Average SM Active Cycles         cycle      4952.10
    Total SM Elapsed Cycles          cycle       402738
    Average SMSP Active Cycles       cycle      4925.32
    Total SMSP Elapsed Cycles        cycle      1610952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.27
    Elapsed Cycles                cycle         7179
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4969.72
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27712
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4969.72
    Total L1 Elapsed Cycles          cycle       413854
    Average L2 Active Cycles         cycle      4542.88
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4969.72
    Total SM Elapsed Cycles          cycle       413854
    Average SMSP Active Cycles       cycle      4945.38
    Total SMSP Elapsed Cycles        cycle      1655416
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4438747
    Memory Throughput                 %        50.80
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205002.12
    Compute (SM) Throughput           %        50.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77666.67
    Total DRAM Elapsed Cycles        cycle    209198080
    Average L1 Active Cycles         cycle   3205002.12
    Total L1 Elapsed Cycles          cycle    258313452
    Average L2 Active Cycles         cycle   1026289.42
    Total L2 Elapsed Cycles          cycle    110552304
    Average SM Active Cycles         cycle   3205002.12
    Total SM Elapsed Cycles          cycle    258313452
    Average SMSP Active Cycles       cycle   3204674.82
    Total SMSP Elapsed Cycles        cycle   1033253808
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.76% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.13%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.98% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.76% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.74
    Elapsed Cycles                cycle         6615
    Memory Throughput                 %        45.84
    DRAM Throughput                   %        45.84
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.24
    L2 Cache Throughput               %        25.48
    SM Active Cycles              cycle      4476.97
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.89
    Achieved Active Warps Per SM           warp        34.51
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23861.33
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4476.97
    Total L1 Elapsed Cycles          cycle       382138
    Average L2 Active Cycles         cycle      4029.17
    Total L2 Elapsed Cycles          cycle       164664
    Average SM Active Cycles         cycle      4476.97
    Total SM Elapsed Cycles          cycle       382138
    Average SMSP Active Cycles       cycle      4386.78
    Total SMSP Elapsed Cycles        cycle      1528552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.144%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.76% above the average, while the minimum instance value is 2.76% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.44
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      5077.31
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.63
    Achieved Active Warps Per SM           warp        34.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.37%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5077.31
    Total L1 Elapsed Cycles          cycle       407262
    Average L2 Active Cycles         cycle      4500.92
    Total L2 Elapsed Cycles          cycle       179832
    Average SM Active Cycles         cycle      5077.31
    Total SM Elapsed Cycles          cycle       407262
    Average SMSP Active Cycles       cycle      4908.45
    Total SMSP Elapsed Cycles        cycle      1629048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.82
    Elapsed Cycles                cycle         7126
    Memory Throughput                 %        49.59
    DRAM Throughput                   %        49.59
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.28
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      4831.24
    Compute (SM) Throughput           %        16.25
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.89
    Achieved Active Warps Per SM           warp        35.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4831.24
    Total L1 Elapsed Cycles          cycle       403400
    Average L2 Active Cycles         cycle      4497.46
    Total L2 Elapsed Cycles          cycle       177432
    Average SM Active Cycles         cycle      4831.24
    Total SM Elapsed Cycles          cycle       403400
    Average SMSP Active Cycles       cycle      4874.70
    Total SMSP Elapsed Cycles        cycle      1613600
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.10
    Elapsed Cycles                cycle         7137
    Memory Throughput                 %        49.71
    DRAM Throughput                   %        49.71
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.08
    L2 Cache Throughput               %        27.87
    SM Active Cycles              cycle      4912.36
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.91
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4912.36
    Total L1 Elapsed Cycles          cycle       406974
    Average L2 Active Cycles         cycle      4473.17
    Total L2 Elapsed Cycles          cycle       177600
    Average SM Active Cycles         cycle      4912.36
    Total SM Elapsed Cycles          cycle       406974
    Average SMSP Active Cycles       cycle      4873.15
    Total SMSP Elapsed Cycles        cycle      1627896
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4438719
    Memory Throughput                 %        50.92
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205170.33
    Compute (SM) Throughput           %        50.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77258.67
    Total DRAM Elapsed Cycles        cycle    209197056
    Average L1 Active Cycles         cycle   3205170.33
    Total L1 Elapsed Cycles          cycle    257699198
    Average L2 Active Cycles         cycle   1021009.88
    Total L2 Elapsed Cycles          cycle    110563224
    Average SM Active Cycles         cycle   3205170.33
    Total SM Elapsed Cycles          cycle    257699198
    Average SMSP Active Cycles       cycle   3205367.41
    Total SMSP Elapsed Cycles        cycle   1030796792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.01%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.74% above the average, while the minimum instance value is 10.43% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.01%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.74% above the average, while the minimum instance value is 10.43% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.92
    Elapsed Cycles                cycle         6699
    Memory Throughput                 %        45.75
    DRAM Throughput                   %        45.75
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.33
    L2 Cache Throughput               %        25.14
    SM Active Cycles              cycle      4460.09
    Compute (SM) Throughput           %        17.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.00
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24128
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4460.09
    Total L1 Elapsed Cycles          cycle       373892
    Average L2 Active Cycles         cycle      4138.62
    Total L2 Elapsed Cycles          cycle       166848
    Average SM Active Cycles         cycle      4460.09
    Total SM Elapsed Cycles          cycle       373892
    Average SMSP Active Cycles       cycle      4471.09
    Total SMSP Elapsed Cycles        cycle      1495568
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.77
    Elapsed Cycles                cycle         7275
    Memory Throughput                 %        48.68
    DRAM Throughput                   %        48.68
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      4933.91
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.87
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.13%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4933.91
    Total L1 Elapsed Cycles          cycle       416488
    Average L2 Active Cycles         cycle      4474.67
    Total L2 Elapsed Cycles          cycle       181056
    Average SM Active Cycles         cycle      4933.91
    Total SM Elapsed Cycles          cycle       416488
    Average SMSP Active Cycles       cycle      4889.79
    Total SMSP Elapsed Cycles        cycle      1665952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.68
    Elapsed Cycles                cycle         7243
    Memory Throughput                 %        48.98
    DRAM Throughput                   %        48.98
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        27.45
    SM Active Cycles              cycle      4989.84
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.53
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4989.84
    Total L1 Elapsed Cycles          cycle       418192
    Average L2 Active Cycles         cycle      4457.92
    Total L2 Elapsed Cycles          cycle       180384
    Average SM Active Cycles         cycle      4989.84
    Total SM Elapsed Cycles          cycle       418192
    Average SMSP Active Cycles       cycle      4872.06
    Total SMSP Elapsed Cycles        cycle      1672768
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.42
    Elapsed Cycles                cycle         7230
    Memory Throughput                 %        48.88
    DRAM Throughput                   %        48.88
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle      5013.17
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.09
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5013.17
    Total L1 Elapsed Cycles          cycle       414444
    Average L2 Active Cycles         cycle      4496.62
    Total L2 Elapsed Cycles          cycle       179952
    Average SM Active Cycles         cycle      5013.17
    Total SM Elapsed Cycles          cycle       414444
    Average SMSP Active Cycles       cycle      4929.16
    Total SMSP Elapsed Cycles        cycle      1657776
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4436386
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205538.02
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77392
    Total DRAM Elapsed Cycles        cycle    209087488
    Average L1 Active Cycles         cycle   3205538.02
    Total L1 Elapsed Cycles          cycle    257370204
    Average L2 Active Cycles         cycle   1031589.50
    Total L2 Elapsed Cycles          cycle    110504880
    Average SM Active Cycles         cycle   3205538.02
    Total SM Elapsed Cycles          cycle    257370204
    Average SMSP Active Cycles       cycle   3204998.84
    Total SMSP Elapsed Cycles        cycle   1029480816
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.73% above the average, while the minimum instance value is 10.38% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.19%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.95% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.38% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.52
    Elapsed Cycles                cycle         6771
    Memory Throughput                 %        44.70
    DRAM Throughput                   %        44.70
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        25.38
    L2 Cache Throughput               %        24.88
    SM Active Cycles              cycle      4451.31
    Compute (SM) Throughput           %        17.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.40
    Achieved Active Warps Per SM           warp        35.71
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23802.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4451.31
    Total L1 Elapsed Cycles          cycle       378668
    Average L2 Active Cycles         cycle      4040.29
    Total L2 Elapsed Cycles          cycle       168552
    Average SM Active Cycles         cycle      4451.31
    Total SM Elapsed Cycles          cycle       378668
    Average SMSP Active Cycles       cycle      4374.72
    Total SMSP Elapsed Cycles        cycle      1514672
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.025%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.74% above the average, while the minimum instance value is 3.18% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.86
    Elapsed Cycles                cycle         7092
    Memory Throughput                 %        49.71
    DRAM Throughput                   %        49.71
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.24
    L2 Cache Throughput               %        28.04
    SM Active Cycles              cycle      5002.72
    Compute (SM) Throughput           %        16.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.24
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      5002.72
    Total L1 Elapsed Cycles          cycle       404562
    Average L2 Active Cycles         cycle      4531.25
    Total L2 Elapsed Cycles          cycle       176496
    Average SM Active Cycles         cycle      5002.72
    Total SM Elapsed Cycles          cycle       404562
    Average SMSP Active Cycles       cycle      4930.46
    Total SMSP Elapsed Cycles        cycle      1618248
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.71
    Elapsed Cycles                cycle         7205
    Memory Throughput                 %        48.88
    DRAM Throughput                   %        48.88
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        27.59
    SM Active Cycles              cycle      4972.86
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.54
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4972.86
    Total L1 Elapsed Cycles          cycle       408798
    Average L2 Active Cycles         cycle         4530
    Total L2 Elapsed Cycles          cycle       179400
    Average SM Active Cycles         cycle      4972.86
    Total SM Elapsed Cycles          cycle       408798
    Average SMSP Active Cycles       cycle      4982.65
    Total SMSP Elapsed Cycles        cycle      1635192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.46
    Elapsed Cycles                cycle         7247
    Memory Throughput                 %        48.78
    DRAM Throughput                   %        48.78
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.47
    SM Active Cycles              cycle      4978.69
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.55
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4978.69
    Total L1 Elapsed Cycles          cycle       407360
    Average L2 Active Cycles         cycle      4501.42
    Total L2 Elapsed Cycles          cycle       180432
    Average SM Active Cycles         cycle      4978.69
    Total SM Elapsed Cycles          cycle       407360
    Average SMSP Active Cycles       cycle      4896.24
    Total SMSP Elapsed Cycles        cycle      1629440
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4443676
    Memory Throughput                 %        50.96
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204927.41
    Compute (SM) Throughput           %        50.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78216
    Total DRAM Elapsed Cycles        cycle    209432576
    Average L1 Active Cycles         cycle   3204927.41
    Total L1 Elapsed Cycles          cycle    257528898
    Average L2 Active Cycles         cycle   1030334.50
    Total L2 Elapsed Cycles          cycle    110672616
    Average SM Active Cycles         cycle   3204927.41
    Total SM Elapsed Cycles          cycle    257528898
    Average SMSP Active Cycles       cycle   3205377.88
    Total SMSP Elapsed Cycles        cycle   1030115592
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.19%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 28.00% above the average, while the minimum instance value is 10.58% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.19%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.55
    Elapsed Cycles                cycle         6706
    Memory Throughput                 %        45.56
    DRAM Throughput                   %        45.56
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.75
    L2 Cache Throughput               %        25.15
    SM Active Cycles              cycle      4388.28
    Compute (SM) Throughput           %        17.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.37
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.63%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24024
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4388.28
    Total L1 Elapsed Cycles          cycle       382296
    Average L2 Active Cycles         cycle      3974.75
    Total L2 Elapsed Cycles          cycle       167016
    Average SM Active Cycles         cycle      4388.28
    Total SM Elapsed Cycles          cycle       382296
    Average SMSP Active Cycles       cycle      4341.43
    Total SMSP Elapsed Cycles        cycle      1529184
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.95
    Elapsed Cycles                cycle         7263
    Memory Throughput                 %        48.57
    DRAM Throughput                   %        48.57
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      4989.50
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.19
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4989.50
    Total L1 Elapsed Cycles          cycle       407244
    Average L2 Active Cycles         cycle      4488.58
    Total L2 Elapsed Cycles          cycle       180936
    Average SM Active Cycles         cycle      4989.50
    Total SM Elapsed Cycles          cycle       407244
    Average SMSP Active Cycles       cycle      4920.03
    Total SMSP Elapsed Cycles        cycle      1628976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.17%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.38% above the average, while the minimum instance value is 5.63% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.93
    Elapsed Cycles                cycle         7087
    Memory Throughput                 %        49.91
    DRAM Throughput                   %        49.91
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        28.06
    SM Active Cycles              cycle      5011.14
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.36
    Achieved Active Warps Per SM           warp        34.26
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       333824
    Average L1 Active Cycles         cycle      5011.14
    Total L1 Elapsed Cycles          cycle       406666
    Average L2 Active Cycles         cycle      4500.12
    Total L2 Elapsed Cycles          cycle       176544
    Average SM Active Cycles         cycle      5011.14
    Total SM Elapsed Cycles          cycle       406666
    Average SMSP Active Cycles       cycle      4941.71
    Total SMSP Elapsed Cycles        cycle      1626664
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.68
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        27.69
    SM Active Cycles              cycle      4991.76
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.50
    Achieved Active Warps Per SM           warp        35.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4991.76
    Total L1 Elapsed Cycles          cycle       408852
    Average L2 Active Cycles         cycle      4469.17
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4991.76
    Total SM Elapsed Cycles          cycle       408852
    Average SMSP Active Cycles       cycle      4867.17
    Total SMSP Elapsed Cycles        cycle      1635408
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4444569
    Memory Throughput                 %        50.91
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205562.33
    Compute (SM) Throughput           %        50.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.22
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77546.67
    Total DRAM Elapsed Cycles        cycle    209473536
    Average L1 Active Cycles         cycle   3205562.33
    Total L1 Elapsed Cycles          cycle    257773530
    Average L2 Active Cycles         cycle   1031308.62
    Total L2 Elapsed Cycles          cycle    110697528
    Average SM Active Cycles         cycle   3205562.33
    Total SM Elapsed Cycles          cycle    257773530
    Average SMSP Active Cycles       cycle   3205684.74
    Total SMSP Elapsed Cycles        cycle   1031094120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 28.01% above the average, while the minimum instance value is 10.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.85% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 28.01% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.55
    Elapsed Cycles                cycle         6554
    Memory Throughput                 %        46.04
    DRAM Throughput                   %        46.04
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.62
    L2 Cache Throughput               %        25.66
    SM Active Cycles              cycle      4410.71
    Compute (SM) Throughput           %        16.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.59
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23810.67
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4410.71
    Total L1 Elapsed Cycles          cycle       390070
    Average L2 Active Cycles         cycle      4059.79
    Total L2 Elapsed Cycles          cycle       163152
    Average SM Active Cycles         cycle      4410.71
    Total SM Elapsed Cycles          cycle       390070
    Average SMSP Active Cycles       cycle      4406.35
    Total SMSP Elapsed Cycles        cycle      1560280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.553%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.30% above the average, while the minimum instance value is 3.22% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.68
    Elapsed Cycles                cycle         7233
    Memory Throughput                 %        48.70
    DRAM Throughput                   %        48.70
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.20
    L2 Cache Throughput               %        27.48
    SM Active Cycles              cycle      4955.57
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.55
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4955.57
    Total L1 Elapsed Cycles          cycle       405032
    Average L2 Active Cycles         cycle      4461.71
    Total L2 Elapsed Cycles          cycle       180240
    Average SM Active Cycles         cycle      4955.57
    Total SM Elapsed Cycles          cycle       405032
    Average SMSP Active Cycles       cycle      4843.77
    Total SMSP Elapsed Cycles        cycle      1620128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.036%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.26% above the average, while the minimum instance value is 8.91% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.16
    Elapsed Cycles                cycle         7178
    Memory Throughput                 %        49.27
    DRAM Throughput                   %        49.27
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.37
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4988.53
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.86
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.53
    Total L1 Elapsed Cycles          cycle       419424
    Average L2 Active Cycles         cycle      4564.83
    Total L2 Elapsed Cycles          cycle       178608
    Average SM Active Cycles         cycle      4988.53
    Total SM Elapsed Cycles          cycle       419424
    Average SMSP Active Cycles       cycle      4945.77
    Total SMSP Elapsed Cycles        cycle      1677696
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.59
    Elapsed Cycles                cycle         7232
    Memory Throughput                 %        48.94
    DRAM Throughput                   %        48.94
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      5069.22
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5069.22
    Total L1 Elapsed Cycles          cycle       409354
    Average L2 Active Cycles         cycle      4516.29
    Total L2 Elapsed Cycles          cycle       180072
    Average SM Active Cycles         cycle      5069.22
    Total SM Elapsed Cycles          cycle       409354
    Average SMSP Active Cycles       cycle      4925.82
    Total SMSP Elapsed Cycles        cycle      1637416
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4442333
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205529.10
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77306.67
    Total DRAM Elapsed Cycles        cycle    209368064
    Average L1 Active Cycles         cycle   3205529.10
    Total L1 Elapsed Cycles          cycle    257637094
    Average L2 Active Cycles         cycle   1038508.54
    Total L2 Elapsed Cycles          cycle    110639208
    Average SM Active Cycles         cycle   3205529.10
    Total SM Elapsed Cycles          cycle    257637094
    Average SMSP Active Cycles       cycle   3204759.08
    Total SMSP Elapsed Cycles        cycle   1030548376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.18%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.96%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.66% above the average, while the minimum instance value is 10.34% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.18%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       781.32
    Elapsed Cycles                cycle         6553
    Memory Throughput                 %        46.82
    DRAM Throughput                   %        46.82
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.23
    L2 Cache Throughput               %        25.70
    SM Active Cycles              cycle      4478.36
    Compute (SM) Throughput           %        17.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.65
    Achieved Active Warps Per SM           warp        33.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.35%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24053.33
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4478.36
    Total L1 Elapsed Cycles          cycle       381562
    Average L2 Active Cycles         cycle      4110.17
    Total L2 Elapsed Cycles          cycle       163080
    Average SM Active Cycles         cycle      4478.36
    Total SM Elapsed Cycles          cycle       381562
    Average SMSP Active Cycles       cycle      4442.94
    Total SMSP Elapsed Cycles        cycle      1526248
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.52
    Elapsed Cycles                cycle         7139
    Memory Throughput                 %        49.44
    DRAM Throughput                   %        49.44
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        27.86
    SM Active Cycles              cycle      4880.91
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.47
    Achieved Active Warps Per SM           warp        36.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.53%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4880.91
    Total L1 Elapsed Cycles          cycle       417496
    Average L2 Active Cycles         cycle      4488.58
    Total L2 Elapsed Cycles          cycle       177624
    Average SM Active Cycles         cycle      4880.91
    Total SM Elapsed Cycles          cycle       417496
    Average SMSP Active Cycles       cycle      4911.67
    Total SMSP Elapsed Cycles        cycle      1669984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.57
    Elapsed Cycles                cycle         7132
    Memory Throughput                 %        49.41
    DRAM Throughput                   %        49.41
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.92
    SM Active Cycles              cycle      4944.53
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.09
    Achieved Active Warps Per SM           warp        35.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.91%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4944.53
    Total L1 Elapsed Cycles          cycle       414322
    Average L2 Active Cycles         cycle      4531.08
    Total L2 Elapsed Cycles          cycle       177504
    Average SM Active Cycles         cycle      4944.53
    Total SM Elapsed Cycles          cycle       414322
    Average SMSP Active Cycles       cycle      4936.22
    Total SMSP Elapsed Cycles        cycle      1657288
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.61
    Elapsed Cycles                cycle         7173
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      4944.22
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.04
    Achieved Active Warps Per SM           warp        36.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4944.22
    Total L1 Elapsed Cycles          cycle       413286
    Average L2 Active Cycles         cycle      4468.29
    Total L2 Elapsed Cycles          cycle       178512
    Average SM Active Cycles         cycle      4944.22
    Total SM Elapsed Cycles          cycle       413286
    Average SMSP Active Cycles       cycle      4870.16
    Total SMSP Elapsed Cycles        cycle      1653144
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4451100
    Memory Throughput                 %        51.06
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204936.31
    Compute (SM) Throughput           %        51.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78008
    Total DRAM Elapsed Cycles        cycle    209780736
    Average L1 Active Cycles         cycle   3204936.31
    Total L1 Elapsed Cycles          cycle    257010232
    Average L2 Active Cycles         cycle   1024878.58
    Total L2 Elapsed Cycles          cycle    110860752
    Average SM Active Cycles         cycle   3204936.31
    Total SM Elapsed Cycles          cycle    257010232
    Average SMSP Active Cycles       cycle   3205129.73
    Total SMSP Elapsed Cycles        cycle   1028040928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.68% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.22%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.95% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       781.02
    Elapsed Cycles                cycle         6749
    Memory Throughput                 %        45.08
    DRAM Throughput                   %        45.08
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        25.43
    L2 Cache Throughput               %        24.94
    SM Active Cycles              cycle      4443.57
    Compute (SM) Throughput           %        17.29
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.21
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.79%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23848
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4443.57
    Total L1 Elapsed Cycles          cycle       379130
    Average L2 Active Cycles         cycle      4034.96
    Total L2 Elapsed Cycles          cycle       168000
    Average SM Active Cycles         cycle      4443.57
    Total SM Elapsed Cycles          cycle       379130
    Average SMSP Active Cycles       cycle      4389.19
    Total SMSP Elapsed Cycles        cycle      1516520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.12
    Elapsed Cycles                cycle         7154
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.92
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      4988.67
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.40
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.67
    Total L1 Elapsed Cycles          cycle       410062
    Average L2 Active Cycles         cycle      4510.29
    Total L2 Elapsed Cycles          cycle       178008
    Average SM Active Cycles         cycle      4988.67
    Total SM Elapsed Cycles          cycle       410062
    Average SMSP Active Cycles       cycle      4927.98
    Total SMSP Elapsed Cycles        cycle      1640248
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.21
    Elapsed Cycles                cycle         7188
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.53
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      4901.07
    Compute (SM) Throughput           %        16.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.97
    Achieved Active Warps Per SM           warp        35.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4901.07
    Total L1 Elapsed Cycles          cycle       399586
    Average L2 Active Cycles         cycle      4525.50
    Total L2 Elapsed Cycles          cycle       178896
    Average SM Active Cycles         cycle      4901.07
    Total SM Elapsed Cycles          cycle       399586
    Average SMSP Active Cycles       cycle      4940.69
    Total SMSP Elapsed Cycles        cycle      1598344
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.39
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.03
    DRAM Throughput                   %        49.03
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        27.58
    SM Active Cycles              cycle      4974.40
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.02
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4974.40
    Total L1 Elapsed Cycles          cycle       405220
    Average L2 Active Cycles         cycle      4456.17
    Total L2 Elapsed Cycles          cycle       179424
    Average SM Active Cycles         cycle      4974.40
    Total SM Elapsed Cycles          cycle       405220
    Average SMSP Active Cycles       cycle      4857.61
    Total SMSP Elapsed Cycles        cycle      1620880
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4431059
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.57
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205354.84
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77765.33
    Total DRAM Elapsed Cycles        cycle    208834560
    Average L1 Active Cycles         cycle   3205354.84
    Total L1 Elapsed Cycles          cycle    257376578
    Average L2 Active Cycles         cycle      1025077
    Total L2 Elapsed Cycles          cycle    110358408
    Average SM Active Cycles         cycle   3205354.84
    Total SM Elapsed Cycles          cycle    257376578
    Average SMSP Active Cycles       cycle   3205009.07
    Total SMSP Elapsed Cycles        cycle   1029506312
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.81% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.29
    Elapsed Cycles                cycle         6762
    Memory Throughput                 %        45.44
    DRAM Throughput                   %        45.44
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.18
    L2 Cache Throughput               %        24.89
    SM Active Cycles              cycle      4486.81
    Compute (SM) Throughput           %        17.34
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.82
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24040
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4486.81
    Total L1 Elapsed Cycles          cycle       377986
    Average L2 Active Cycles         cycle      4105.50
    Total L2 Elapsed Cycles          cycle       168264
    Average SM Active Cycles         cycle      4486.81
    Total SM Elapsed Cycles          cycle       377986
    Average SMSP Active Cycles       cycle      4454.94
    Total SMSP Elapsed Cycles        cycle      1511944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.85
    Elapsed Cycles                cycle         7160
    Memory Throughput                 %        49.28
    DRAM Throughput                   %        49.28
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      5012.81
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.66
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.34%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5012.81
    Total L1 Elapsed Cycles          cycle       413524
    Average L2 Active Cycles         cycle      4526.04
    Total L2 Elapsed Cycles          cycle       178152
    Average SM Active Cycles         cycle      5012.81
    Total SM Elapsed Cycles          cycle       413524
    Average SMSP Active Cycles       cycle      4952.56
    Total SMSP Elapsed Cycles        cycle      1654096
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.62
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.23
    DRAM Throughput                   %        49.23
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.26
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      4990.10
    Compute (SM) Throughput           %        16.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4990.10
    Total L1 Elapsed Cycles          cycle       404176
    Average L2 Active Cycles         cycle      4498.50
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      4990.10
    Total SM Elapsed Cycles          cycle       404176
    Average SMSP Active Cycles       cycle      4946.94
    Total SMSP Elapsed Cycles        cycle      1616704
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.48
    Elapsed Cycles                cycle         7173
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      5007.95
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27706.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5007.95
    Total L1 Elapsed Cycles          cycle       408782
    Average L2 Active Cycles         cycle      4392.46
    Total L2 Elapsed Cycles          cycle       178584
    Average SM Active Cycles         cycle      5007.95
    Total SM Elapsed Cycles          cycle       408782
    Average SMSP Active Cycles       cycle      4889.60
    Total SMSP Elapsed Cycles        cycle      1635128
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4451452
    Memory Throughput                 %        50.82
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205568.97
    Compute (SM) Throughput           %        50.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77370.67
    Total DRAM Elapsed Cycles        cycle    209796096
    Average L1 Active Cycles         cycle   3205568.97
    Total L1 Elapsed Cycles          cycle    258210480
    Average L2 Active Cycles         cycle   1015014.33
    Total L2 Elapsed Cycles          cycle    110873952
    Average SM Active Cycles         cycle   3205568.97
    Total SM Elapsed Cycles          cycle    258210480
    Average SMSP Active Cycles       cycle   3205210.62
    Total SMSP Elapsed Cycles        cycle   1032841920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.85% above the average, while the minimum instance value is 10.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.17%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 28.01% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.85% above the average, while the minimum instance value is 10.40% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       780.36
    Elapsed Cycles                cycle         6644
    Memory Throughput                 %        45.51
    DRAM Throughput                   %        45.51
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.95
    L2 Cache Throughput               %        25.35
    SM Active Cycles              cycle      4353.52
    Compute (SM) Throughput           %        17.45
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23765.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4353.52
    Total L1 Elapsed Cycles          cycle       375498
    Average L2 Active Cycles         cycle      3999.83
    Total L2 Elapsed Cycles          cycle       165384
    Average SM Active Cycles         cycle      4353.52
    Total SM Elapsed Cycles          cycle       375498
    Average SMSP Active Cycles       cycle      4332.22
    Total SMSP Elapsed Cycles        cycle      1501992
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.267%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.07% above the average, while the minimum instance value is 2.70% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.67
    Elapsed Cycles                cycle         7258
    Memory Throughput                 %        48.62
    DRAM Throughput                   %        48.62
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.41
    SM Active Cycles              cycle      5015.38
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.03
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5015.38
    Total L1 Elapsed Cycles          cycle       414064
    Average L2 Active Cycles         cycle      4533.67
    Total L2 Elapsed Cycles          cycle       180624
    Average SM Active Cycles         cycle      5015.38
    Total SM Elapsed Cycles          cycle       414064
    Average SMSP Active Cycles       cycle      4933.57
    Total SMSP Elapsed Cycles        cycle      1656256
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.01
    Elapsed Cycles                cycle         7227
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      4924.64
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4924.64
    Total L1 Elapsed Cycles          cycle       411640
    Average L2 Active Cycles         cycle      4493.21
    Total L2 Elapsed Cycles          cycle       179904
    Average SM Active Cycles         cycle      4924.64
    Total SM Elapsed Cycles          cycle       411640
    Average SMSP Active Cycles       cycle      4910.90
    Total SMSP Elapsed Cycles        cycle      1646560
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.64
    Elapsed Cycles                cycle         7148
    Memory Throughput                 %        49.42
    DRAM Throughput                   %        49.42
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.37
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4963.90
    Compute (SM) Throughput           %        16.29
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4963.90
    Total L1 Elapsed Cycles          cycle       402292
    Average L2 Active Cycles         cycle      4503.38
    Total L2 Elapsed Cycles          cycle       177912
    Average SM Active Cycles         cycle      4963.90
    Total SM Elapsed Cycles          cycle       402292
    Average SMSP Active Cycles       cycle      4914.62
    Total SMSP Elapsed Cycles        cycle      1609168
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4436906
    Memory Throughput                 %        50.97
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205150.74
    Compute (SM) Throughput           %        50.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78600
    Total DRAM Elapsed Cycles        cycle    209114112
    Average L1 Active Cycles         cycle   3205150.74
    Total L1 Elapsed Cycles          cycle    257451122
    Average L2 Active Cycles         cycle   1030339.50
    Total L2 Elapsed Cycles          cycle    110506584
    Average SM Active Cycles         cycle   3205150.74
    Total SM Elapsed Cycles          cycle    257451122
    Average SMSP Active Cycles       cycle   3205563.56
    Total SMSP Elapsed Cycles        cycle   1029804488
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.68% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.18%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.95% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.57
    Elapsed Cycles                cycle         6738
    Memory Throughput                 %        45.61
    DRAM Throughput                   %        45.61
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.55
    L2 Cache Throughput               %        24.99
    SM Active Cycles              cycle      4422.91
    Compute (SM) Throughput           %        17.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.49
    Achieved Active Warps Per SM           warp        35.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24050.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4422.91
    Total L1 Elapsed Cycles          cycle       383866
    Average L2 Active Cycles         cycle      4077.46
    Total L2 Elapsed Cycles          cycle       167664
    Average SM Active Cycles         cycle      4422.91
    Total SM Elapsed Cycles          cycle       383866
    Average SMSP Active Cycles       cycle      4428.39
    Total SMSP Elapsed Cycles        cycle      1535464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.71
    Elapsed Cycles                cycle         7190
    Memory Throughput                 %        49.21
    DRAM Throughput                   %        49.21
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.74
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      4950.98
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4950.98
    Total L1 Elapsed Cycles          cycle       412880
    Average L2 Active Cycles         cycle      4529.42
    Total L2 Elapsed Cycles          cycle       179088
    Average SM Active Cycles         cycle      4950.98
    Total SM Elapsed Cycles          cycle       412880
    Average SMSP Active Cycles       cycle      4962.44
    Total SMSP Elapsed Cycles        cycle      1651520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.26
    Elapsed Cycles                cycle         7244
    Memory Throughput                 %        49.04
    DRAM Throughput                   %        49.04
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.61
    L2 Cache Throughput               %        27.43
    SM Active Cycles              cycle      4988.17
    Compute (SM) Throughput           %        15.79
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.49
    Achieved Active Warps Per SM           warp        34.79
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27869.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4988.17
    Total L1 Elapsed Cycles          cycle       415084
    Average L2 Active Cycles         cycle      4397.92
    Total L2 Elapsed Cycles          cycle       180528
    Average SM Active Cycles         cycle      4988.17
    Total SM Elapsed Cycles          cycle       415084
    Average SMSP Active Cycles       cycle      4834.02
    Total SMSP Elapsed Cycles        cycle      1660336
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.87
    Elapsed Cycles                cycle         7159
    Memory Throughput                 %        49.35
    DRAM Throughput                   %        49.35
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      4963.98
    Compute (SM) Throughput           %        16.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.26
    Achieved Active Warps Per SM           warp        35.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4963.98
    Total L1 Elapsed Cycles          cycle       407730
    Average L2 Active Cycles         cycle      4498.88
    Total L2 Elapsed Cycles          cycle       178224
    Average SM Active Cycles         cycle      4963.98
    Total SM Elapsed Cycles          cycle       407730
    Average SMSP Active Cycles       cycle      4906.69
    Total SMSP Elapsed Cycles        cycle      1630920
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4442390
    Memory Throughput                 %        51.02
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204982.02
    Compute (SM) Throughput           %        51.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77882.67
    Total DRAM Elapsed Cycles        cycle    209370112
    Average L1 Active Cycles         cycle   3204982.02
    Total L1 Elapsed Cycles          cycle    257234558
    Average L2 Active Cycles         cycle   1025537.92
    Total L2 Elapsed Cycles          cycle    110640696
    Average SM Active Cycles         cycle   3204982.02
    Total SM Elapsed Cycles          cycle    257234558
    Average SMSP Active Cycles       cycle   3204779.28
    Total SMSP Elapsed Cycles        cycle   1028938232
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.72% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.72% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       781.44
    Elapsed Cycles                cycle         6528
    Memory Throughput                 %        46.40
    DRAM Throughput                   %        46.40
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.10
    L2 Cache Throughput               %        25.80
    SM Active Cycles              cycle      4501.62
    Compute (SM) Throughput           %        17.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.12
    Achieved Active Warps Per SM           warp        34.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.88%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23754.67
    Total DRAM Elapsed Cycles        cycle       307200
    Average L1 Active Cycles         cycle      4501.62
    Total L1 Elapsed Cycles          cycle       371854
    Average L2 Active Cycles         cycle      3978.17
    Total L2 Elapsed Cycles          cycle       162504
    Average SM Active Cycles         cycle      4501.62
    Total SM Elapsed Cycles          cycle       371854
    Average SMSP Active Cycles       cycle      4347.93
    Total SMSP Elapsed Cycles        cycle      1487416
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.207%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.86% above the average, while the minimum instance value is 2.89% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.69
    Elapsed Cycles                cycle         7202
    Memory Throughput                 %        48.90
    DRAM Throughput                   %        48.90
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.61
    SM Active Cycles              cycle      4968.84
    Compute (SM) Throughput           %        16.09
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27709.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4968.84
    Total L1 Elapsed Cycles          cycle       407434
    Average L2 Active Cycles         cycle      4476.08
    Total L2 Elapsed Cycles          cycle       179352
    Average SM Active Cycles         cycle      4968.84
    Total SM Elapsed Cycles          cycle       407434
    Average SMSP Active Cycles       cycle      4896.98
    Total SMSP Elapsed Cycles        cycle      1629736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.25
    Elapsed Cycles                cycle         7121
    Memory Throughput                 %        49.73
    DRAM Throughput                   %        49.73
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.38
    L2 Cache Throughput               %        27.95
    SM Active Cycles              cycle      4979.91
    Compute (SM) Throughput           %        16.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.54
    Achieved Active Warps Per SM           warp        34.82
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4979.91
    Total L1 Elapsed Cycles          cycle       402024
    Average L2 Active Cycles         cycle         4578
    Total L2 Elapsed Cycles          cycle       177192
    Average SM Active Cycles         cycle      4979.91
    Total SM Elapsed Cycles          cycle       402024
    Average SMSP Active Cycles       cycle      4959.95
    Total SMSP Elapsed Cycles        cycle      1608096
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.72
    Elapsed Cycles                cycle         7184
    Memory Throughput                 %        49.19
    DRAM Throughput                   %        49.19
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.16
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4951.78
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.39
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27701.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4951.78
    Total L1 Elapsed Cycles          cycle       405606
    Average L2 Active Cycles         cycle      4472.79
    Total L2 Elapsed Cycles          cycle       178776
    Average SM Active Cycles         cycle      4951.78
    Total SM Elapsed Cycles          cycle       405606
    Average SMSP Active Cycles       cycle      4875.25
    Total SMSP Elapsed Cycles        cycle      1622424
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4444317
    Memory Throughput                 %        50.82
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205583.90
    Compute (SM) Throughput           %        50.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77338.67
    Total DRAM Elapsed Cycles        cycle    209461248
    Average L1 Active Cycles         cycle   3205583.90
    Total L1 Elapsed Cycles          cycle    258241242
    Average L2 Active Cycles         cycle   1023983.75
    Total L2 Elapsed Cycles          cycle    110688576
    Average SM Active Cycles         cycle   3205583.90
    Total SM Elapsed Cycles          cycle    258241242
    Average SMSP Active Cycles       cycle   3204796.42
    Total SMSP Elapsed Cycles        cycle   1032964968
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.90% above the average, while the minimum instance value is 10.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.93% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.90% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.39
    Elapsed Cycles                cycle         6671
    Memory Throughput                 %        46.06
    DRAM Throughput                   %        46.06
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.63
    L2 Cache Throughput               %        25.23
    SM Active Cycles              cycle      4587.10
    Compute (SM) Throughput           %        17.40
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.08
    Achieved Active Warps Per SM           warp        33.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24056
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4587.10
    Total L1 Elapsed Cycles          cycle       376722
    Average L2 Active Cycles         cycle      3965.42
    Total L2 Elapsed Cycles          cycle       166032
    Average SM Active Cycles         cycle      4587.10
    Total SM Elapsed Cycles          cycle       376722
    Average SMSP Active Cycles       cycle      4335.44
    Total SMSP Elapsed Cycles        cycle      1506888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.03
    Elapsed Cycles                cycle         7258
    Memory Throughput                 %        48.54
    DRAM Throughput                   %        48.54
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        27.41
    SM Active Cycles              cycle      4980.91
    Compute (SM) Throughput           %        15.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4980.91
    Total L1 Elapsed Cycles          cycle       419598
    Average L2 Active Cycles         cycle      4488.92
    Total L2 Elapsed Cycles          cycle       180672
    Average SM Active Cycles         cycle      4980.91
    Total SM Elapsed Cycles          cycle       419598
    Average SMSP Active Cycles       cycle      4930.11
    Total SMSP Elapsed Cycles        cycle      1678392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.81
    Elapsed Cycles                cycle         7177
    Memory Throughput                 %        49.37
    DRAM Throughput                   %        49.37
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      4970.17
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.03
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27802.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4970.17
    Total L1 Elapsed Cycles          cycle       410684
    Average L2 Active Cycles         cycle      4504.08
    Total L2 Elapsed Cycles          cycle       178488
    Average SM Active Cycles         cycle      4970.17
    Total SM Elapsed Cycles          cycle       410684
    Average SMSP Active Cycles       cycle      4958.25
    Total SMSP Elapsed Cycles        cycle      1642736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       786.15
    Elapsed Cycles                cycle         7347
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.42
    L2 Cache Throughput               %        27.08
    SM Active Cycles              cycle      4991.40
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.04
    Achieved Active Warps Per SM           warp        35.54
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4991.40
    Total L1 Elapsed Cycles          cycle       418362
    Average L2 Active Cycles         cycle      4504.67
    Total L2 Elapsed Cycles          cycle       182880
    Average SM Active Cycles         cycle      4991.40
    Total SM Elapsed Cycles          cycle       418362
    Average SMSP Active Cycles       cycle      4913.24
    Total SMSP Elapsed Cycles        cycle      1673448
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4444512
    Memory Throughput                 %        51.01
    DRAM Throughput                   %         0.23
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205555.78
    Compute (SM) Throughput           %        51.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     81354.67
    Total DRAM Elapsed Cycles        cycle    209470464
    Average L1 Active Cycles         cycle   3205555.78
    Total L1 Elapsed Cycles          cycle    257277504
    Average L2 Active Cycles         cycle      1022313
    Total L2 Elapsed Cycles          cycle    110693376
    Average SM Active Cycles         cycle   3205555.78
    Total SM Elapsed Cycles          cycle    257277504
    Average SMSP Active Cycles       cycle   3204617.17
    Total SMSP Elapsed Cycles        cycle   1029110016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.82% above the average, while the minimum instance value is 10.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.22%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.98% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.44
    Elapsed Cycles                cycle         6579
    Memory Throughput                 %        45.97
    DRAM Throughput                   %        45.97
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.22
    L2 Cache Throughput               %        25.57
    SM Active Cycles              cycle      4480.76
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.39
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.61%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23770.67
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4480.76
    Total L1 Elapsed Cycles          cycle       381888
    Average L2 Active Cycles         cycle      3940.71
    Total L2 Elapsed Cycles          cycle       163824
    Average SM Active Cycles         cycle      4480.76
    Total SM Elapsed Cycles          cycle       381888
    Average SMSP Active Cycles       cycle      4296.72
    Total SMSP Elapsed Cycles        cycle      1527552
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.42%                                                                                           
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.39% above the average, while the minimum instance value is 3.14% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.77
    Elapsed Cycles                cycle         7150
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.81
    SM Active Cycles              cycle      4988.76
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.70
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.76
    Total L1 Elapsed Cycles          cycle       412036
    Average L2 Active Cycles         cycle      4526.29
    Total L2 Elapsed Cycles          cycle       178104
    Average SM Active Cycles         cycle      4988.76
    Total SM Elapsed Cycles          cycle       412036
    Average SMSP Active Cycles       cycle      4919.15
    Total SMSP Elapsed Cycles        cycle      1648144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.24
    Elapsed Cycles                cycle         7074
    Memory Throughput                 %        50.01
    DRAM Throughput                   %        50.01
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        24.14
    L2 Cache Throughput               %        28.12
    SM Active Cycles              cycle      4930.17
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.18
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       332800
    Average L1 Active Cycles         cycle      4930.17
    Total L1 Elapsed Cycles          cycle       406348
    Average L2 Active Cycles         cycle      4557.83
    Total L2 Elapsed Cycles          cycle       176040
    Average SM Active Cycles         cycle      4930.17
    Total SM Elapsed Cycles          cycle       406348
    Average SMSP Active Cycles       cycle      4948.47
    Total SMSP Elapsed Cycles        cycle      1625392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.46
    Elapsed Cycles                cycle         7149
    Memory Throughput                 %        49.47
    DRAM Throughput                   %        49.47
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.81
    SM Active Cycles              cycle      4976.59
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.81
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4976.59
    Total L1 Elapsed Cycles          cycle       409902
    Average L2 Active Cycles         cycle      4530.83
    Total L2 Elapsed Cycles          cycle       178104
    Average SM Active Cycles         cycle      4976.59
    Total SM Elapsed Cycles          cycle       409902
    Average SMSP Active Cycles       cycle      4918.21
    Total SMSP Elapsed Cycles        cycle      1639608
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4441299
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204946.38
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77181.33
    Total DRAM Elapsed Cycles        cycle    209319936
    Average L1 Active Cycles         cycle   3204946.38
    Total L1 Elapsed Cycles          cycle    257611482
    Average L2 Active Cycles         cycle   1039233.33
    Total L2 Elapsed Cycles          cycle    110613408
    Average SM Active Cycles         cycle   3204946.38
    Total SM Elapsed Cycles          cycle    257611482
    Average SMSP Active Cycles       cycle   3205279.56
    Total SMSP Elapsed Cycles        cycle   1030445928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.71% above the average, while the minimum instance value is 10.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.59% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.71% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.36
    Elapsed Cycles                cycle         6547
    Memory Throughput                 %        46.72
    DRAM Throughput                   %        46.72
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        24.81
    L2 Cache Throughput               %        25.71
    SM Active Cycles              cycle      4553.47
    Compute (SM) Throughput           %        17.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.46
    Achieved Active Warps Per SM           warp        33.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.54%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24002.67
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4553.47
    Total L1 Elapsed Cycles          cycle       379370
    Average L2 Active Cycles         cycle      3988.75
    Total L2 Elapsed Cycles          cycle       163080
    Average SM Active Cycles         cycle      4553.47
    Total SM Elapsed Cycles          cycle       379370
    Average SMSP Active Cycles       cycle      4350.10
    Total SMSP Elapsed Cycles        cycle      1517480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.63
    Elapsed Cycles                cycle         7167
    Memory Throughput                 %        49.27
    DRAM Throughput                   %        49.27
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      4917.55
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.61
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4917.55
    Total L1 Elapsed Cycles          cycle       409556
    Average L2 Active Cycles         cycle      4469.67
    Total L2 Elapsed Cycles          cycle       178440
    Average SM Active Cycles         cycle      4917.55
    Total SM Elapsed Cycles          cycle       409556
    Average SMSP Active Cycles       cycle      4905.08
    Total SMSP Elapsed Cycles        cycle      1638224
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.21
    Elapsed Cycles                cycle         7122
    Memory Throughput                 %        49.63
    DRAM Throughput                   %        49.63
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.65
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4957.09
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4957.09
    Total L1 Elapsed Cycles          cycle       414364
    Average L2 Active Cycles         cycle      4366.88
    Total L2 Elapsed Cycles          cycle       177432
    Average SM Active Cycles         cycle      4957.09
    Total SM Elapsed Cycles          cycle       414364
    Average SMSP Active Cycles       cycle      4802.83
    Total SMSP Elapsed Cycles        cycle      1657456
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.34
    Elapsed Cycles                cycle         7218
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.88
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      4918.93
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.86
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.14%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4918.93
    Total L1 Elapsed Cycles          cycle       410540
    Average L2 Active Cycles         cycle      4497.71
    Total L2 Elapsed Cycles          cycle       179760
    Average SM Active Cycles         cycle      4918.93
    Total SM Elapsed Cycles          cycle       410540
    Average SMSP Active Cycles       cycle      4925.97
    Total SMSP Elapsed Cycles        cycle      1642160
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4430932
    Memory Throughput                 %        51.02
    DRAM Throughput                   %         0.23
    Duration                         ms         5.57
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205443.45
    Compute (SM) Throughput           %        51.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.22
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        79824
    Total DRAM Elapsed Cycles        cycle    208831488
    Average L1 Active Cycles         cycle   3205443.45
    Total L1 Elapsed Cycles          cycle    257192134
    Average L2 Active Cycles         cycle   1025610.88
    Total L2 Elapsed Cycles          cycle    110355168
    Average SM Active Cycles         cycle   3205443.45
    Total SM Elapsed Cycles          cycle    257192134
    Average SMSP Active Cycles       cycle   3205284.39
    Total SMSP Elapsed Cycles        cycle   1028768536
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.96% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.96% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.09
    Elapsed Cycles                cycle         6542
    Memory Throughput                 %        46.49
    DRAM Throughput                   %        46.49
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.27
    L2 Cache Throughput               %        25.75
    SM Active Cycles              cycle      4471.55
    Compute (SM) Throughput           %        17.37
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.11
    Achieved Active Warps Per SM           warp        34.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23802.67
    Total DRAM Elapsed Cycles        cycle       307200
    Average L1 Active Cycles         cycle      4471.55
    Total L1 Elapsed Cycles          cycle       377258
    Average L2 Active Cycles         cycle      4036.50
    Total L2 Elapsed Cycles          cycle       162792
    Average SM Active Cycles         cycle      4471.55
    Total SM Elapsed Cycles          cycle       377258
    Average SMSP Active Cycles       cycle      4414.15
    Total SMSP Elapsed Cycles        cycle      1509032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.699%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.58% above the average, while the minimum instance value is 2.74% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.56
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.14
    DRAM Throughput                   %        49.14
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.13
    L2 Cache Throughput               %        27.63
    SM Active Cycles              cycle      4970.90
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4970.90
    Total L1 Elapsed Cycles          cycle       406496
    Average L2 Active Cycles         cycle      4428.50
    Total L2 Elapsed Cycles          cycle       179160
    Average SM Active Cycles         cycle      4970.90
    Total SM Elapsed Cycles          cycle       406496
    Average SMSP Active Cycles       cycle      4849.45
    Total SMSP Elapsed Cycles        cycle      1625984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.059%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.31% above the average, while the minimum instance value is 8.15% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.98
    Elapsed Cycles                cycle         7228
    Memory Throughput                 %        48.94
    DRAM Throughput                   %        48.94
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.16
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4971.78
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.80
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27816
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4971.78
    Total L1 Elapsed Cycles          cycle       405984
    Average L2 Active Cycles         cycle      4502.33
    Total L2 Elapsed Cycles          cycle       179904
    Average SM Active Cycles         cycle      4971.78
    Total SM Elapsed Cycles          cycle       405984
    Average SMSP Active Cycles       cycle      4924.74
    Total SMSP Elapsed Cycles        cycle      1623936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.74
    Elapsed Cycles                cycle         7243
    Memory Throughput                 %        48.71
    DRAM Throughput                   %        48.71
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.20
    L2 Cache Throughput               %        27.47
    SM Active Cycles              cycle      4928.07
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.05
    Achieved Active Warps Per SM           warp        35.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27848
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4928.07
    Total L1 Elapsed Cycles          cycle       404994
    Average L2 Active Cycles         cycle      4522.79
    Total L2 Elapsed Cycles          cycle       180240
    Average SM Active Cycles         cycle      4928.07
    Total SM Elapsed Cycles          cycle       404994
    Average SMSP Active Cycles       cycle      4920.20
    Total SMSP Elapsed Cycles        cycle      1619976
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4445044
    Memory Throughput                 %        50.86
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205439.84
    Compute (SM) Throughput           %        50.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77661.33
    Total DRAM Elapsed Cycles        cycle    209497088
    Average L1 Active Cycles         cycle   3205439.84
    Total L1 Elapsed Cycles          cycle    258039750
    Average L2 Active Cycles         cycle   1012355.79
    Total L2 Elapsed Cycles          cycle    110716008
    Average SM Active Cycles         cycle   3205439.84
    Total SM Elapsed Cycles          cycle    258039750
    Average SMSP Active Cycles       cycle   3205439.98
    Total SMSP Elapsed Cycles        cycle   1032159000
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.86% above the average, while the minimum instance value is 10.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.04%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.39% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.86% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.10
    Elapsed Cycles                cycle         6727
    Memory Throughput                 %        45.62
    DRAM Throughput                   %        45.62
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        24.81
    L2 Cache Throughput               %        24.99
    SM Active Cycles              cycle      4553.97
    Compute (SM) Throughput           %        17.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.24
    Achieved Active Warps Per SM           warp        34.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24056
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4553.97
    Total L1 Elapsed Cycles          cycle       372060
    Average L2 Active Cycles         cycle      4093.25
    Total L2 Elapsed Cycles          cycle       167568
    Average SM Active Cycles         cycle      4553.97
    Total SM Elapsed Cycles          cycle       372060
    Average SMSP Active Cycles       cycle      4409.61
    Total SMSP Elapsed Cycles        cycle      1488240
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.73
    Elapsed Cycles                cycle         7268
    Memory Throughput                 %        48.87
    DRAM Throughput                   %        48.87
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      5006.97
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.51
    Achieved Active Warps Per SM           warp        35.28
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5006.97
    Total L1 Elapsed Cycles          cycle       411762
    Average L2 Active Cycles         cycle      4378.54
    Total L2 Elapsed Cycles          cycle       180936
    Average SM Active Cycles         cycle      5006.97
    Total SM Elapsed Cycles          cycle       411762
    Average SMSP Active Cycles       cycle      4810.34
    Total SMSP Elapsed Cycles        cycle      1647048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.12
    Elapsed Cycles                cycle         7119
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      4951.02
    Compute (SM) Throughput           %        15.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.25
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27738.67
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4951.02
    Total L1 Elapsed Cycles          cycle       412518
    Average L2 Active Cycles         cycle      4554.08
    Total L2 Elapsed Cycles          cycle       177144
    Average SM Active Cycles         cycle      4951.02
    Total SM Elapsed Cycles          cycle       412518
    Average SMSP Active Cycles       cycle      4925.10
    Total SMSP Elapsed Cycles        cycle      1650072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.049%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.18% above the average, while the minimum instance value is 4.39% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.12
    Elapsed Cycles                cycle         7153
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        27.80
    SM Active Cycles              cycle      5014.26
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5014.26
    Total L1 Elapsed Cycles          cycle       418134
    Average L2 Active Cycles         cycle      4481.71
    Total L2 Elapsed Cycles          cycle       177960
    Average SM Active Cycles         cycle      5014.26
    Total SM Elapsed Cycles          cycle       418134
    Average SMSP Active Cycles       cycle      4904.50
    Total SMSP Elapsed Cycles        cycle      1672536
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4441313
    Memory Throughput                 %        50.83
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204981.48
    Compute (SM) Throughput           %        50.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77224
    Total DRAM Elapsed Cycles        cycle    209319936
    Average L1 Active Cycles         cycle   3204981.48
    Total L1 Elapsed Cycles          cycle    258187898
    Average L2 Active Cycles         cycle   1026394.62
    Total L2 Elapsed Cycles          cycle    110613840
    Average SM Active Cycles         cycle   3204981.48
    Total SM Elapsed Cycles          cycle    258187898
    Average SMSP Active Cycles       cycle   3205286.97
    Total SMSP Elapsed Cycles        cycle   1032751592
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.77% above the average, while the minimum instance value is 10.50% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.40
    Elapsed Cycles                cycle         6687
    Memory Throughput                 %        45.17
    DRAM Throughput                   %        45.17
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.84
    L2 Cache Throughput               %        25.21
    SM Active Cycles              cycle      4372.36
    Compute (SM) Throughput           %        17.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.83
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23744
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4372.36
    Total L1 Elapsed Cycles          cycle       371328
    Average L2 Active Cycles         cycle      4146.17
    Total L2 Elapsed Cycles          cycle       166416
    Average SM Active Cycles         cycle      4372.36
    Total SM Elapsed Cycles          cycle       371328
    Average SMSP Active Cycles       cycle      4460.83
    Total SMSP Elapsed Cycles        cycle      1485312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.33
    Elapsed Cycles                cycle         7433
    Memory Throughput                 %        47.64
    DRAM Throughput                   %        47.64
    Duration                         us         9.47
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        26.74
    SM Active Cycles              cycle      4973.67
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.67
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27805.33
    Total DRAM Elapsed Cycles        cycle       350208
    Average L1 Active Cycles         cycle      4973.67
    Total L1 Elapsed Cycles          cycle       410202
    Average L2 Active Cycles         cycle      4498.75
    Total L2 Elapsed Cycles          cycle       185256
    Average SM Active Cycles         cycle      4973.67
    Total SM Elapsed Cycles          cycle       410202
    Average SMSP Active Cycles       cycle      4911.75
    Total SMSP Elapsed Cycles        cycle      1640808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.06
    Elapsed Cycles                cycle         7205
    Memory Throughput                 %        48.90
    DRAM Throughput                   %        48.90
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.13
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      5002.79
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.82
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27706.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5002.79
    Total L1 Elapsed Cycles          cycle       406328
    Average L2 Active Cycles         cycle      4515.50
    Total L2 Elapsed Cycles          cycle       179472
    Average SM Active Cycles         cycle      5002.79
    Total SM Elapsed Cycles          cycle       406328
    Average SMSP Active Cycles       cycle      4907.88
    Total SMSP Elapsed Cycles        cycle      1625312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.74
    Elapsed Cycles                cycle         7175
    Memory Throughput                 %        49.34
    DRAM Throughput                   %        49.34
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      5065.45
    Compute (SM) Throughput           %        16.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.82
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5065.45
    Total L1 Elapsed Cycles          cycle       402876
    Average L2 Active Cycles         cycle      4478.67
    Total L2 Elapsed Cycles          cycle       178704
    Average SM Active Cycles         cycle      5065.45
    Total SM Elapsed Cycles          cycle       402876
    Average SMSP Active Cycles       cycle      4888.38
    Total SMSP Elapsed Cycles        cycle      1611504
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4433968
    Memory Throughput                 %        50.98
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204895.28
    Compute (SM) Throughput           %        50.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78805.33
    Total DRAM Elapsed Cycles        cycle    208972800
    Average L1 Active Cycles         cycle   3204895.28
    Total L1 Elapsed Cycles          cycle    257436538
    Average L2 Active Cycles         cycle   1016889.88
    Total L2 Elapsed Cycles          cycle    110430696
    Average SM Active Cycles         cycle   3204895.28
    Total SM Elapsed Cycles          cycle    257436538
    Average SMSP Active Cycles       cycle   3204185.68
    Total SMSP Elapsed Cycles        cycle   1029746152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.68% above the average, while the minimum instance value is 10.56% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.33% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.56% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.40
    Elapsed Cycles                cycle         6681
    Memory Throughput                 %        45.75
    DRAM Throughput                   %        45.75
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.02
    L2 Cache Throughput               %        25.21
    SM Active Cycles              cycle      4516.95
    Compute (SM) Throughput           %        17.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.55
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24048
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4516.95
    Total L1 Elapsed Cycles          cycle       382268
    Average L2 Active Cycles         cycle      4104.21
    Total L2 Elapsed Cycles          cycle       166152
    Average SM Active Cycles         cycle      4516.95
    Total SM Elapsed Cycles          cycle       382268
    Average SMSP Active Cycles       cycle      4441.68
    Total SMSP Elapsed Cycles        cycle      1529072
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.70
    Elapsed Cycles                cycle         7186
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.22
    L2 Cache Throughput               %        27.69
    SM Active Cycles              cycle      4880.83
    Compute (SM) Throughput           %        16.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.34
    Achieved Active Warps Per SM           warp        35.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4880.83
    Total L1 Elapsed Cycles          cycle       404912
    Average L2 Active Cycles         cycle      4488.08
    Total L2 Elapsed Cycles          cycle       178968
    Average SM Active Cycles         cycle      4880.83
    Total SM Elapsed Cycles          cycle       404912
    Average SMSP Active Cycles       cycle      4938.49
    Total SMSP Elapsed Cycles        cycle      1619648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.233%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.40% above the average, while the minimum instance value is 10.36% below the average.      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.94
    Elapsed Cycles                cycle         7148
    Memory Throughput                 %        49.35
    DRAM Throughput                   %        49.35
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.85
    SM Active Cycles              cycle      5017.64
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.53
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27794.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5017.64
    Total L1 Elapsed Cycles          cycle       410048
    Average L2 Active Cycles         cycle      4447.12
    Total L2 Elapsed Cycles          cycle       177984
    Average SM Active Cycles         cycle      5017.64
    Total SM Elapsed Cycles          cycle       410048
    Average SMSP Active Cycles       cycle      4876.89
    Total SMSP Elapsed Cycles        cycle      1640192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.23
    Elapsed Cycles                cycle         7206
    Memory Throughput                 %        48.95
    DRAM Throughput                   %        48.95
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      5005.71
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.10
    Achieved Active Warps Per SM           warp        35.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5005.71
    Total L1 Elapsed Cycles          cycle       408942
    Average L2 Active Cycles         cycle      4454.92
    Total L2 Elapsed Cycles          cycle       179472
    Average SM Active Cycles         cycle      5005.71
    Total SM Elapsed Cycles          cycle       408942
    Average SMSP Active Cycles       cycle      4898.57
    Total SMSP Elapsed Cycles        cycle      1635768
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4436900
    Memory Throughput                 %        50.89
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205571.79
    Compute (SM) Throughput           %        50.89
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79653.33
    Total DRAM Elapsed Cycles        cycle    209111040
    Average L1 Active Cycles         cycle   3205571.79
    Total L1 Elapsed Cycles          cycle    257864950
    Average L2 Active Cycles         cycle   1022452.12
    Total L2 Elapsed Cycles          cycle    110503728
    Average SM Active Cycles         cycle   3205571.79
    Total SM Elapsed Cycles          cycle    257864950
    Average SMSP Active Cycles       cycle   3205270.03
    Total SMSP Elapsed Cycles        cycle   1031459800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.18%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.99% above the average, while the minimum instance value is 10.54% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.18%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.99% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.20
    Elapsed Cycles                cycle         6612
    Memory Throughput                 %        45.72
    DRAM Throughput                   %        45.72
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.50
    L2 Cache Throughput               %        25.46
    SM Active Cycles              cycle      4430.43
    Compute (SM) Throughput           %        16.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23877.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4430.43
    Total L1 Elapsed Cycles          cycle       386258
    Average L2 Active Cycles         cycle      3990.38
    Total L2 Elapsed Cycles          cycle       164760
    Average SM Active Cycles         cycle      4430.43
    Total SM Elapsed Cycles          cycle       386258
    Average SMSP Active Cycles       cycle      4375.99
    Total SMSP Elapsed Cycles        cycle      1545032
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.303%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.12% above the average, while the minimum instance value is 3.04% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.59
    Elapsed Cycles                cycle         7225
    Memory Throughput                 %        48.82
    DRAM Throughput                   %        48.82
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.29
    L2 Cache Throughput               %        27.51
    SM Active Cycles              cycle      4978.67
    Compute (SM) Throughput           %        16.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4978.67
    Total L1 Elapsed Cycles          cycle       403766
    Average L2 Active Cycles         cycle         4479
    Total L2 Elapsed Cycles          cycle       180024
    Average SM Active Cycles         cycle      4978.67
    Total SM Elapsed Cycles          cycle       403766
    Average SMSP Active Cycles       cycle      4898.78
    Total SMSP Elapsed Cycles        cycle      1615064
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.12
    Elapsed Cycles                cycle         7222
    Memory Throughput                 %        49.15
    DRAM Throughput                   %        49.15
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        27.53
    SM Active Cycles              cycle      4923.88
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27850.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4923.88
    Total L1 Elapsed Cycles          cycle       411246
    Average L2 Active Cycles         cycle      4485.46
    Total L2 Elapsed Cycles          cycle       179832
    Average SM Active Cycles         cycle      4923.88
    Total SM Elapsed Cycles          cycle       411246
    Average SMSP Active Cycles       cycle      4872.56
    Total SMSP Elapsed Cycles        cycle      1644984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.21
    Elapsed Cycles                cycle         7206
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      5052.88
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.41
    Achieved Active Warps Per SM           warp        33.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5052.88
    Total L1 Elapsed Cycles          cycle       409910
    Average L2 Active Cycles         cycle      4466.12
    Total L2 Elapsed Cycles          cycle       179616
    Average SM Active Cycles         cycle      5052.88
    Total SM Elapsed Cycles          cycle       409910
    Average SMSP Active Cycles       cycle      4887.31
    Total SMSP Elapsed Cycles        cycle      1639640
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4440628
    Memory Throughput                 %        51.04
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.61
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204382.48
    Compute (SM) Throughput           %        51.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77309.33
    Total DRAM Elapsed Cycles        cycle    209286144
    Average L1 Active Cycles         cycle   3204382.48
    Total L1 Elapsed Cycles          cycle    257111958
    Average L2 Active Cycles         cycle   1014412.33
    Total L2 Elapsed Cycles          cycle    110615736
    Average SM Active Cycles         cycle   3204382.48
    Total SM Elapsed Cycles          cycle    257111958
    Average SMSP Active Cycles       cycle   3205247.68
    Total SMSP Elapsed Cycles        cycle   1028447832
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.12
    Elapsed Cycles                cycle         6718
    Memory Throughput                 %        45.36
    DRAM Throughput                   %        45.36
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.28
    L2 Cache Throughput               %        25.03
    SM Active Cycles              cycle      4469.10
    Compute (SM) Throughput           %        17.37
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.34
    Achieved Active Warps Per SM           warp        34.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23997.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4469.10
    Total L1 Elapsed Cycles          cycle       377220
    Average L2 Active Cycles         cycle      3958.71
    Total L2 Elapsed Cycles          cycle       167400
    Average SM Active Cycles         cycle      4469.10
    Total SM Elapsed Cycles          cycle       377220
    Average SMSP Active Cycles       cycle      4321.18
    Total SMSP Elapsed Cycles        cycle      1508880
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         7322
    Memory Throughput                 %        48.02
    DRAM Throughput                   %        48.02
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.33
    L2 Cache Throughput               %        27.19
    SM Active Cycles              cycle      4884.26
    Compute (SM) Throughput           %        15.60
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.28
    Achieved Active Warps Per SM           warp        36.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4884.26
    Total L1 Elapsed Cycles          cycle       420008
    Average L2 Active Cycles         cycle         4459
    Total L2 Elapsed Cycles          cycle       182232
    Average SM Active Cycles         cycle      4884.26
    Total SM Elapsed Cycles          cycle       420008
    Average SMSP Active Cycles       cycle      4880.42
    Total SMSP Elapsed Cycles        cycle      1680032
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.51
    Elapsed Cycles                cycle         7226
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4998.57
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.67
    Achieved Active Warps Per SM           warp        34.88
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.33%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4998.57
    Total L1 Elapsed Cycles          cycle       409270
    Average L2 Active Cycles         cycle      4472.92
    Total L2 Elapsed Cycles          cycle       179976
    Average SM Active Cycles         cycle      4998.57
    Total SM Elapsed Cycles          cycle       409270
    Average SMSP Active Cycles       cycle      4920.76
    Total SMSP Elapsed Cycles        cycle      1637080
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.39
    Elapsed Cycles                cycle         7167
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      4966.83
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.32
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.68%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4966.83
    Total L1 Elapsed Cycles          cycle       411494
    Average L2 Active Cycles         cycle      4499.42
    Total L2 Elapsed Cycles          cycle       178512
    Average SM Active Cycles         cycle      4966.83
    Total SM Elapsed Cycles          cycle       411494
    Average SMSP Active Cycles       cycle      4917.80
    Total SMSP Elapsed Cycles        cycle      1645976
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4452572
    Memory Throughput                 %        50.84
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204765.67
    Compute (SM) Throughput           %        50.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77346.67
    Total DRAM Elapsed Cycles        cycle    209850368
    Average L1 Active Cycles         cycle   3204765.67
    Total L1 Elapsed Cycles          cycle    258133222
    Average L2 Active Cycles         cycle   1038552.42
    Total L2 Elapsed Cycles          cycle    110895528
    Average SM Active Cycles         cycle   3204765.67
    Total SM Elapsed Cycles          cycle    258133222
    Average SMSP Active Cycles       cycle   3205334.05
    Total SMSP Elapsed Cycles        cycle   1032532888
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.94%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.69% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.13%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.95% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.94%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.69% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.81
    Elapsed Cycles                cycle         6532
    Memory Throughput                 %        46.35
    DRAM Throughput                   %        46.35
    Duration                         us         8.32
    L1/TEX Cache Throughput           %        25.93
    L2 Cache Throughput               %        25.76
    SM Active Cycles              cycle      4358.29
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.13
    Achieved Active Warps Per SM           warp        35.10
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.87%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23808
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4358.29
    Total L1 Elapsed Cycles          cycle       382556
    Average L2 Active Cycles         cycle      4024.58
    Total L2 Elapsed Cycles          cycle       162552
    Average SM Active Cycles         cycle      4358.29
    Total SM Elapsed Cycles          cycle       382556
    Average SMSP Active Cycles       cycle      4414.72
    Total SMSP Elapsed Cycles        cycle      1530224
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.082%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.55% above the average, while the minimum instance value is 3.24% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.88
    Elapsed Cycles                cycle         7184
    Memory Throughput                 %        49.30
    DRAM Throughput                   %        49.30
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.82
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      4932.47
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.98
    Achieved Active Warps Per SM           warp        34.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.02%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4932.47
    Total L1 Elapsed Cycles          cycle       411484
    Average L2 Active Cycles         cycle      4504.67
    Total L2 Elapsed Cycles          cycle       178824
    Average SM Active Cycles         cycle      4932.47
    Total SM Elapsed Cycles          cycle       411484
    Average SMSP Active Cycles       cycle      4919.79
    Total SMSP Elapsed Cycles        cycle      1645936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.68
    Elapsed Cycles                cycle         7225
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        27.52
    SM Active Cycles              cycle      4940.93
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.25
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4940.93
    Total L1 Elapsed Cycles          cycle       410120
    Average L2 Active Cycles         cycle      4592.33
    Total L2 Elapsed Cycles          cycle       179808
    Average SM Active Cycles         cycle      4940.93
    Total SM Elapsed Cycles          cycle       410120
    Average SMSP Active Cycles       cycle      4986.57
    Total SMSP Elapsed Cycles        cycle      1640480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.70
    Elapsed Cycles                cycle         7167
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.06
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      4971.52
    Compute (SM) Throughput           %        16.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.94
    Achieved Active Warps Per SM           warp        35.01
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4971.52
    Total L1 Elapsed Cycles          cycle       407520
    Average L2 Active Cycles         cycle      4464.38
    Total L2 Elapsed Cycles          cycle       178512
    Average SM Active Cycles         cycle      4971.52
    Total SM Elapsed Cycles          cycle       407520
    Average SMSP Active Cycles       cycle      4929.26
    Total SMSP Elapsed Cycles        cycle      1630080
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4439805
    Memory Throughput                 %        50.85
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205048.86
    Compute (SM) Throughput           %        50.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78098.67
    Total DRAM Elapsed Cycles        cycle    209249280
    Average L1 Active Cycles         cycle   3205048.86
    Total L1 Elapsed Cycles          cycle    258056606
    Average L2 Active Cycles         cycle   1021736.12
    Total L2 Elapsed Cycles          cycle    110576208
    Average SM Active Cycles         cycle   3205048.86
    Total SM Elapsed Cycles          cycle    258056606
    Average SMSP Active Cycles       cycle   3205481.88
    Total SMSP Elapsed Cycles        cycle   1032226424
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.01%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.78% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.93% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.01%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.50
    Elapsed Cycles                cycle         6663
    Memory Throughput                 %        45.82
    DRAM Throughput                   %        45.82
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        24.42
    L2 Cache Throughput               %        25.23
    SM Active Cycles              cycle      4627.43
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.43
    Achieved Active Warps Per SM           warp        33.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.57%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24008
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4627.43
    Total L1 Elapsed Cycles          cycle       382088
    Average L2 Active Cycles         cycle      4088.62
    Total L2 Elapsed Cycles          cycle       165960
    Average SM Active Cycles         cycle      4627.43
    Total SM Elapsed Cycles          cycle       382088
    Average SMSP Active Cycles       cycle      4449.66
    Total SMSP Elapsed Cycles        cycle      1528352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.61
    SM Active Cycles              cycle      5154.28
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.07
    Achieved Active Warps Per SM           warp        33.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5154.28
    Total L1 Elapsed Cycles          cycle       409664
    Average L2 Active Cycles         cycle      4482.58
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      5154.28
    Total SM Elapsed Cycles          cycle       409664
    Average SMSP Active Cycles       cycle      4891.53
    Total SMSP Elapsed Cycles        cycle      1638656
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.86
    Elapsed Cycles                cycle         7135
    Memory Throughput                 %        49.35
    DRAM Throughput                   %        49.35
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      4942.29
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4942.29
    Total L1 Elapsed Cycles          cycle       410948
    Average L2 Active Cycles         cycle      4460.71
    Total L2 Elapsed Cycles          cycle       177840
    Average SM Active Cycles         cycle      4942.29
    Total SM Elapsed Cycles          cycle       410948
    Average SMSP Active Cycles       cycle      4869.34
    Total SMSP Elapsed Cycles        cycle      1643792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.83
    Elapsed Cycles                cycle         7062
    Memory Throughput                 %        50.18
    DRAM Throughput                   %        50.18
    Duration                         us         8.99
    L1/TEX Cache Throughput           %        24.15
    L2 Cache Throughput               %        28.13
    SM Active Cycles              cycle      4972.59
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.00
    Achieved Active Warps Per SM           warp        34.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       331776
    Average L1 Active Cycles         cycle      4972.59
    Total L1 Elapsed Cycles          cycle       406026
    Average L2 Active Cycles         cycle      4554.96
    Total L2 Elapsed Cycles          cycle       175944
    Average SM Active Cycles         cycle      4972.59
    Total SM Elapsed Cycles          cycle       406026
    Average SMSP Active Cycles       cycle      4980.53
    Total SMSP Elapsed Cycles        cycle      1624104
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4440713
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205433.52
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77810.67
    Total DRAM Elapsed Cycles        cycle    209292288
    Average L1 Active Cycles         cycle   3205433.52
    Total L1 Elapsed Cycles          cycle    257615226
    Average L2 Active Cycles         cycle   1026119.83
    Total L2 Elapsed Cycles          cycle    110598744
    Average SM Active Cycles         cycle   3205433.52
    Total SM Elapsed Cycles          cycle    257615226
    Average SMSP Active Cycles       cycle   3204954.42
    Total SMSP Elapsed Cycles        cycle   1030460904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.96%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.66% above the average, while the minimum instance value is 10.43% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.76% above the average, while the minimum instance value is 10.42% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.96%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.66% above the average, while the minimum instance value is 10.43% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.41
    Elapsed Cycles                cycle         6546
    Memory Throughput                 %        46.27
    DRAM Throughput                   %        46.27
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        26.22
    L2 Cache Throughput               %        25.71
    SM Active Cycles              cycle      4309.05
    Compute (SM) Throughput           %        17.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.79
    Achieved Active Warps Per SM           warp        35.42
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23768
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4309.05
    Total L1 Elapsed Cycles          cycle       373046
    Average L2 Active Cycles         cycle      4048.67
    Total L2 Elapsed Cycles          cycle       162936
    Average SM Active Cycles         cycle      4309.05
    Total SM Elapsed Cycles          cycle       373046
    Average SMSP Active Cycles       cycle      4460.12
    Total SMSP Elapsed Cycles        cycle      1492184
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.766%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.67% above the average, while the minimum instance value is 3.13% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.77
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.21
    L2 Cache Throughput               %        27.69
    SM Active Cycles              cycle      4876.36
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.11
    Achieved Active Warps Per SM           warp        35.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4876.36
    Total L1 Elapsed Cycles          cycle       404952
    Average L2 Active Cycles         cycle      4453.67
    Total L2 Elapsed Cycles          cycle       178944
    Average SM Active Cycles         cycle      4876.36
    Total SM Elapsed Cycles          cycle       404952
    Average SMSP Active Cycles       cycle      4861.93
    Total SMSP Elapsed Cycles        cycle      1619808
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.07
    Elapsed Cycles                cycle         7237
    Memory Throughput                 %        49.09
    DRAM Throughput                   %        49.09
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.20
    L2 Cache Throughput               %        27.48
    SM Active Cycles              cycle      4923.29
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.85
    Achieved Active Warps Per SM           warp        35.45
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27901.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4923.29
    Total L1 Elapsed Cycles          cycle       405034
    Average L2 Active Cycles         cycle      4514.38
    Total L2 Elapsed Cycles          cycle       180168
    Average SM Active Cycles         cycle      4923.29
    Total SM Elapsed Cycles          cycle       405034
    Average SMSP Active Cycles       cycle      4936.97
    Total SMSP Elapsed Cycles        cycle      1620136
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       786.08
    Elapsed Cycles                cycle         7199
    Memory Throughput                 %        48.87
    DRAM Throughput                   %        48.87
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.61
    SM Active Cycles              cycle      5006.72
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.20
    Achieved Active Warps Per SM           warp        35.14
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5006.72
    Total L1 Elapsed Cycles          cycle       412024
    Average L2 Active Cycles         cycle      4564.62
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      5006.72
    Total SM Elapsed Cycles          cycle       412024
    Average SMSP Active Cycles       cycle      4959.81
    Total SMSP Elapsed Cycles        cycle      1648096
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4434154
    Memory Throughput                 %        50.95
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205137.40
    Compute (SM) Throughput           %        50.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78394.67
    Total DRAM Elapsed Cycles        cycle    208983040
    Average L1 Active Cycles         cycle   3205137.40
    Total L1 Elapsed Cycles          cycle    257562146
    Average L2 Active Cycles         cycle   1030029.17
    Total L2 Elapsed Cycles          cycle    110462784
    Average SM Active Cycles         cycle   3205137.40
    Total SM Elapsed Cycles          cycle    257562146
    Average SMSP Active Cycles       cycle   3205011.30
    Total SMSP Elapsed Cycles        cycle   1030248584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.78% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.97%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       781.68
    Elapsed Cycles                cycle         6606
    Memory Throughput                 %        46.01
    DRAM Throughput                   %        46.01
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.14
    L2 Cache Throughput               %        25.55
    SM Active Cycles              cycle      4495.34
    Compute (SM) Throughput           %        17.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.71
    Achieved Active Warps Per SM           warp        33.94
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4495.34
    Total L1 Elapsed Cycles          cycle       378496
    Average L2 Active Cycles         cycle      3971.38
    Total L2 Elapsed Cycles          cycle       164448
    Average SM Active Cycles         cycle      4495.34
    Total SM Elapsed Cycles          cycle       378496
    Average SMSP Active Cycles       cycle      4317.66
    Total SMSP Elapsed Cycles        cycle      1513984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.07
    Elapsed Cycles                cycle         7177
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.38
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4973.72
    Compute (SM) Throughput           %        15.63
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.91
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4973.72
    Total L1 Elapsed Cycles          cycle       419352
    Average L2 Active Cycles         cycle      4441.54
    Total L2 Elapsed Cycles          cycle       178656
    Average SM Active Cycles         cycle      4973.72
    Total SM Elapsed Cycles          cycle       419352
    Average SMSP Active Cycles       cycle      4869.95
    Total SMSP Elapsed Cycles        cycle      1677408
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.49
    Elapsed Cycles                cycle         7147
    Memory Throughput                 %        49.40
    DRAM Throughput                   %        49.40
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4926.66
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.42
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.58%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27821.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4926.66
    Total L1 Elapsed Cycles          cycle       406986
    Average L2 Active Cycles         cycle         4437
    Total L2 Elapsed Cycles          cycle       177840
    Average SM Active Cycles         cycle      4926.66
    Total SM Elapsed Cycles          cycle       406986
    Average SMSP Active Cycles       cycle      4842.86
    Total SMSP Elapsed Cycles        cycle      1627944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.86
    Elapsed Cycles                cycle         7133
    Memory Throughput                 %        49.84
    DRAM Throughput                   %        49.84
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        27.89
    SM Active Cycles              cycle      4995.60
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.27
    Achieved Active Warps Per SM           warp        35.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.73%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4995.60
    Total L1 Elapsed Cycles          cycle       413554
    Average L2 Active Cycles         cycle         4488
    Total L2 Elapsed Cycles          cycle       177552
    Average SM Active Cycles         cycle      4995.60
    Total SM Elapsed Cycles          cycle       413554
    Average SMSP Active Cycles       cycle      4903.91
    Total SMSP Elapsed Cycles        cycle      1654216
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4451139
    Memory Throughput                 %        51.04
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205546.97
    Compute (SM) Throughput           %        51.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77352
    Total DRAM Elapsed Cycles        cycle    209783808
    Average L1 Active Cycles         cycle   3205546.97
    Total L1 Elapsed Cycles          cycle    257093540
    Average L2 Active Cycles         cycle   1028779.83
    Total L2 Elapsed Cycles          cycle    110858544
    Average SM Active Cycles         cycle   3205546.97
    Total SM Elapsed Cycles          cycle    257093540
    Average SMSP Active Cycles       cycle   3206033.41
    Total SMSP Elapsed Cycles        cycle   1028374160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.79% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.35
    Elapsed Cycles                cycle         6639
    Memory Throughput                 %        45.52
    DRAM Throughput                   %        45.52
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.38
    L2 Cache Throughput               %        25.34
    SM Active Cycles              cycle      4451.33
    Compute (SM) Throughput           %        17.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.29
    Achieved Active Warps Per SM           warp        34.70
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23773.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4451.33
    Total L1 Elapsed Cycles          cycle       381540
    Average L2 Active Cycles         cycle      3922.38
    Total L2 Elapsed Cycles          cycle       165336
    Average SM Active Cycles         cycle      4451.33
    Total SM Elapsed Cycles          cycle       381540
    Average SMSP Active Cycles       cycle      4312.56
    Total SMSP Elapsed Cycles        cycle      1526160
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.276%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.27% above the average, while the minimum instance value is 2.99% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.71
    Elapsed Cycles                cycle         7152
    Memory Throughput                 %        49.45
    DRAM Throughput                   %        49.45
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.73
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      4871.78
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.11
    Achieved Active Warps Per SM           warp        35.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4871.78
    Total L1 Elapsed Cycles          cycle       413254
    Average L2 Active Cycles         cycle      4448.17
    Total L2 Elapsed Cycles          cycle       178152
    Average SM Active Cycles         cycle      4871.78
    Total SM Elapsed Cycles          cycle       413254
    Average SMSP Active Cycles       cycle      4845.04
    Total SMSP Elapsed Cycles        cycle      1653016
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       786.04
    Elapsed Cycles                cycle         7221
    Memory Throughput                 %        49.03
    DRAM Throughput                   %        49.03
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.95
    L2 Cache Throughput               %        27.56
    SM Active Cycles              cycle      5002.48
    Compute (SM) Throughput           %        16.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.95
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5002.48
    Total L1 Elapsed Cycles          cycle       409084
    Average L2 Active Cycles         cycle      4518.25
    Total L2 Elapsed Cycles          cycle       179736
    Average SM Active Cycles         cycle      5002.48
    Total SM Elapsed Cycles          cycle       409084
    Average SMSP Active Cycles       cycle      4944.48
    Total SMSP Elapsed Cycles        cycle      1636336
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.96
    Elapsed Cycles                cycle         7172
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.33
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      4988.36
    Compute (SM) Throughput           %        16.27
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.51
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27730.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.36
    Total L1 Elapsed Cycles          cycle       402736
    Average L2 Active Cycles         cycle      4570.96
    Total L2 Elapsed Cycles          cycle       178584
    Average SM Active Cycles         cycle      4988.36
    Total SM Elapsed Cycles          cycle       402736
    Average SMSP Active Cycles       cycle      4982.28
    Total SMSP Elapsed Cycles        cycle      1610944
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4437783
    Memory Throughput                 %        50.92
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205308.45
    Compute (SM) Throughput           %        50.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.21
    Achieved Active Warps Per SM           warp        14.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78077.33
    Total DRAM Elapsed Cycles        cycle    209154048
    Average L1 Active Cycles         cycle   3205308.45
    Total L1 Elapsed Cycles          cycle    257715826
    Average L2 Active Cycles         cycle   1030507.38
    Total L2 Elapsed Cycles          cycle    110525664
    Average SM Active Cycles         cycle   3205308.45
    Total SM Elapsed Cycles          cycle    257715826
    Average SMSP Active Cycles       cycle   3205184.78
    Total SMSP Elapsed Cycles        cycle   1030863304
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.16%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.94% above the average, while the minimum instance value is 10.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.19%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.98% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.16%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.22
    Elapsed Cycles                cycle         6610
    Memory Throughput                 %        46.51
    DRAM Throughput                   %        46.51
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.78
    L2 Cache Throughput               %        25.46
    SM Active Cycles              cycle      4382.84
    Compute (SM) Throughput           %        17.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.80
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24050.67
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4382.84
    Total L1 Elapsed Cycles          cycle       380276
    Average L2 Active Cycles         cycle      4129.21
    Total L2 Elapsed Cycles          cycle       164616
    Average SM Active Cycles         cycle      4382.84
    Total SM Elapsed Cycles          cycle       380276
    Average SMSP Active Cycles       cycle      4453.63
    Total SMSP Elapsed Cycles        cycle      1521104
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.80
    Elapsed Cycles                cycle         7124
    Memory Throughput                 %        49.48
    DRAM Throughput                   %        49.48
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4951.98
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.18
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27741.33
    Total DRAM Elapsed Cycles        cycle       336384
    Average L1 Active Cycles         cycle      4951.98
    Total L1 Elapsed Cycles          cycle       411638
    Average L2 Active Cycles         cycle      4539.83
    Total L2 Elapsed Cycles          cycle       177312
    Average SM Active Cycles         cycle      4951.98
    Total SM Elapsed Cycles          cycle       411638
    Average SMSP Active Cycles       cycle      4970.91
    Total SMSP Elapsed Cycles        cycle      1646552
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.40
    Elapsed Cycles                cycle         7147
    Memory Throughput                 %        49.68
    DRAM Throughput                   %        49.68
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.82
    SM Active Cycles              cycle      4990.98
    Compute (SM) Throughput           %        15.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.36
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4990.98
    Total L1 Elapsed Cycles          cycle       409934
    Average L2 Active Cycles         cycle      4453.25
    Total L2 Elapsed Cycles          cycle       177912
    Average SM Active Cycles         cycle      4990.98
    Total SM Elapsed Cycles          cycle       409934
    Average SMSP Active Cycles       cycle      4845.17
    Total SMSP Elapsed Cycles        cycle      1639736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.90
    Elapsed Cycles                cycle         7269
    Memory Throughput                 %        48.72
    DRAM Throughput                   %        48.72
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        27.38
    SM Active Cycles              cycle      4968.69
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.71
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4968.69
    Total L1 Elapsed Cycles          cycle       408716
    Average L2 Active Cycles         cycle      4522.25
    Total L2 Elapsed Cycles          cycle       180816
    Average SM Active Cycles         cycle      4968.69
    Total SM Elapsed Cycles          cycle       408716
    Average SMSP Active Cycles       cycle      4928.88
    Total SMSP Elapsed Cycles        cycle      1634864
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449121
    Memory Throughput                 %        50.95
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205466.81
    Compute (SM) Throughput           %        50.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77346.67
    Total DRAM Elapsed Cycles        cycle    209688576
    Average L1 Active Cycles         cycle   3205466.81
    Total L1 Elapsed Cycles          cycle    257585980
    Average L2 Active Cycles         cycle   1027441.25
    Total L2 Elapsed Cycles          cycle    110824296
    Average SM Active Cycles         cycle   3205466.81
    Total SM Elapsed Cycles          cycle    257585980
    Average SMSP Active Cycles       cycle   3205008.02
    Total SMSP Elapsed Cycles        cycle   1030343920
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.92%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.60% above the average, while the minimum instance value is 10.56% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.04%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.76% above the average, while the minimum instance value is 10.57% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.92%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.60% above the average, while the minimum instance value is 10.56% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.54
    Elapsed Cycles                cycle         6696
    Memory Throughput                 %        45.35
    DRAM Throughput                   %        45.35
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.25
    L2 Cache Throughput               %        25.14
    SM Active Cycles              cycle      4475.33
    Compute (SM) Throughput           %        17.39
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.59
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23837.33
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4475.33
    Total L1 Elapsed Cycles          cycle       376934
    Average L2 Active Cycles         cycle      3937.33
    Total L2 Elapsed Cycles          cycle       166800
    Average SM Active Cycles         cycle      4475.33
    Total SM Elapsed Cycles          cycle       376934
    Average SMSP Active Cycles       cycle      4315.72
    Total SMSP Elapsed Cycles        cycle      1507736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.749%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 10.15% above the average, while the minimum instance value is 2.93% below the       
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.03
    Elapsed Cycles                cycle         7179
    Memory Throughput                 %        49.32
    DRAM Throughput                   %        49.32
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.21
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      4980.48
    Compute (SM) Throughput           %        16.18
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.06
    Achieved Active Warps Per SM           warp        34.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.94%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27776
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4980.48
    Total L1 Elapsed Cycles          cycle       405010
    Average L2 Active Cycles         cycle      4528.04
    Total L2 Elapsed Cycles          cycle       178776
    Average SM Active Cycles         cycle      4980.48
    Total SM Elapsed Cycles          cycle       405010
    Average SMSP Active Cycles       cycle      4934.47
    Total SMSP Elapsed Cycles        cycle      1620040
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.86
    Elapsed Cycles                cycle         7195
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.09
    L2 Cache Throughput               %        27.58
    SM Active Cycles              cycle      4949.53
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.22
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4949.53
    Total L1 Elapsed Cycles          cycle       406630
    Average L2 Active Cycles         cycle      4510.42
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      4949.53
    Total SM Elapsed Cycles          cycle       406630
    Average SMSP Active Cycles       cycle      4900.98
    Total SMSP Elapsed Cycles        cycle      1626520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.87
    Elapsed Cycles                cycle         7177
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.13
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      4987.93
    Compute (SM) Throughput           %        16.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.38
    Achieved Active Warps Per SM           warp        34.74
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27730.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4987.93
    Total L1 Elapsed Cycles          cycle       406344
    Average L2 Active Cycles         cycle      4389.38
    Total L2 Elapsed Cycles          cycle       178800
    Average SM Active Cycles         cycle      4987.93
    Total SM Elapsed Cycles          cycle       406344
    Average SMSP Active Cycles       cycle      4765.34
    Total SMSP Elapsed Cycles        cycle      1625376
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4433704
    Memory Throughput                 %        51.06
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205421.79
    Compute (SM) Throughput           %        51.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80405.33
    Total DRAM Elapsed Cycles        cycle    208960512
    Average L1 Active Cycles         cycle   3205421.79
    Total L1 Elapsed Cycles          cycle    256989594
    Average L2 Active Cycles         cycle   1034607.62
    Total L2 Elapsed Cycles          cycle    110424192
    Average SM Active Cycles         cycle   3205421.79
    Total SM Elapsed Cycles          cycle    256989594
    Average SMSP Active Cycles       cycle   3204731.16
    Total SMSP Elapsed Cycles        cycle   1027958376
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.12%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.82% above the average, while the minimum instance value is 10.57% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.69% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.12%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.57% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       781.70
    Elapsed Cycles                cycle         6658
    Memory Throughput                 %        45.72
    DRAM Throughput                   %        45.72
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.28
    L2 Cache Throughput               %        25.29
    SM Active Cycles              cycle      4469.16
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.72
    Achieved Active Warps Per SM           warp        34.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24034.67
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4469.16
    Total L1 Elapsed Cycles          cycle       382474
    Average L2 Active Cycles         cycle      4062.08
    Total L2 Elapsed Cycles          cycle       165840
    Average SM Active Cycles         cycle      4469.16
    Total SM Elapsed Cycles          cycle       382474
    Average SMSP Active Cycles       cycle      4397.94
    Total SMSP Elapsed Cycles        cycle      1529896
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.25
    Elapsed Cycles                cycle         7191
    Memory Throughput                 %        49.24
    DRAM Throughput                   %        49.24
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.63
    SM Active Cycles              cycle      4995.03
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.58
    Achieved Active Warps Per SM           warp        34.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.42%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4995.03
    Total L1 Elapsed Cycles          cycle       414146
    Average L2 Active Cycles         cycle         4557
    Total L2 Elapsed Cycles          cycle       179160
    Average SM Active Cycles         cycle      4995.03
    Total SM Elapsed Cycles          cycle       414146
    Average SMSP Active Cycles       cycle      4962.78
    Total SMSP Elapsed Cycles        cycle      1656584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       786.68
    Elapsed Cycles                cycle         7128
    Memory Throughput                 %        49.43
    DRAM Throughput                   %        49.43
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.01
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      4896.79
    Compute (SM) Throughput           %        16.05
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.69
    Achieved Active Warps Per SM           warp        35.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4896.79
    Total L1 Elapsed Cycles          cycle       408368
    Average L2 Active Cycles         cycle      4420.67
    Total L2 Elapsed Cycles          cycle       177456
    Average SM Active Cycles         cycle      4896.79
    Total SM Elapsed Cycles          cycle       408368
    Average SMSP Active Cycles       cycle      4839.05
    Total SMSP Elapsed Cycles        cycle      1633472
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.23
    Elapsed Cycles                cycle         7157
    Memory Throughput                 %        49.27
    DRAM Throughput                   %        49.27
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.68
    L2 Cache Throughput               %        27.77
    SM Active Cycles              cycle      5034.29
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.24
    Achieved Active Warps Per SM           warp        34.68
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5034.29
    Total L1 Elapsed Cycles          cycle       413778
    Average L2 Active Cycles         cycle      4517.08
    Total L2 Elapsed Cycles          cycle       178224
    Average SM Active Cycles         cycle      5034.29
    Total SM Elapsed Cycles          cycle       413778
    Average SMSP Active Cycles       cycle      4942.81
    Total SMSP Elapsed Cycles        cycle      1655112
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4434692
    Memory Throughput                 %        50.84
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205467.28
    Compute (SM) Throughput           %        50.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77373.33
    Total DRAM Elapsed Cycles        cycle    209006592
    Average L1 Active Cycles         cycle   3205467.28
    Total L1 Elapsed Cycles          cycle    258128530
    Average L2 Active Cycles         cycle   1030910.58
    Total L2 Elapsed Cycles          cycle    110448888
    Average SM Active Cycles         cycle   3205467.28
    Total SM Elapsed Cycles          cycle    258128530
    Average SMSP Active Cycles       cycle   3205474.97
    Total SMSP Elapsed Cycles        cycle   1032514120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.79% above the average, while the minimum instance value is 10.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.14%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.50% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 13, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       783.74
    Elapsed Cycles                cycle         6672
    Memory Throughput                 %        44.95
    DRAM Throughput                   %        44.95
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.36
    L2 Cache Throughput               %        25.20
    SM Active Cycles              cycle      4456.12
    Compute (SM) Throughput           %        16.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.00
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23704
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4456.12
    Total L1 Elapsed Cycles          cycle       386950
    Average L2 Active Cycles         cycle      3933.58
    Total L2 Elapsed Cycles          cycle       166176
    Average SM Active Cycles         cycle      4456.12
    Total SM Elapsed Cycles          cycle       386950
    Average SMSP Active Cycles       cycle      4315.09
    Total SMSP Elapsed Cycles        cycle      1547800
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.308%                                                                                          
          One or more L2 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 9.34% above the average, while the minimum instance value is 2.73% below the        
          average.                                                                                                      

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.22
    SM Frequency                    Mhz       786.20
    Elapsed Cycles                cycle         7121
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.23
    L2 Cache Throughput               %        27.93
    SM Active Cycles              cycle      4967.69
    Compute (SM) Throughput           %        16.21
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.60
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4967.69
    Total L1 Elapsed Cycles          cycle       404342
    Average L2 Active Cycles         cycle      4427.38
    Total L2 Elapsed Cycles          cycle       177336
    Average SM Active Cycles         cycle      4967.69
    Total SM Elapsed Cycles          cycle       404342
    Average SMSP Active Cycles       cycle      4817.56
    Total SMSP Elapsed Cycles        cycle      1617368
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.93
    Elapsed Cycles                cycle         7128
    Memory Throughput                 %        49.84
    DRAM Throughput                   %        49.84
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.10
    L2 Cache Throughput               %        27.87
    SM Active Cycles              cycle      4882.48
    Compute (SM) Throughput           %        16.12
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.03
    Achieved Active Warps Per SM           warp        35.05
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4882.48
    Total L1 Elapsed Cycles          cycle       406646
    Average L2 Active Cycles         cycle      4526.17
    Total L2 Elapsed Cycles          cycle       177624
    Average SM Active Cycles         cycle      4882.48
    Total SM Elapsed Cycles          cycle       406646
    Average SMSP Active Cycles       cycle      4950.38
    Total SMSP Elapsed Cycles        cycle      1626584
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.38
    Elapsed Cycles                cycle         7282
    Memory Throughput                 %        48.55
    DRAM Throughput                   %        48.55
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        27.32
    SM Active Cycles              cycle      4995.84
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      4995.84
    Total L1 Elapsed Cycles          cycle       408844
    Average L2 Active Cycles         cycle      4503.12
    Total L2 Elapsed Cycles          cycle       181344
    Average SM Active Cycles         cycle      4995.84
    Total SM Elapsed Cycles          cycle       408844
    Average SMSP Active Cycles       cycle      4910.89
    Total SMSP Elapsed Cycles        cycle      1635376
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4451486
    Memory Throughput                 %        50.90
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204963.10
    Compute (SM) Throughput           %        50.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78144
    Total DRAM Elapsed Cycles        cycle    209799168
    Average L1 Active Cycles         cycle   3204963.10
    Total L1 Elapsed Cycles          cycle    257804002
    Average L2 Active Cycles         cycle   1025383.75
    Total L2 Elapsed Cycles          cycle    110867184
    Average SM Active Cycles         cycle   3204963.10
    Total SM Elapsed Cycles          cycle    257804002
    Average SMSP Active Cycles       cycle   3204810.73
    Total SMSP Elapsed Cycles        cycle   1031216008
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.48% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.39% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 14, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.33
    Elapsed Cycles                cycle         6679
    Memory Throughput                 %        45.61
    DRAM Throughput                   %        45.61
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.44
    L2 Cache Throughput               %        25.23
    SM Active Cycles              cycle      4442.34
    Compute (SM) Throughput           %        17.23
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.80
    Achieved Active Warps Per SM           warp        34.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23973.33
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4442.34
    Total L1 Elapsed Cycles          cycle       380402
    Average L2 Active Cycles         cycle      4084.71
    Total L2 Elapsed Cycles          cycle       166344
    Average SM Active Cycles         cycle      4442.34
    Total SM Elapsed Cycles          cycle       380402
    Average SMSP Active Cycles       cycle      4391.23
    Total SMSP Elapsed Cycles        cycle      1521608
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.07
    Elapsed Cycles                cycle         7485
    Memory Throughput                 %        47.14
    DRAM Throughput                   %        47.14
    Duration                         us         9.57
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        26.60
    SM Active Cycles              cycle      4985.86
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.16
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.84%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       353792
    Average L1 Active Cycles         cycle      4985.86
    Total L1 Elapsed Cycles          cycle       412666
    Average L2 Active Cycles         cycle      4461.33
    Total L2 Elapsed Cycles          cycle       186240
    Average SM Active Cycles         cycle      4985.86
    Total SM Elapsed Cycles          cycle       412666
    Average SMSP Active Cycles       cycle      4892.65
    Total SMSP Elapsed Cycles        cycle      1650664
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.81
    Elapsed Cycles                cycle         7243
    Memory Throughput                 %        48.89
    DRAM Throughput                   %        48.89
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle      4947.45
    Compute (SM) Throughput           %        15.65
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.80
    Achieved Active Warps Per SM           warp        35.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27866.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4947.45
    Total L1 Elapsed Cycles          cycle       418724
    Average L2 Active Cycles         cycle      4483.75
    Total L2 Elapsed Cycles          cycle       180432
    Average SM Active Cycles         cycle      4947.45
    Total SM Elapsed Cycles          cycle       418724
    Average SMSP Active Cycles       cycle      4900.60
    Total SMSP Elapsed Cycles        cycle      1674896
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.41
    Elapsed Cycles                cycle         7206
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.63
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4972.74
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.30
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4972.74
    Total L1 Elapsed Cycles          cycle       415906
    Average L2 Active Cycles         cycle      4452.04
    Total L2 Elapsed Cycles          cycle       179376
    Average SM Active Cycles         cycle      4972.74
    Total SM Elapsed Cycles          cycle       415906
    Average SMSP Active Cycles       cycle      4861.78
    Total SMSP Elapsed Cycles        cycle      1663624
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4431185
    Memory Throughput                 %        50.91
    DRAM Throughput                   %         0.22
    Duration                         ms         5.57
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205368.95
    Compute (SM) Throughput           %        50.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78152
    Total DRAM Elapsed Cycles        cycle    208843776
    Average L1 Active Cycles         cycle   3205368.95
    Total L1 Elapsed Cycles          cycle    257761338
    Average L2 Active Cycles         cycle   1030866.92
    Total L2 Elapsed Cycles          cycle    110376000
    Average SM Active Cycles         cycle   3205368.95
    Total SM Elapsed Cycles          cycle    257761338
    Average SMSP Active Cycles       cycle   3205338.42
    Total SMSP Elapsed Cycles        cycle   1031045352
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.78% above the average, while the minimum instance value is 10.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.96% above the average, while the minimum instance value is 10.50% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.05
    Elapsed Cycles                cycle         6616
    Memory Throughput                 %        45.58
    DRAM Throughput                   %        45.58
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.42
    L2 Cache Throughput               %        25.41
    SM Active Cycles              cycle      4444.97
    Compute (SM) Throughput           %        17.41
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.36
    Achieved Active Warps Per SM           warp        34.25
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23805.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4444.97
    Total L1 Elapsed Cycles          cycle       376348
    Average L2 Active Cycles         cycle      4031.38
    Total L2 Elapsed Cycles          cycle       164832
    Average SM Active Cycles         cycle      4444.97
    Total SM Elapsed Cycles          cycle       376348
    Average SMSP Active Cycles       cycle      4378.60
    Total SMSP Elapsed Cycles        cycle      1505392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       785.08
    Elapsed Cycles                cycle         7136
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.03
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      4857.91
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4857.91
    Total L1 Elapsed Cycles          cycle       408772
    Average L2 Active Cycles         cycle      4512.75
    Total L2 Elapsed Cycles          cycle       177864
    Average SM Active Cycles         cycle      4857.91
    Total SM Elapsed Cycles          cycle       408772
    Average SMSP Active Cycles       cycle      4922.95
    Total SMSP Elapsed Cycles        cycle      1635088
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.10
    Elapsed Cycles                cycle         7125
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        28.04
    SM Active Cycles              cycle      4991.50
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.33
    Achieved Active Warps Per SM           warp        34.72
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.67%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27725.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4991.50
    Total L1 Elapsed Cycles          cycle       412830
    Average L2 Active Cycles         cycle      4484.38
    Total L2 Elapsed Cycles          cycle       177264
    Average SM Active Cycles         cycle      4991.50
    Total SM Elapsed Cycles          cycle       412830
    Average SMSP Active Cycles       cycle      4894.12
    Total SMSP Elapsed Cycles        cycle      1651320
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.25
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      4985.67
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.95
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.05%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4985.67
    Total L1 Elapsed Cycles          cycle       408134
    Average L2 Active Cycles         cycle         4365
    Total L2 Elapsed Cycles          cycle       178824
    Average SM Active Cycles         cycle      4985.67
    Total SM Elapsed Cycles          cycle       408134
    Average SMSP Active Cycles       cycle      4811.44
    Total SMSP Elapsed Cycles        cycle      1632536
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4441429
    Memory Throughput                 %        51.08
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204889.38
    Compute (SM) Throughput           %        51.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.25
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77546.67
    Total DRAM Elapsed Cycles        cycle    209326080
    Average L1 Active Cycles         cycle   3204889.38
    Total L1 Elapsed Cycles          cycle    256915212
    Average L2 Active Cycles         cycle   1035918.92
    Total L2 Elapsed Cycles          cycle    110624496
    Average SM Active Cycles         cycle   3204889.38
    Total SM Elapsed Cycles          cycle    256915212
    Average SMSP Active Cycles       cycle   3205277.90
    Total SMSP Elapsed Cycles        cycle   1027660848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.72% above the average, while the minimum instance value is 10.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.72% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.37
    Elapsed Cycles                cycle         6552
    Memory Throughput                 %        46.57
    DRAM Throughput                   %        46.57
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.17
    L2 Cache Throughput               %        25.69
    SM Active Cycles              cycle      4488.34
    Compute (SM) Throughput           %        17.43
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.26
    Achieved Active Warps Per SM           warp        34.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24002.67
    Total DRAM Elapsed Cycles        cycle       309248
    Average L1 Active Cycles         cycle      4488.34
    Total L1 Elapsed Cycles          cycle       376048
    Average L2 Active Cycles         cycle      4126.38
    Total L2 Elapsed Cycles          cycle       163200
    Average SM Active Cycles         cycle      4488.34
    Total SM Elapsed Cycles          cycle       376048
    Average SMSP Active Cycles       cycle      4476.33
    Total SMSP Elapsed Cycles        cycle      1504192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.19
    Elapsed Cycles                cycle         7093
    Memory Throughput                 %        49.72
    DRAM Throughput                   %        49.72
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.52
    L2 Cache Throughput               %        28.07
    SM Active Cycles              cycle      4953.76
    Compute (SM) Throughput           %        15.69
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.54
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.46%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4953.76
    Total L1 Elapsed Cycles          cycle       417798
    Average L2 Active Cycles         cycle      4433.96
    Total L2 Elapsed Cycles          cycle       176688
    Average SM Active Cycles         cycle      4953.76
    Total SM Elapsed Cycles          cycle       417798
    Average SMSP Active Cycles       cycle      4856.53
    Total SMSP Elapsed Cycles        cycle      1671192
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.40
    Elapsed Cycles                cycle         7061
    Memory Throughput                 %        49.89
    DRAM Throughput                   %        49.89
    Duration                         us         9.02
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        28.24
    SM Active Cycles              cycle      4950.33
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.10
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       333824
    Average L1 Active Cycles         cycle      4950.33
    Total L1 Elapsed Cycles          cycle       409666
    Average L2 Active Cycles         cycle      4514.08
    Total L2 Elapsed Cycles          cycle       175704
    Average SM Active Cycles         cycle      4950.33
    Total SM Elapsed Cycles          cycle       409666
    Average SMSP Active Cycles       cycle      4934.27
    Total SMSP Elapsed Cycles        cycle      1638664
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.57
    Elapsed Cycles                cycle         7214
    Memory Throughput                 %        49.12
    DRAM Throughput                   %        49.12
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.64
    L2 Cache Throughput               %        27.63
    SM Active Cycles              cycle      4974.95
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.51
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4974.95
    Total L1 Elapsed Cycles          cycle       415592
    Average L2 Active Cycles         cycle      4507.79
    Total L2 Elapsed Cycles          cycle       179520
    Average SM Active Cycles         cycle      4974.95
    Total SM Elapsed Cycles          cycle       415592
    Average SMSP Active Cycles       cycle      4925.81
    Total SMSP Elapsed Cycles        cycle      1662368
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4452150
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205669.36
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78301.33
    Total DRAM Elapsed Cycles        cycle    209829888
    Average L1 Active Cycles         cycle   3205669.36
    Total L1 Elapsed Cycles          cycle    257613632
    Average L2 Active Cycles         cycle   1037911.88
    Total L2 Elapsed Cycles          cycle    110883552
    Average SM Active Cycles         cycle   3205669.36
    Total SM Elapsed Cycles          cycle    257613632
    Average SMSP Active Cycles       cycle   3205588.17
    Total SMSP Elapsed Cycles        cycle   1030454528
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.01%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.72% above the average, while the minimum instance value is 10.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.56% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.01%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.72% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.33
    Elapsed Cycles                cycle         6786
    Memory Throughput                 %        44.42
    DRAM Throughput                   %        44.42
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.55
    L2 Cache Throughput               %        24.76
    SM Active Cycles              cycle      4421.86
    Compute (SM) Throughput           %        17.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.81
    Achieved Active Warps Per SM           warp        35.43
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23805.33
    Total DRAM Elapsed Cycles        cycle       321536
    Average L1 Active Cycles         cycle      4421.86
    Total L1 Elapsed Cycles          cycle       384238
    Average L2 Active Cycles         cycle      4038.54
    Total L2 Elapsed Cycles          cycle       169008
    Average SM Active Cycles         cycle      4421.86
    Total SM Elapsed Cycles          cycle       384238
    Average SMSP Active Cycles       cycle      4367.30
    Total SMSP Elapsed Cycles        cycle      1536952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.01
    Elapsed Cycles                cycle         7234
    Memory Throughput                 %        48.64
    DRAM Throughput                   %        48.64
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.18
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle      5005.50
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.08
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5005.50
    Total L1 Elapsed Cycles          cycle       406034
    Average L2 Active Cycles         cycle      4444.42
    Total L2 Elapsed Cycles          cycle       180192
    Average SM Active Cycles         cycle      5005.50
    Total SM Elapsed Cycles          cycle       406034
    Average SMSP Active Cycles       cycle      4841.55
    Total SMSP Elapsed Cycles        cycle      1624136
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.16
    Elapsed Cycles                cycle         7178
    Memory Throughput                 %        49.07
    DRAM Throughput                   %        49.07
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.78
    SM Active Cycles              cycle      4931.71
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.03
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4931.71
    Total L1 Elapsed Cycles          cycle       412624
    Average L2 Active Cycles         cycle      4455.42
    Total L2 Elapsed Cycles          cycle       178728
    Average SM Active Cycles         cycle      4931.71
    Total SM Elapsed Cycles          cycle       412624
    Average SMSP Active Cycles       cycle      4879.40
    Total SMSP Elapsed Cycles        cycle      1650496
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.71
    Elapsed Cycles                cycle         7159
    Memory Throughput                 %        49.61
    DRAM Throughput                   %        49.61
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.81
    SM Active Cycles              cycle      4993.19
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.85
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.15%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4993.19
    Total L1 Elapsed Cycles          cycle       412660
    Average L2 Active Cycles         cycle      4502.12
    Total L2 Elapsed Cycles          cycle       178296
    Average SM Active Cycles         cycle      4993.19
    Total SM Elapsed Cycles          cycle       412660
    Average SMSP Active Cycles       cycle      4948.00
    Total SMSP Elapsed Cycles        cycle      1650640
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449519
    Memory Throughput                 %        50.93
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205171.83
    Compute (SM) Throughput           %        50.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77226.67
    Total DRAM Elapsed Cycles        cycle    209707008
    Average L1 Active Cycles         cycle   3205171.83
    Total L1 Elapsed Cycles          cycle    257684524
    Average L2 Active Cycles         cycle   1032820.08
    Total L2 Elapsed Cycles          cycle    110820168
    Average SM Active Cycles         cycle   3205171.83
    Total SM Elapsed Cycles          cycle    257684524
    Average SMSP Active Cycles       cycle   3204734.91
    Total SMSP Elapsed Cycles        cycle   1030738096
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.17%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.96% above the average, while the minimum instance value is 10.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.17%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.96% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.18
    Elapsed Cycles                cycle         6676
    Memory Throughput                 %        45.47
    DRAM Throughput                   %        45.47
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        24.85
    L2 Cache Throughput               %        25.20
    SM Active Cycles              cycle      4547.57
    Compute (SM) Throughput           %        17.49
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.00
    Achieved Active Warps Per SM           warp        34.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23981.33
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4547.57
    Total L1 Elapsed Cycles          cycle       374618
    Average L2 Active Cycles         cycle      4049.50
    Total L2 Elapsed Cycles          cycle       166176
    Average SM Active Cycles         cycle      4547.57
    Total SM Elapsed Cycles          cycle       374618
    Average SMSP Active Cycles       cycle      4413.22
    Total SMSP Elapsed Cycles        cycle      1498472
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.04
    Elapsed Cycles                cycle         7284
    Memory Throughput                 %        48.43
    DRAM Throughput                   %        48.43
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        27.36
    SM Active Cycles              cycle      5003.19
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.93
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.07%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      5003.19
    Total L1 Elapsed Cycles          cycle       413496
    Average L2 Active Cycles         cycle         4460
    Total L2 Elapsed Cycles          cycle       181320
    Average SM Active Cycles         cycle      5003.19
    Total SM Elapsed Cycles          cycle       413496
    Average SMSP Active Cycles       cycle      4870.60
    Total SMSP Elapsed Cycles        cycle      1653984
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.04
    Elapsed Cycles                cycle         7193
    Memory Throughput                 %        49.26
    DRAM Throughput                   %        49.26
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      5018.29
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.51
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       338432
    Average L1 Active Cycles         cycle      5018.29
    Total L1 Elapsed Cycles          cycle       416348
    Average L2 Active Cycles         cycle      4465.88
    Total L2 Elapsed Cycles          cycle       179064
    Average SM Active Cycles         cycle      5018.29
    Total SM Elapsed Cycles          cycle       416348
    Average SMSP Active Cycles       cycle      4884.44
    Total SMSP Elapsed Cycles        cycle      1665392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.89
    Elapsed Cycles                cycle         7175
    Memory Throughput                 %        49.13
    DRAM Throughput                   %        49.13
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.76
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      4858.98
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.72
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27837.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4858.98
    Total L1 Elapsed Cycles          cycle       413320
    Average L2 Active Cycles         cycle      4507.42
    Total L2 Elapsed Cycles          cycle       178680
    Average SM Active Cycles         cycle      4858.98
    Total SM Elapsed Cycles          cycle       413320
    Average SMSP Active Cycles       cycle      4917.33
    Total SMSP Elapsed Cycles        cycle      1653280
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449272
    Memory Throughput                 %        50.82
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205343.26
    Compute (SM) Throughput           %        50.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77461.33
    Total DRAM Elapsed Cycles        cycle    209694720
    Average L1 Active Cycles         cycle   3205343.26
    Total L1 Elapsed Cycles          cycle    258243644
    Average L2 Active Cycles         cycle   1023018.08
    Total L2 Elapsed Cycles          cycle    110811864
    Average SM Active Cycles         cycle   3205343.26
    Total SM Elapsed Cycles          cycle    258243644
    Average SMSP Active Cycles       cycle   3204640.80
    Total SMSP Elapsed Cycles        cycle   1032974576
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20%                                                                                             
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.78% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.43% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20%                                                                                             
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.81
    Elapsed Cycles                cycle         6659
    Memory Throughput                 %        45.66
    DRAM Throughput                   %        45.66
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.94
    L2 Cache Throughput               %        25.26
    SM Active Cycles              cycle      4355.84
    Compute (SM) Throughput           %        17.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.64
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23845.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4355.84
    Total L1 Elapsed Cycles          cycle       377716
    Average L2 Active Cycles         cycle      3948.50
    Total L2 Elapsed Cycles          cycle       165888
    Average SM Active Cycles         cycle      4355.84
    Total SM Elapsed Cycles          cycle       377716
    Average SMSP Active Cycles       cycle      4294.36
    Total SMSP Elapsed Cycles        cycle      1510864
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.16
    Elapsed Cycles                cycle         7144
    Memory Throughput                 %        49.25
    DRAM Throughput                   %        49.25
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        27.85
    SM Active Cycles              cycle      4976.07
    Compute (SM) Throughput           %        16.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.72
    Achieved Active Warps Per SM           warp        34.91
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4976.07
    Total L1 Elapsed Cycles          cycle       409204
    Average L2 Active Cycles         cycle      4494.92
    Total L2 Elapsed Cycles          cycle       178104
    Average SM Active Cycles         cycle      4976.07
    Total SM Elapsed Cycles          cycle       409204
    Average SMSP Active Cycles       cycle      4910.27
    Total SMSP Elapsed Cycles        cycle      1636816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.36
    Elapsed Cycles                cycle         7216
    Memory Throughput                 %        49.02
    DRAM Throughput                   %        49.02
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4921.26
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27861.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4921.26
    Total L1 Elapsed Cycles          cycle       416194
    Average L2 Active Cycles         cycle      4384.50
    Total L2 Elapsed Cycles          cycle       179640
    Average SM Active Cycles         cycle      4921.26
    Total SM Elapsed Cycles          cycle       416194
    Average SMSP Active Cycles       cycle      4804.78
    Total SMSP Elapsed Cycles        cycle      1664776
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       783.76
    Elapsed Cycles                cycle         7129
    Memory Throughput                 %        49.42
    DRAM Throughput                   %        49.42
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.07
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      5019.62
    Compute (SM) Throughput           %        16.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.34
    Achieved Active Warps Per SM           warp        34.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.66%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27834.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5019.62
    Total L1 Elapsed Cycles          cycle       408134
    Average L2 Active Cycles         cycle      4427.12
    Total L2 Elapsed Cycles          cycle       177504
    Average SM Active Cycles         cycle      5019.62
    Total SM Elapsed Cycles          cycle       408134
    Average SMSP Active Cycles       cycle      4858.28
    Total SMSP Elapsed Cycles        cycle      1632536
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4444361
    Memory Throughput                 %        50.85
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205128.17
    Compute (SM) Throughput           %        50.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.25
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77746.67
    Total DRAM Elapsed Cycles        cycle    209464320
    Average L1 Active Cycles         cycle   3205128.17
    Total L1 Elapsed Cycles          cycle    258062906
    Average L2 Active Cycles         cycle   1036082.38
    Total L2 Elapsed Cycles          cycle    110689632
    Average SM Active Cycles         cycle   3205128.17
    Total SM Elapsed Cycles          cycle    258062906
    Average SMSP Active Cycles       cycle   3205118.34
    Total SMSP Elapsed Cycles        cycle   1032251624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.88% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.40% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.88% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.02
    Elapsed Cycles                cycle         6652
    Memory Throughput                 %        45.96
    DRAM Throughput                   %        45.96
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        24.56
    L2 Cache Throughput               %        25.29
    SM Active Cycles              cycle      4601.26
    Compute (SM) Throughput           %        17.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.83
    Achieved Active Warps Per SM           warp        33.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24002.67
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4601.26
    Total L1 Elapsed Cycles          cycle       382168
    Average L2 Active Cycles         cycle      4141.67
    Total L2 Elapsed Cycles          cycle       165648
    Average SM Active Cycles         cycle      4601.26
    Total SM Elapsed Cycles          cycle       382168
    Average SMSP Active Cycles       cycle      4481.34
    Total SMSP Elapsed Cycles        cycle      1528672
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.39
    Elapsed Cycles                cycle         7208
    Memory Throughput                 %        48.89
    DRAM Throughput                   %        48.89
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.20
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      5028.84
    Compute (SM) Throughput           %        15.48
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.81
    Achieved Active Warps Per SM           warp        34.47
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.19%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5028.84
    Total L1 Elapsed Cycles          cycle       423290
    Average L2 Active Cycles         cycle      4506.08
    Total L2 Elapsed Cycles          cycle       179448
    Average SM Active Cycles         cycle      5028.84
    Total SM Elapsed Cycles          cycle       423290
    Average SMSP Active Cycles       cycle      4914.94
    Total SMSP Elapsed Cycles        cycle      1693160
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       785.96
    Elapsed Cycles                cycle         7148
    Memory Throughput                 %        49.32
    DRAM Throughput                   %        49.32
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.75
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      5002.14
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.91
    Achieved Active Warps Per SM           warp        34.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5002.14
    Total L1 Elapsed Cycles          cycle       413528
    Average L2 Active Cycles         cycle      4429.67
    Total L2 Elapsed Cycles          cycle       177984
    Average SM Active Cycles         cycle      5002.14
    Total SM Elapsed Cycles          cycle       413528
    Average SMSP Active Cycles       cycle      4834.66
    Total SMSP Elapsed Cycles        cycle      1654112
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.69
    Elapsed Cycles                cycle         7165
    Memory Throughput                 %        49.29
    DRAM Throughput                   %        49.29
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      5009.03
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.19
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5009.03
    Total L1 Elapsed Cycles          cycle       416570
    Average L2 Active Cycles         cycle      4467.38
    Total L2 Elapsed Cycles          cycle       178512
    Average SM Active Cycles         cycle      5009.03
    Total SM Elapsed Cycles          cycle       416570
    Average SMSP Active Cycles       cycle      4884.68
    Total SMSP Elapsed Cycles        cycle      1666280
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4429850
    Memory Throughput                 %        50.97
    DRAM Throughput                   %         0.22
    Duration                         ms         5.57
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205631.17
    Compute (SM) Throughput           %        50.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77245.33
    Total DRAM Elapsed Cycles        cycle    208780288
    Average L1 Active Cycles         cycle   3205631.17
    Total L1 Elapsed Cycles          cycle    257445788
    Average L2 Active Cycles         cycle   1028962.25
    Total L2 Elapsed Cycles          cycle    110330448
    Average SM Active Cycles         cycle   3205631.17
    Total SM Elapsed Cycles          cycle    257445788
    Average SMSP Active Cycles       cycle   3205070.12
    Total SMSP Elapsed Cycles        cycle   1029783152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.15%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.90% above the average, while the minimum instance value is 10.46% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.84% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.15%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.90% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.51
    Elapsed Cycles                cycle         6689
    Memory Throughput                 %        45.18
    DRAM Throughput                   %        45.18
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.33
    L2 Cache Throughput               %        25.14
    SM Active Cycles              cycle      4461.16
    Compute (SM) Throughput           %        16.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.75
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23826.67
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4461.16
    Total L1 Elapsed Cycles          cycle       385738
    Average L2 Active Cycles         cycle      4034.58
    Total L2 Elapsed Cycles          cycle       166680
    Average SM Active Cycles         cycle      4461.16
    Total SM Elapsed Cycles          cycle       385738
    Average SMSP Active Cycles       cycle      4353.80
    Total SMSP Elapsed Cycles        cycle      1542952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.23
    Elapsed Cycles                cycle         7188
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.73
    L2 Cache Throughput               %        27.71
    SM Active Cycles              cycle      4933.47
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.08
    Achieved Active Warps Per SM           warp        35.56
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4933.47
    Total L1 Elapsed Cycles          cycle       413958
    Average L2 Active Cycles         cycle      4556.67
    Total L2 Elapsed Cycles          cycle       179088
    Average SM Active Cycles         cycle      4933.47
    Total SM Elapsed Cycles          cycle       413958
    Average SMSP Active Cycles       cycle      4914.78
    Total SMSP Elapsed Cycles        cycle      1655832
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.06
    Elapsed Cycles                cycle         7139
    Memory Throughput                 %        49.68
    DRAM Throughput                   %        49.68
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.90
    SM Active Cycles              cycle      4932.19
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.64
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4932.19
    Total L1 Elapsed Cycles          cycle       416094
    Average L2 Active Cycles         cycle      4453.88
    Total L2 Elapsed Cycles          cycle       177792
    Average SM Active Cycles         cycle      4932.19
    Total SM Elapsed Cycles          cycle       416094
    Average SMSP Active Cycles       cycle      4881.38
    Total SMSP Elapsed Cycles        cycle      1664376
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.08
    Elapsed Cycles                cycle         7298
    Memory Throughput                 %        48.45
    DRAM Throughput                   %        48.45
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.45
    L2 Cache Throughput               %        27.31
    SM Active Cycles              cycle         5026
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.11
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27864
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle         5026
    Total L1 Elapsed Cycles          cycle       419068
    Average L2 Active Cycles         cycle      4523.71
    Total L2 Elapsed Cycles          cycle       181656
    Average SM Active Cycles         cycle         5026
    Total SM Elapsed Cycles          cycle       419068
    Average SMSP Active Cycles       cycle      4939.82
    Total SMSP Elapsed Cycles        cycle      1676272
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4442999
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.23
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205266.84
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79965.33
    Total DRAM Elapsed Cycles        cycle    209397760
    Average L1 Active Cycles         cycle   3205266.84
    Total L1 Elapsed Cycles          cycle    257610538
    Average L2 Active Cycles         cycle   1032732.62
    Total L2 Elapsed Cycles          cycle    110669064
    Average SM Active Cycles         cycle   3205266.84
    Total SM Elapsed Cycles          cycle    257610538
    Average SMSP Active Cycles       cycle   3205144.68
    Total SMSP Elapsed Cycles        cycle   1030442152
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.16%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.04
    Elapsed Cycles                cycle         6734
    Memory Throughput                 %        45.28
    DRAM Throughput                   %        45.28
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.21
    L2 Cache Throughput               %        24.95
    SM Active Cycles              cycle      4481.34
    Compute (SM) Throughput           %        17.34
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.05
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.95%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24032
    Total DRAM Elapsed Cycles        cycle       318464
    Average L1 Active Cycles         cycle      4481.34
    Total L1 Elapsed Cycles          cycle       377960
    Average L2 Active Cycles         cycle      4048.79
    Total L2 Elapsed Cycles          cycle       167808
    Average SM Active Cycles         cycle      4481.34
    Total SM Elapsed Cycles          cycle       377960
    Average SMSP Active Cycles       cycle      4418.99
    Total SMSP Elapsed Cycles        cycle      1511840
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.49
    Elapsed Cycles                cycle         7228
    Memory Throughput                 %        48.85
    DRAM Throughput                   %        48.85
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.35
    L2 Cache Throughput               %        27.59
    SM Active Cycles              cycle      5015.62
    Compute (SM) Throughput           %        15.57
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.64
    Achieved Active Warps Per SM           warp        34.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27762.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5015.62
    Total L1 Elapsed Cycles          cycle       420938
    Average L2 Active Cycles         cycle      4479.92
    Total L2 Elapsed Cycles          cycle       179808
    Average SM Active Cycles         cycle      5015.62
    Total SM Elapsed Cycles          cycle       420938
    Average SMSP Active Cycles       cycle      4921.86
    Total SMSP Elapsed Cycles        cycle      1683752
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.41
    Elapsed Cycles                cycle         7155
    Memory Throughput                 %        49.15
    DRAM Throughput                   %        49.15
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.87
    SM Active Cycles              cycle      4914.95
    Compute (SM) Throughput           %        15.80
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.03
    Achieved Active Warps Per SM           warp        35.53
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.97%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27680
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4914.95
    Total L1 Elapsed Cycles          cycle       414842
    Average L2 Active Cycles         cycle      4491.54
    Total L2 Elapsed Cycles          cycle       178056
    Average SM Active Cycles         cycle      4914.95
    Total SM Elapsed Cycles          cycle       414842
    Average SMSP Active Cycles       cycle      4901.41
    Total SMSP Elapsed Cycles        cycle      1659368
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.30
    Elapsed Cycles                cycle         7157
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.99
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      4990.48
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.01
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.99%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4990.48
    Total L1 Elapsed Cycles          cycle       409438
    Average L2 Active Cycles         cycle      4398.50
    Total L2 Elapsed Cycles          cycle       178344
    Average SM Active Cycles         cycle      4990.48
    Total SM Elapsed Cycles          cycle       409438
    Average SMSP Active Cycles       cycle      4810.70
    Total SMSP Elapsed Cycles        cycle      1637752
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4439140
    Memory Throughput                 %        50.90
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205182.19
    Compute (SM) Throughput           %        50.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77501.33
    Total DRAM Elapsed Cycles        cycle    209218560
    Average L1 Active Cycles         cycle   3205182.19
    Total L1 Elapsed Cycles          cycle    257829886
    Average L2 Active Cycles         cycle   1030381.29
    Total L2 Elapsed Cycles          cycle    110559600
    Average SM Active Cycles         cycle   3205182.19
    Total SM Elapsed Cycles          cycle    257829886
    Average SMSP Active Cycles       cycle   3204925.88
    Total SMSP Elapsed Cycles        cycle   1031319544
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.89%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.59% above the average, while the minimum instance value is 10.56% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.91%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.61% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.89%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.59% above the average, while the minimum instance value is 10.56% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       784.98
    Elapsed Cycles                cycle         6610
    Memory Throughput                 %        45.99
    DRAM Throughput                   %        45.99
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.67
    L2 Cache Throughput               %        25.43
    SM Active Cycles              cycle      4401.55
    Compute (SM) Throughput           %        17.11
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.55
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.45%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23784
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4401.55
    Total L1 Elapsed Cycles          cycle       382986
    Average L2 Active Cycles         cycle      4096.92
    Total L2 Elapsed Cycles          cycle       164688
    Average SM Active Cycles         cycle      4401.55
    Total SM Elapsed Cycles          cycle       382986
    Average SMSP Active Cycles       cycle      4440.44
    Total SMSP Elapsed Cycles        cycle      1531944
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.58
    Elapsed Cycles                cycle         7274
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.37
    SM Active Cycles              cycle      4994.10
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.56
    Achieved Active Warps Per SM           warp        34.83
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27832
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4994.10
    Total L1 Elapsed Cycles          cycle       410920
    Average L2 Active Cycles         cycle      4579.83
    Total L2 Elapsed Cycles          cycle       181224
    Average SM Active Cycles         cycle      4994.10
    Total SM Elapsed Cycles          cycle       410920
    Average SMSP Active Cycles       cycle      5004.37
    Total SMSP Elapsed Cycles        cycle      1643680
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.33
    Elapsed Cycles                cycle         7095
    Memory Throughput                 %        49.77
    DRAM Throughput                   %        49.77
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        24.04
    L2 Cache Throughput               %        28.11
    SM Active Cycles              cycle      4996.93
    Compute (SM) Throughput           %        16.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.61
    Achieved Active Warps Per SM           warp        34.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4996.93
    Total L1 Elapsed Cycles          cycle       408656
    Average L2 Active Cycles         cycle      4502.29
    Total L2 Elapsed Cycles          cycle       176592
    Average SM Active Cycles         cycle      4996.93
    Total SM Elapsed Cycles          cycle       408656
    Average SMSP Active Cycles       cycle      4906.28
    Total SMSP Elapsed Cycles        cycle      1634624
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.06
    Elapsed Cycles                cycle         7184
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.14
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4971.12
    Compute (SM) Throughput           %        16.10
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.38
    Achieved Active Warps Per SM           warp        35.22
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.62%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4971.12
    Total L1 Elapsed Cycles          cycle       406962
    Average L2 Active Cycles         cycle      4468.33
    Total L2 Elapsed Cycles          cycle       178920
    Average SM Active Cycles         cycle      4971.12
    Total SM Elapsed Cycles          cycle       406962
    Average SMSP Active Cycles       cycle      4948.80
    Total SMSP Elapsed Cycles        cycle      1627848
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4436980
    Memory Throughput                 %        50.92
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204639.40
    Compute (SM) Throughput           %        50.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79058.67
    Total DRAM Elapsed Cycles        cycle    209117184
    Average L1 Active Cycles         cycle   3204639.40
    Total L1 Elapsed Cycles          cycle    257733070
    Average L2 Active Cycles         cycle   1025082.33
    Total L2 Elapsed Cycles          cycle    110507976
    Average SM Active Cycles         cycle   3204639.40
    Total SM Elapsed Cycles          cycle    257733070
    Average SMSP Active Cycles       cycle   3205203.63
    Total SMSP Elapsed Cycles        cycle   1030932280
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.81% above the average, while the minimum instance value is 10.35% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.17%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.96% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.35% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.28
    Elapsed Cycles                cycle         6568
    Memory Throughput                 %        46.41
    DRAM Throughput                   %        46.41
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        25.34
    L2 Cache Throughput               %        25.61
    SM Active Cycles              cycle      4459.41
    Compute (SM) Throughput           %        17.19
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.40
    Achieved Active Warps Per SM           warp        34.27
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24000
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4459.41
    Total L1 Elapsed Cycles          cycle       381336
    Average L2 Active Cycles         cycle      4019.88
    Total L2 Elapsed Cycles          cycle       163560
    Average SM Active Cycles         cycle      4459.41
    Total SM Elapsed Cycles          cycle       381336
    Average SMSP Active Cycles       cycle      4391.04
    Total SMSP Elapsed Cycles        cycle      1525344
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.59
    Elapsed Cycles                cycle         7282
    Memory Throughput                 %        48.42
    DRAM Throughput                   %        48.42
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.58
    L2 Cache Throughput               %        27.35
    SM Active Cycles              cycle      4980.12
    Compute (SM) Throughput           %        15.73
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.10
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27768
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4980.12
    Total L1 Elapsed Cycles          cycle       416534
    Average L2 Active Cycles         cycle      4481.08
    Total L2 Elapsed Cycles          cycle       181368
    Average SM Active Cycles         cycle      4980.12
    Total SM Elapsed Cycles          cycle       416534
    Average SMSP Active Cycles       cycle      4903.22
    Total SMSP Elapsed Cycles        cycle      1666136
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.08
    Elapsed Cycles                cycle         7184
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      4964.76
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.91
    Achieved Active Warps Per SM           warp        35.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.09%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27685.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4964.76
    Total L1 Elapsed Cycles          cycle       416476
    Average L2 Active Cycles         cycle      4478.71
    Total L2 Elapsed Cycles          cycle       178800
    Average SM Active Cycles         cycle      4964.76
    Total SM Elapsed Cycles          cycle       416476
    Average SMSP Active Cycles       cycle      4897.57
    Total SMSP Elapsed Cycles        cycle      1665904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.06
    Elapsed Cycles                cycle         7183
    Memory Throughput                 %        49.20
    DRAM Throughput                   %        49.20
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.27
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4971.86
    Compute (SM) Throughput           %        15.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.19
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4971.86
    Total L1 Elapsed Cycles          cycle       422102
    Average L2 Active Cycles         cycle      4495.08
    Total L2 Elapsed Cycles          cycle       178896
    Average SM Active Cycles         cycle      4971.86
    Total SM Elapsed Cycles          cycle       422102
    Average SMSP Active Cycles       cycle      4881.56
    Total SMSP Elapsed Cycles        cycle      1688408
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449168
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205205.36
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77277.33
    Total DRAM Elapsed Cycles        cycle    209688576
    Average L1 Active Cycles         cycle   3205205.36
    Total L1 Elapsed Cycles          cycle    257356764
    Average L2 Active Cycles         cycle   1026029.71
    Total L2 Elapsed Cycles          cycle    110809416
    Average SM Active Cycles         cycle   3205205.36
    Total SM Elapsed Cycles          cycle    257356764
    Average SMSP Active Cycles       cycle   3204807.76
    Total SMSP Elapsed Cycles        cycle   1029427056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.53% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.85
    Elapsed Cycles                cycle         6768
    Memory Throughput                 %        44.64
    DRAM Throughput                   %        44.64
    Duration                         us         8.64
    L1/TEX Cache Throughput           %        25.44
    L2 Cache Throughput               %        24.85
    SM Active Cycles              cycle      4441.33
    Compute (SM) Throughput           %        17.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.29
    Achieved Active Warps Per SM           warp        35.18
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.71%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23770.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4441.33
    Total L1 Elapsed Cycles          cycle       381676
    Average L2 Active Cycles         cycle      4023.46
    Total L2 Elapsed Cycles          cycle       168672
    Average SM Active Cycles         cycle      4441.33
    Total SM Elapsed Cycles          cycle       381676
    Average SMSP Active Cycles       cycle      4349.12
    Total SMSP Elapsed Cycles        cycle      1526704
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.86
    Elapsed Cycles                cycle         7220
    Memory Throughput                 %        49.00
    DRAM Throughput                   %        49.00
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.54
    L2 Cache Throughput               %        27.60
    SM Active Cycles              cycle      5052.24
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.76
    Achieved Active Warps Per SM           warp        34.44
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.24%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5052.24
    Total L1 Elapsed Cycles          cycle       417322
    Average L2 Active Cycles         cycle      4506.17
    Total L2 Elapsed Cycles          cycle       179784
    Average SM Active Cycles         cycle      5052.24
    Total SM Elapsed Cycles          cycle       417322
    Average SMSP Active Cycles       cycle      4955.16
    Total SMSP Elapsed Cycles        cycle      1669288
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.54
    Elapsed Cycles                cycle         7044
    Memory Throughput                 %        50.25
    DRAM Throughput                   %        50.25
    Duration                         us         8.96
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        28.26
    SM Active Cycles              cycle      4994.90
    Compute (SM) Throughput           %        16.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.53
    Achieved Active Warps Per SM           warp        34.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.47%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       331776
    Average L1 Active Cycles         cycle      4994.90
    Total L1 Elapsed Cycles          cycle       405552
    Average L2 Active Cycles         cycle      4467.67
    Total L2 Elapsed Cycles          cycle       175464
    Average SM Active Cycles         cycle      4994.90
    Total SM Elapsed Cycles          cycle       405552
    Average SMSP Active Cycles       cycle      4894.53
    Total SMSP Elapsed Cycles        cycle      1622208
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.18
    Elapsed Cycles                cycle         7203
    Memory Throughput                 %        49.01
    DRAM Throughput                   %        49.01
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.94
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      5031.66
    Compute (SM) Throughput           %        15.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.80
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27853.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5031.66
    Total L1 Elapsed Cycles          cycle       410342
    Average L2 Active Cycles         cycle      4470.71
    Total L2 Elapsed Cycles          cycle       179424
    Average SM Active Cycles         cycle      5031.66
    Total SM Elapsed Cycles          cycle       410342
    Average SMSP Active Cycles       cycle      4906.34
    Total SMSP Elapsed Cycles        cycle      1641368
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4445447
    Memory Throughput                 %        51.00
    DRAM Throughput                   %         0.23
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205731.48
    Compute (SM) Throughput           %        51.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        80544
    Total DRAM Elapsed Cycles        cycle    209515520
    Average L1 Active Cycles         cycle   3205731.48
    Total L1 Elapsed Cycles          cycle    257299976
    Average L2 Active Cycles         cycle   1030642.29
    Total L2 Elapsed Cycles          cycle    110716752
    Average SM Active Cycles         cycle   3205731.48
    Total SM Elapsed Cycles          cycle    257299976
    Average SMSP Active Cycles       cycle   3204667.80
    Total SMSP Elapsed Cycles        cycle   1029199904
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.21%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.96%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.63% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.21%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.19
    Elapsed Cycles                cycle         6611
    Memory Throughput                 %        46.00
    DRAM Throughput                   %        46.00
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        24.53
    L2 Cache Throughput               %        25.47
    SM Active Cycles              cycle      4606.88
    Compute (SM) Throughput           %        17.53
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.52
    Achieved Active Warps Per SM           warp        33.85
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23946.67
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4606.88
    Total L1 Elapsed Cycles          cycle       373828
    Average L2 Active Cycles         cycle         4079
    Total L2 Elapsed Cycles          cycle       164712
    Average SM Active Cycles         cycle      4606.88
    Total SM Elapsed Cycles          cycle       373828
    Average SMSP Active Cycles       cycle      4426.37
    Total SMSP Elapsed Cycles        cycle      1495312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.19
    Elapsed Cycles                cycle         7205
    Memory Throughput                 %        48.94
    DRAM Throughput                   %        48.94
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.43
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      5012.05
    Compute (SM) Throughput           %        15.62
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.97
    Achieved Active Warps Per SM           warp        35.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27813.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5012.05
    Total L1 Elapsed Cycles          cycle       419434
    Average L2 Active Cycles         cycle      4467.17
    Total L2 Elapsed Cycles          cycle       179424
    Average SM Active Cycles         cycle      5012.05
    Total SM Elapsed Cycles          cycle       419434
    Average SMSP Active Cycles       cycle      4906.90
    Total SMSP Elapsed Cycles        cycle      1677736
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.66
    Elapsed Cycles                cycle         7242
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.24
    L2 Cache Throughput               %        27.52
    SM Active Cycles              cycle      4994.41
    Compute (SM) Throughput           %        16.17
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.52
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27842.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4994.41
    Total L1 Elapsed Cycles          cycle       405406
    Average L2 Active Cycles         cycle      4522.38
    Total L2 Elapsed Cycles          cycle       180336
    Average SM Active Cycles         cycle      4994.41
    Total SM Elapsed Cycles          cycle       405406
    Average SMSP Active Cycles       cycle      4943.63
    Total SMSP Elapsed Cycles        cycle      1621624
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.39
    Elapsed Cycles                cycle         7356
    Memory Throughput                 %        48.19
    DRAM Throughput                   %        48.19
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.08
    SM Active Cycles              cycle      4969.98
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.52
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27880
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4969.98
    Total L1 Elapsed Cycles          cycle       412788
    Average L2 Active Cycles         cycle      4482.92
    Total L2 Elapsed Cycles          cycle       183192
    Average SM Active Cycles         cycle      4969.98
    Total SM Elapsed Cycles          cycle       412788
    Average SMSP Active Cycles       cycle      4874.48
    Total SMSP Elapsed Cycles        cycle      1651152
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4452795
    Memory Throughput                 %        50.83
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205801.98
    Compute (SM) Throughput           %        50.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77469.33
    Total DRAM Elapsed Cycles        cycle    209860608
    Average L1 Active Cycles         cycle   3205801.98
    Total L1 Elapsed Cycles          cycle    258172082
    Average L2 Active Cycles         cycle   1028822.83
    Total L2 Elapsed Cycles          cycle    110899632
    Average SM Active Cycles         cycle   3205801.98
    Total SM Elapsed Cycles          cycle    258172082
    Average SMSP Active Cycles       cycle   3205197.37
    Total SMSP Elapsed Cycles        cycle   1032688328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.95% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.14%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.35% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.95% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.87
    Elapsed Cycles                cycle         6559
    Memory Throughput                 %        45.98
    DRAM Throughput                   %        45.98
    Duration                         us         8.38
    L1/TEX Cache Throughput           %        24.97
    L2 Cache Throughput               %        25.64
    SM Active Cycles              cycle      4524.53
    Compute (SM) Throughput           %        17.36
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.02
    Achieved Active Warps Per SM           warp        33.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23776
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4524.53
    Total L1 Elapsed Cycles          cycle       377532
    Average L2 Active Cycles         cycle      3931.83
    Total L2 Elapsed Cycles          cycle       163440
    Average SM Active Cycles         cycle      4524.53
    Total SM Elapsed Cycles          cycle       377532
    Average SMSP Active Cycles       cycle      4286.33
    Total SMSP Elapsed Cycles        cycle      1510128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.50
    Elapsed Cycles                cycle         7157
    Memory Throughput                 %        49.51
    DRAM Throughput                   %        49.51
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.45
    L2 Cache Throughput               %        27.82
    SM Active Cycles              cycle      5036.48
    Compute (SM) Throughput           %        16.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.75
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.25%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27882.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5036.48
    Total L1 Elapsed Cycles          cycle       401704
    Average L2 Active Cycles         cycle      4488.46
    Total L2 Elapsed Cycles          cycle       178296
    Average SM Active Cycles         cycle      5036.48
    Total SM Elapsed Cycles          cycle       401704
    Average SMSP Active Cycles       cycle      4899.40
    Total SMSP Elapsed Cycles        cycle      1606816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.60
    Elapsed Cycles                cycle         7218
    Memory Throughput                 %        49.26
    DRAM Throughput                   %        49.26
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.61
    SM Active Cycles              cycle      5004.22
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.61
    Achieved Active Warps Per SM           warp        34.37
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5004.22
    Total L1 Elapsed Cycles          cycle       412954
    Average L2 Active Cycles         cycle      4403.21
    Total L2 Elapsed Cycles          cycle       179760
    Average SM Active Cycles         cycle      5004.22
    Total SM Elapsed Cycles          cycle       412954
    Average SMSP Active Cycles       cycle      4831.98
    Total SMSP Elapsed Cycles        cycle      1651816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.57
    Elapsed Cycles                cycle         7150
    Memory Throughput                 %        49.66
    DRAM Throughput                   %        49.66
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.46
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4990.83
    Compute (SM) Throughput           %        16.32
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.82
    Achieved Active Warps Per SM           warp        34.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.18%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4990.83
    Total L1 Elapsed Cycles          cycle       401622
    Average L2 Active Cycles         cycle      4495.33
    Total L2 Elapsed Cycles          cycle       178176
    Average SM Active Cycles         cycle      4990.83
    Total SM Elapsed Cycles          cycle       401622
    Average SMSP Active Cycles       cycle      4905.60
    Total SMSP Elapsed Cycles        cycle      1606488
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4450018
    Memory Throughput                 %        50.85
    DRAM Throughput                   %         0.23
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204857.62
    Compute (SM) Throughput           %        50.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79221.33
    Total DRAM Elapsed Cycles        cycle    209730560
    Average L1 Active Cycles         cycle   3204857.62
    Total L1 Elapsed Cycles          cycle    258049390
    Average L2 Active Cycles         cycle   1018800.38
    Total L2 Elapsed Cycles          cycle    110830488
    Average SM Active Cycles         cycle   3204857.62
    Total SM Elapsed Cycles          cycle    258049390
    Average SMSP Active Cycles       cycle   3205076.45
    Total SMSP Elapsed Cycles        cycle   1032197560
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20%                                                                                             
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.77% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20%                                                                                             
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.77% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       785.17
    Elapsed Cycles                cycle         6610
    Memory Throughput                 %        46.30
    DRAM Throughput                   %        46.30
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.31
    L2 Cache Throughput               %        25.50
    SM Active Cycles              cycle      4464.33
    Compute (SM) Throughput           %        17.04
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.07
    Achieved Active Warps Per SM           warp        34.59
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24024
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4464.33
    Total L1 Elapsed Cycles          cycle       384588
    Average L2 Active Cycles         cycle      4060.46
    Total L2 Elapsed Cycles          cycle       164520
    Average SM Active Cycles         cycle      4464.33
    Total SM Elapsed Cycles          cycle       384588
    Average SMSP Active Cycles       cycle      4404.76
    Total SMSP Elapsed Cycles        cycle      1538352
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.89
    Elapsed Cycles                cycle         7272
    Memory Throughput                 %        48.74
    DRAM Throughput                   %        48.74
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.29
    L2 Cache Throughput               %        27.40
    SM Active Cycles              cycle      4983.79
    Compute (SM) Throughput           %        15.55
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.24
    Achieved Active Warps Per SM           warp        35.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.76%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27781.33
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4983.79
    Total L1 Elapsed Cycles          cycle       421454
    Average L2 Active Cycles         cycle      4524.67
    Total L2 Elapsed Cycles          cycle       181056
    Average SM Active Cycles         cycle      4983.79
    Total SM Elapsed Cycles          cycle       421454
    Average SMSP Active Cycles       cycle      4947.20
    Total SMSP Elapsed Cycles        cycle      1685816
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.75
    Elapsed Cycles                cycle         7151
    Memory Throughput                 %        49.49
    DRAM Throughput                   %        49.49
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.62
    L2 Cache Throughput               %        27.84
    SM Active Cycles              cycle      4875.09
    Compute (SM) Throughput           %        15.76
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.77
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4875.09
    Total L1 Elapsed Cycles          cycle       415786
    Average L2 Active Cycles         cycle      4514.50
    Total L2 Elapsed Cycles          cycle       178176
    Average SM Active Cycles         cycle      4875.09
    Total SM Elapsed Cycles          cycle       415786
    Average SMSP Active Cycles       cycle      4961.19
    Total SMSP Elapsed Cycles        cycle      1663144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       785.43
    Elapsed Cycles                cycle         7368
    Memory Throughput                 %        48.23
    DRAM Throughput                   %        48.23
    Duration                         us         9.38
    L1/TEX Cache Throughput           %        23.39
    L2 Cache Throughput               %        27.02
    SM Active Cycles              cycle      4973.78
    Compute (SM) Throughput           %        15.61
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.89
    Achieved Active Warps Per SM           warp        35.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27904
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4973.78
    Total L1 Elapsed Cycles          cycle       419902
    Average L2 Active Cycles         cycle      4458.71
    Total L2 Elapsed Cycles          cycle       183648
    Average SM Active Cycles         cycle      4973.78
    Total SM Elapsed Cycles          cycle       419902
    Average SMSP Active Cycles       cycle      4887.12
    Total SMSP Elapsed Cycles        cycle      1679608
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4441446
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3204674.90
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.25
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77384
    Total DRAM Elapsed Cycles        cycle    209326080
    Average L1 Active Cycles         cycle   3204674.90
    Total L1 Elapsed Cycles          cycle    257386186
    Average L2 Active Cycles         cycle   1027754.83
    Total L2 Elapsed Cycles          cycle    110616984
    Average SM Active Cycles         cycle   3204674.90
    Total SM Elapsed Cycles          cycle    257386186
    Average SMSP Active Cycles       cycle   3205318.93
    Total SMSP Elapsed Cycles        cycle   1029544744
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.74% above the average, while the minimum instance value is 10.40% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.07%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.74% above the average, while the minimum instance value is 10.40% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.69
    Elapsed Cycles                cycle         6708
    Memory Throughput                 %        45.04
    DRAM Throughput                   %        45.04
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.97
    L2 Cache Throughput               %        25.07
    SM Active Cycles              cycle      4351.02
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.89
    Achieved Active Warps Per SM           warp        34.99
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.11%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23829.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4351.02
    Total L1 Elapsed Cycles          cycle       386004
    Average L2 Active Cycles         cycle      4136.25
    Total L2 Elapsed Cycles          cycle       167040
    Average SM Active Cycles         cycle      4351.02
    Total SM Elapsed Cycles          cycle       386004
    Average SMSP Active Cycles       cycle      4432.82
    Total SMSP Elapsed Cycles        cycle      1544016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.363%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.05% above the average, while the minimum instance value is 9.74% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.82
    Elapsed Cycles                cycle         7127
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.78
    L2 Cache Throughput               %        27.96
    SM Active Cycles              cycle      4998.38
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.26
    Achieved Active Warps Per SM           warp        34.69
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4998.38
    Total L1 Elapsed Cycles          cycle       412976
    Average L2 Active Cycles         cycle      4456.83
    Total L2 Elapsed Cycles          cycle       177528
    Average SM Active Cycles         cycle      4998.38
    Total SM Elapsed Cycles          cycle       412976
    Average SMSP Active Cycles       cycle      4897.38
    Total SMSP Elapsed Cycles        cycle      1651904
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.08
    Elapsed Cycles                cycle         7227
    Memory Throughput                 %        48.85
    DRAM Throughput                   %        48.85
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        27.55
    SM Active Cycles              cycle      4976.88
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.23
    Achieved Active Warps Per SM           warp        35.15
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27848
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      4976.88
    Total L1 Elapsed Cycles          cycle       409228
    Average L2 Active Cycles         cycle      4441.46
    Total L2 Elapsed Cycles          cycle       180000
    Average SM Active Cycles         cycle      4976.88
    Total SM Elapsed Cycles          cycle       409228
    Average SMSP Active Cycles       cycle      4854.71
    Total SMSP Elapsed Cycles        cycle      1636912
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.61
    Elapsed Cycles                cycle         7141
    Memory Throughput                 %        49.31
    DRAM Throughput                   %        49.31
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.94
    L2 Cache Throughput               %        27.86
    SM Active Cycles              cycle      4965.64
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.07
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27773.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4965.64
    Total L1 Elapsed Cycles          cycle       410168
    Average L2 Active Cycles         cycle         4478
    Total L2 Elapsed Cycles          cycle       177912
    Average SM Active Cycles         cycle      4965.64
    Total SM Elapsed Cycles          cycle       410168
    Average SMSP Active Cycles       cycle      4882.12
    Total SMSP Elapsed Cycles        cycle      1640672
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4442618
    Memory Throughput                 %        51.07
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205117.52
    Compute (SM) Throughput           %        51.07
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77373.33
    Total DRAM Elapsed Cycles        cycle    209381376
    Average L1 Active Cycles         cycle   3205117.52
    Total L1 Elapsed Cycles          cycle    256950014
    Average L2 Active Cycles         cycle   1044187.50
    Total L2 Elapsed Cycles          cycle    110646192
    Average SM Active Cycles         cycle   3205117.52
    Total SM Elapsed Cycles          cycle    256950014
    Average SMSP Active Cycles       cycle   3205145.78
    Total SMSP Elapsed Cycles        cycle   1027800056
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.12%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.81% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.22%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.56% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.12%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.08
    Elapsed Cycles                cycle         6627
    Memory Throughput                 %        46.05
    DRAM Throughput                   %        46.05
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        25.46
    L2 Cache Throughput               %        25.39
    SM Active Cycles              cycle      4438.55
    Compute (SM) Throughput           %        17.31
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.35
    Achieved Active Warps Per SM           warp        35.21
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.65%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23970.67
    Total DRAM Elapsed Cycles        cycle       312320
    Average L1 Active Cycles         cycle      4438.55
    Total L1 Elapsed Cycles          cycle       378536
    Average L2 Active Cycles         cycle      4111.67
    Total L2 Elapsed Cycles          cycle       165120
    Average SM Active Cycles         cycle      4438.55
    Total SM Elapsed Cycles          cycle       378536
    Average SMSP Active Cycles       cycle      4469.75
    Total SMSP Elapsed Cycles        cycle      1514144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.79
    Elapsed Cycles                cycle         7316
    Memory Throughput                 %        48.10
    DRAM Throughput                   %        48.10
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.60
    L2 Cache Throughput               %        27.22
    SM Active Cycles              cycle      4939.33
    Compute (SM) Throughput           %        15.75
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.70
    Achieved Active Warps Per SM           warp        35.38
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.3%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27749.33
    Total DRAM Elapsed Cycles        cycle       346112
    Average L1 Active Cycles         cycle      4939.33
    Total L1 Elapsed Cycles          cycle       416116
    Average L2 Active Cycles         cycle      4455.29
    Total L2 Elapsed Cycles          cycle       182208
    Average SM Active Cycles         cycle      4939.33
    Total SM Elapsed Cycles          cycle       416116
    Average SMSP Active Cycles       cycle      4895.16
    Total SMSP Elapsed Cycles        cycle      1664464
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.92
    Elapsed Cycles                cycle         7185
    Memory Throughput                 %        49.09
    DRAM Throughput                   %        49.09
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.64
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      4909.95
    Compute (SM) Throughput           %        15.77
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.52
    Achieved Active Warps Per SM           warp        35.29
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      4909.95
    Total L1 Elapsed Cycles          cycle       415502
    Average L2 Active Cycles         cycle      4471.04
    Total L2 Elapsed Cycles          cycle       178896
    Average SM Active Cycles         cycle      4909.95
    Total SM Elapsed Cycles          cycle       415502
    Average SMSP Active Cycles       cycle      4912.25
    Total SMSP Elapsed Cycles        cycle      1662008
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.49
    Elapsed Cycles                cycle         7188
    Memory Throughput                 %        49.12
    DRAM Throughput                   %        49.12
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      5023.19
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.96
    Achieved Active Warps Per SM           warp        34.06
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5023.19
    Total L1 Elapsed Cycles          cycle       414150
    Average L2 Active Cycles         cycle      4511.08
    Total L2 Elapsed Cycles          cycle       179016
    Average SM Active Cycles         cycle      5023.19
    Total SM Elapsed Cycles          cycle       414150
    Average SMSP Active Cycles       cycle      4938.47
    Total SMSP Elapsed Cycles        cycle      1656600
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4431498
    Memory Throughput                 %        50.93
    DRAM Throughput                   %         0.23
    Duration                         ms         5.57
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205366.48
    Compute (SM) Throughput           %        50.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        78776
    Total DRAM Elapsed Cycles        cycle    208856064
    Average L1 Active Cycles         cycle   3205366.48
    Total L1 Elapsed Cycles          cycle    257690162
    Average L2 Active Cycles         cycle      1034223
    Total L2 Elapsed Cycles          cycle    110369328
    Average SM Active Cycles         cycle   3205366.48
    Total SM Elapsed Cycles          cycle    257690162
    Average SMSP Active Cycles       cycle   3204532.12
    Total SMSP Elapsed Cycles        cycle   1030760648
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.76% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20%                                                                                             
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.72% above the average, while the minimum instance value is 10.36% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.76% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.57
    Elapsed Cycles                cycle         6779
    Memory Throughput                 %        44.78
    DRAM Throughput                   %        44.78
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        24.73
    L2 Cache Throughput               %        24.85
    SM Active Cycles              cycle      4569.17
    Compute (SM) Throughput           %        17.13
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.23
    Achieved Active Warps Per SM           warp        34.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.77%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23842.67
    Total DRAM Elapsed Cycles        cycle       319488
    Average L1 Active Cycles         cycle      4569.17
    Total L1 Elapsed Cycles          cycle       382532
    Average L2 Active Cycles         cycle      4014.58
    Total L2 Elapsed Cycles          cycle       168840
    Average SM Active Cycles         cycle      4569.17
    Total SM Elapsed Cycles          cycle       382532
    Average SMSP Active Cycles       cycle      4351.30
    Total SMSP Elapsed Cycles        cycle      1530128
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.77
    Elapsed Cycles                cycle         7290
    Memory Throughput                 %        48.46
    DRAM Throughput                   %        48.46
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.74
    L2 Cache Throughput               %        27.33
    SM Active Cycles              cycle      4960.16
    Compute (SM) Throughput           %        15.85
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.30
    Achieved Active Warps Per SM           warp        35.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.7%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27789.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4960.16
    Total L1 Elapsed Cycles          cycle       413528
    Average L2 Active Cycles         cycle      4508.25
    Total L2 Elapsed Cycles          cycle       181440
    Average SM Active Cycles         cycle      4960.16
    Total SM Elapsed Cycles          cycle       413528
    Average SMSP Active Cycles       cycle      4948.93
    Total SMSP Elapsed Cycles        cycle      1654112
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.25
    Elapsed Cycles                cycle         7153
    Memory Throughput                 %        49.46
    DRAM Throughput                   %        49.46
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      4996.03
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.64
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4996.03
    Total L1 Elapsed Cycles          cycle       412636
    Average L2 Active Cycles         cycle      4540.12
    Total L2 Elapsed Cycles          cycle       178032
    Average SM Active Cycles         cycle      4996.03
    Total SM Elapsed Cycles          cycle       412636
    Average SMSP Active Cycles       cycle      4975.79
    Total SMSP Elapsed Cycles        cycle      1650544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.99
    Elapsed Cycles                cycle         7092
    Memory Throughput                 %        49.83
    DRAM Throughput                   %        49.83
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.83
    L2 Cache Throughput               %        28.13
    SM Active Cycles              cycle      4915.93
    Compute (SM) Throughput           %        15.90
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.26
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4915.93
    Total L1 Elapsed Cycles          cycle       412124
    Average L2 Active Cycles         cycle      4422.04
    Total L2 Elapsed Cycles          cycle       176472
    Average SM Active Cycles         cycle      4915.93
    Total SM Elapsed Cycles          cycle       412124
    Average SMSP Active Cycles       cycle      4870.29
    Total SMSP Elapsed Cycles        cycle      1648496
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4449704
    Memory Throughput                 %        50.93
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204748.60
    Compute (SM) Throughput           %        50.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77261.33
    Total DRAM Elapsed Cycles        cycle    209716224
    Average L1 Active Cycles         cycle   3204748.60
    Total L1 Elapsed Cycles          cycle    257689684
    Average L2 Active Cycles         cycle   1017607.92
    Total L2 Elapsed Cycles          cycle    110824416
    Average SM Active Cycles         cycle   3204748.60
    Total SM Elapsed Cycles          cycle    257689684
    Average SMSP Active Cycles       cycle   3205210.28
    Total SMSP Elapsed Cycles        cycle   1030758736
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.03%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.77% above the average, while the minimum instance value is 10.45% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.03%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.77% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.71
    Elapsed Cycles                cycle         6740
    Memory Throughput                 %        45.58
    DRAM Throughput                   %        45.58
    Duration                         us         8.61
    L1/TEX Cache Throughput           %        25.21
    L2 Cache Throughput               %        24.96
    SM Active Cycles              cycle      4481.31
    Compute (SM) Throughput           %        17.08
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.14
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24117.33
    Total DRAM Elapsed Cycles        cycle       317440
    Average L1 Active Cycles         cycle      4481.31
    Total L1 Elapsed Cycles          cycle       383648
    Average L2 Active Cycles         cycle      4075.04
    Total L2 Elapsed Cycles          cycle       167808
    Average SM Active Cycles         cycle      4481.31
    Total SM Elapsed Cycles          cycle       383648
    Average SMSP Active Cycles       cycle      4434.17
    Total SMSP Elapsed Cycles        cycle      1534592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       785.10
    Elapsed Cycles                cycle         7212
    Memory Throughput                 %        48.90
    DRAM Throughput                   %        48.90
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.56
    L2 Cache Throughput               %        27.63
    SM Active Cycles              cycle      5003.78
    Compute (SM) Throughput           %        15.71
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.51
    Achieved Active Warps Per SM           warp        34.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.49%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27792
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5003.78
    Total L1 Elapsed Cycles          cycle       417136
    Average L2 Active Cycles         cycle      4523.21
    Total L2 Elapsed Cycles          cycle       179592
    Average SM Active Cycles         cycle      5003.78
    Total SM Elapsed Cycles          cycle       417136
    Average SMSP Active Cycles       cycle      4954.45
    Total SMSP Elapsed Cycles        cycle      1668544
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       781.03
    Elapsed Cycles                cycle         7199
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.59
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      4933.50
    Compute (SM) Throughput           %        15.74
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.25
    Achieved Active Warps Per SM           warp        35.16
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.75%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27770.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4933.50
    Total L1 Elapsed Cycles          cycle       416334
    Average L2 Active Cycles         cycle      4363.67
    Total L2 Elapsed Cycles          cycle       179184
    Average SM Active Cycles         cycle      4933.50
    Total SM Elapsed Cycles          cycle       416334
    Average SMSP Active Cycles       cycle      4782.76
    Total SMSP Elapsed Cycles        cycle      1665336
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.83
    Elapsed Cycles                cycle         7241
    Memory Throughput                 %        48.77
    DRAM Throughput                   %        48.77
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        27.55
    SM Active Cycles              cycle      4974.83
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.60
    Achieved Active Warps Per SM           warp        35.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.4%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27717.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4974.83
    Total L1 Elapsed Cycles          cycle       414016
    Average L2 Active Cycles         cycle      4474.71
    Total L2 Elapsed Cycles          cycle       180144
    Average SM Active Cycles         cycle      4974.83
    Total SM Elapsed Cycles          cycle       414016
    Average SMSP Active Cycles       cycle      4896.60
    Total SMSP Elapsed Cycles        cycle      1656064
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4439238
    Memory Throughput                 %        50.82
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205285.64
    Compute (SM) Throughput           %        50.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     79413.33
    Total DRAM Elapsed Cycles        cycle    209221632
    Average L1 Active Cycles         cycle   3205285.64
    Total L1 Elapsed Cycles          cycle    258202624
    Average L2 Active Cycles         cycle   1028550.46
    Total L2 Elapsed Cycles          cycle    110562528
    Average SM Active Cycles         cycle   3205285.64
    Total SM Elapsed Cycles          cycle    258202624
    Average SMSP Active Cycles       cycle   3205285.41
    Total SMSP Elapsed Cycles        cycle   1032810496
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.93%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.68% above the average, while the minimum instance value is 10.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.75% above the average, while the minimum instance value is 10.46% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.93%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.68% above the average, while the minimum instance value is 10.37% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.93
    Elapsed Cycles                cycle         6794
    Memory Throughput                 %        44.58
    DRAM Throughput                   %        44.58
    Duration                         us         8.67
    L1/TEX Cache Throughput           %        25.44
    L2 Cache Throughput               %        24.79
    SM Active Cycles              cycle      4441.48
    Compute (SM) Throughput           %        17.37
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.62
    Achieved Active Warps Per SM           warp        35.34
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.38%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23813.33
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4441.48
    Total L1 Elapsed Cycles          cycle       377370
    Average L2 Active Cycles         cycle      3984.67
    Total L2 Elapsed Cycles          cycle       169032
    Average SM Active Cycles         cycle      4441.48
    Total SM Elapsed Cycles          cycle       377370
    Average SMSP Active Cycles       cycle      4319.38
    Total SMSP Elapsed Cycles        cycle      1509480
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.30
    Elapsed Cycles                cycle         7245
    Memory Throughput                 %        48.69
    DRAM Throughput                   %        48.69
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.53
    L2 Cache Throughput               %        27.48
    SM Active Cycles              cycle      5000.66
    Compute (SM) Throughput           %        15.70
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.59
    Achieved Active Warps Per SM           warp        35.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.41%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       342016
    Average L1 Active Cycles         cycle      5000.66
    Total L1 Elapsed Cycles          cycle       417448
    Average L2 Active Cycles         cycle      4517.58
    Total L2 Elapsed Cycles          cycle       180552
    Average SM Active Cycles         cycle      5000.66
    Total SM Elapsed Cycles          cycle       417448
    Average SMSP Active Cycles       cycle      4954.10
    Total SMSP Elapsed Cycles        cycle      1669792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.496%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.98% above the average, while the minimum instance value is 6.42% below    
          the average.                                                                                                  

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       783.71
    Elapsed Cycles                cycle         7128
    Memory Throughput                 %        49.46
    DRAM Throughput                   %        49.46
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        24.21
    L2 Cache Throughput               %        27.94
    SM Active Cycles              cycle      4988.45
    Compute (SM) Throughput           %        16.15
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.02
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27856
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4988.45
    Total L1 Elapsed Cycles          cycle       405674
    Average L2 Active Cycles         cycle      4368.83
    Total L2 Elapsed Cycles          cycle       177408
    Average SM Active Cycles         cycle      4988.45
    Total SM Elapsed Cycles          cycle       405674
    Average SMSP Active Cycles       cycle      4812.12
    Total SMSP Elapsed Cycles        cycle      1622696
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.56
    Elapsed Cycles                cycle         7201
    Memory Throughput                 %        49.51
    DRAM Throughput                   %        49.51
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle         4930
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.61
    Achieved Active Warps Per SM           warp        35.81
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.39%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27882.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle         4930
    Total L1 Elapsed Cycles          cycle       406048
    Average L2 Active Cycles         cycle      4500.75
    Total L2 Elapsed Cycles          cycle       179280
    Average SM Active Cycles         cycle         4930
    Total SM Elapsed Cycles          cycle       406048
    Average SMSP Active Cycles       cycle      4905.97
    Total SMSP Elapsed Cycles        cycle      1624192
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4450225
    Memory Throughput                 %        50.95
    DRAM Throughput                   %         0.23
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204940.48
    Compute (SM) Throughput           %        50.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.25
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     80450.67
    Total DRAM Elapsed Cycles        cycle    209738752
    Average L1 Active Cycles         cycle   3204940.48
    Total L1 Elapsed Cycles          cycle    257559448
    Average L2 Active Cycles         cycle   1033631.29
    Total L2 Elapsed Cycles          cycle    110835744
    Average SM Active Cycles         cycle   3204940.48
    Total SM Elapsed Cycles          cycle    257559448
    Average SMSP Active Cycles       cycle   3205033.41
    Total SMSP Elapsed Cycles        cycle   1030237792
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.16%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.94% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.16%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.94% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.93
    Elapsed Cycles                cycle         6543
    Memory Throughput                 %        46.72
    DRAM Throughput                   %        46.72
    Duration                         us         8.35
    L1/TEX Cache Throughput           %        25.00
    L2 Cache Throughput               %        25.70
    SM Active Cycles              cycle      4520.48
    Compute (SM) Throughput           %        17.47
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.14
    Achieved Active Warps Per SM           warp        33.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.86%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24000
    Total DRAM Elapsed Cycles        cycle       308224
    Average L1 Active Cycles         cycle      4520.48
    Total L1 Elapsed Cycles          cycle       375098
    Average L2 Active Cycles         cycle      4085.38
    Total L2 Elapsed Cycles          cycle       163008
    Average SM Active Cycles         cycle      4520.48
    Total SM Elapsed Cycles          cycle       375098
    Average SMSP Active Cycles       cycle      4439.36
    Total SMSP Elapsed Cycles        cycle      1500392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.14
    Elapsed Cycles                cycle         7363
    Memory Throughput                 %        47.97
    DRAM Throughput                   %        47.97
    Duration                         us         9.41
    L1/TEX Cache Throughput           %        23.36
    L2 Cache Throughput               %        27.04
    SM Active Cycles              cycle      4960.47
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.80
    Achieved Active Warps Per SM           warp        35.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.2%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       347136
    Average L1 Active Cycles         cycle      4960.47
    Total L1 Elapsed Cycles          cycle       420522
    Average L2 Active Cycles         cycle      4520.79
    Total L2 Elapsed Cycles          cycle       183360
    Average SM Active Cycles         cycle      4960.47
    Total SM Elapsed Cycles          cycle       420522
    Average SMSP Active Cycles       cycle      4902.94
    Total SMSP Elapsed Cycles        cycle      1682088
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.41
    Elapsed Cycles                cycle         7198
    Memory Throughput                 %        48.97
    DRAM Throughput                   %        48.97
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.57
    L2 Cache Throughput               %        27.68
    SM Active Cycles              cycle      5033.88
    Compute (SM) Throughput           %        15.72
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.79
    Achieved Active Warps Per SM           warp        34.46
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.21%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27746.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      5033.88
    Total L1 Elapsed Cycles          cycle       416786
    Average L2 Active Cycles         cycle         4533
    Total L2 Elapsed Cycles          cycle       179232
    Average SM Active Cycles         cycle      5033.88
    Total SM Elapsed Cycles          cycle       416786
    Average SMSP Active Cycles       cycle      4947.31
    Total SMSP Elapsed Cycles        cycle      1667144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.78
    Elapsed Cycles                cycle         7204
    Memory Throughput                 %        48.77
    DRAM Throughput                   %        48.77
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.44
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      5034.33
    Compute (SM) Throughput           %        15.64
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.26
    Achieved Active Warps Per SM           warp        34.20
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.74%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27714.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5034.33
    Total L1 Elapsed Cycles          cycle       418938
    Average L2 Active Cycles         cycle      4390.17
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      5034.33
    Total SM Elapsed Cycles          cycle       418938
    Average SMSP Active Cycles       cycle      4795.56
    Total SMSP Elapsed Cycles        cycle      1675752
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4451685
    Memory Throughput                 %        50.98
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204737.12
    Compute (SM) Throughput           %        50.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77664
    Total DRAM Elapsed Cycles        cycle    209808384
    Average L1 Active Cycles         cycle   3204737.12
    Total L1 Elapsed Cycles          cycle    257398782
    Average L2 Active Cycles         cycle   1037787.79
    Total L2 Elapsed Cycles          cycle    110871960
    Average SM Active Cycles         cycle   3204737.12
    Total SM Elapsed Cycles          cycle    257398782
    Average SMSP Active Cycles       cycle   3205163.32
    Total SMSP Elapsed Cycles        cycle   1029595128
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.93%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.60% above the average, while the minimum instance value is 10.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.85% above the average, while the minimum instance value is 10.55% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.93%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.60% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       783.14
    Elapsed Cycles                cycle         6617
    Memory Throughput                 %        45.86
    DRAM Throughput                   %        45.86
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.30
    L2 Cache Throughput               %        25.47
    SM Active Cycles              cycle      4466.69
    Compute (SM) Throughput           %        17.20
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.72
    Achieved Active Warps Per SM           warp        33.95
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.28%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23792
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4466.69
    Total L1 Elapsed Cycles          cycle       380944
    Average L2 Active Cycles         cycle      4030.79
    Total L2 Elapsed Cycles          cycle       164712
    Average SM Active Cycles         cycle      4466.69
    Total SM Elapsed Cycles          cycle       380944
    Average SMSP Active Cycles       cycle      4377.47
    Total SMSP Elapsed Cycles        cycle      1523776
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.25
    Elapsed Cycles                cycle         7146
    Memory Throughput                 %        49.34
    DRAM Throughput                   %        49.34
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.88
    SM Active Cycles              cycle      4947.78
    Compute (SM) Throughput           %        15.83
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.56
    Achieved Active Warps Per SM           warp        35.31
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.44%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27786.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4947.78
    Total L1 Elapsed Cycles          cycle       414050
    Average L2 Active Cycles         cycle      4366.38
    Total L2 Elapsed Cycles          cycle       177840
    Average SM Active Cycles         cycle      4947.78
    Total SM Elapsed Cycles          cycle       414050
    Average SMSP Active Cycles       cycle      4814.37
    Total SMSP Elapsed Cycles        cycle      1656200
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.83
    Elapsed Cycles                cycle         7240
    Memory Throughput                 %        48.73
    DRAM Throughput                   %        48.73
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.84
    L2 Cache Throughput               %        27.54
    SM Active Cycles              cycle         5017
    Compute (SM) Throughput           %        15.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.71
    Achieved Active Warps Per SM           warp        34.90
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.29%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27696
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle         5017
    Total L1 Elapsed Cycles          cycle       411948
    Average L2 Active Cycles         cycle      4478.54
    Total L2 Elapsed Cycles          cycle       180168
    Average SM Active Cycles         cycle         5017
    Total SM Elapsed Cycles          cycle       411948
    Average SMSP Active Cycles       cycle      4957.16
    Total SMSP Elapsed Cycles        cycle      1647792
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.92
    Elapsed Cycles                cycle         7277
    Memory Throughput                 %        48.38
    DRAM Throughput                   %        48.38
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.39
    SM Active Cycles              cycle      4969.91
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.94
    Achieved Active Warps Per SM           warp        35.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.06%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4969.91
    Total L1 Elapsed Cycles          cycle       410868
    Average L2 Active Cycles         cycle      4474.46
    Total L2 Elapsed Cycles          cycle       181152
    Average SM Active Cycles         cycle      4969.91
    Total SM Elapsed Cycles          cycle       410868
    Average SMSP Active Cycles       cycle      4911.38
    Total SMSP Elapsed Cycles        cycle      1643472
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4453315
    Memory Throughput                 %        50.96
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.61
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204505.66
    Compute (SM) Throughput           %        50.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77269.33
    Total DRAM Elapsed Cycles        cycle    209886208
    Average L1 Active Cycles         cycle   3204505.66
    Total L1 Elapsed Cycles          cycle    257520836
    Average L2 Active Cycles         cycle   1013176.54
    Total L2 Elapsed Cycles          cycle    110912832
    Average SM Active Cycles         cycle   3204505.66
    Total SM Elapsed Cycles          cycle    257520836
    Average SMSP Active Cycles       cycle   3205198.43
    Total SMSP Elapsed Cycles        cycle   1030083344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 19.98%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.69% above the average, while the minimum instance value is 10.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.37% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.98%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.69% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       784.05
    Elapsed Cycles                cycle         6675
    Memory Throughput                 %        45.86
    DRAM Throughput                   %        45.86
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.21
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4481.71
    Compute (SM) Throughput           %        17.28
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.77
    Achieved Active Warps Per SM           warp        34.93
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4481.71
    Total L1 Elapsed Cycles          cycle       379286
    Average L2 Active Cycles         cycle      4052.67
    Total L2 Elapsed Cycles          cycle       166152
    Average SM Active Cycles         cycle      4481.71
    Total SM Elapsed Cycles          cycle       379286
    Average SMSP Active Cycles       cycle      4432.66
    Total SMSP Elapsed Cycles        cycle      1517144
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.15
    Elapsed Cycles                cycle         7269
    Memory Throughput                 %        48.75
    DRAM Throughput                   %        48.75
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.81
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      5013.86
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.97
    Achieved Active Warps Per SM           warp        34.55
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.03%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27872
    Total DRAM Elapsed Cycles        cycle       343040
    Average L1 Active Cycles         cycle      5013.86
    Total L1 Elapsed Cycles          cycle       412622
    Average L2 Active Cycles         cycle      4465.46
    Total L2 Elapsed Cycles          cycle       180960
    Average SM Active Cycles         cycle      5013.86
    Total SM Elapsed Cycles          cycle       412622
    Average SMSP Active Cycles       cycle      4885.55
    Total SMSP Elapsed Cycles        cycle      1650488
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       783.78
    Elapsed Cycles                cycle         7300
    Memory Throughput                 %        48.41
    DRAM Throughput                   %        48.41
    Duration                         us         9.31
    L1/TEX Cache Throughput           %        23.37
    L2 Cache Throughput               %        27.27
    SM Active Cycles              cycle      5016.59
    Compute (SM) Throughput           %        15.58
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.17
    Achieved Active Warps Per SM           warp        34.64
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.83%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       345088
    Average L1 Active Cycles         cycle      5016.59
    Total L1 Elapsed Cycles          cycle       420578
    Average L2 Active Cycles         cycle      4374.88
    Total L2 Elapsed Cycles          cycle       181848
    Average SM Active Cycles         cycle      5016.59
    Total SM Elapsed Cycles          cycle       420578
    Average SMSP Active Cycles       cycle      4811.97
    Total SMSP Elapsed Cycles        cycle      1682312
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.99
    Elapsed Cycles                cycle         7149
    Memory Throughput                 %        49.25
    DRAM Throughput                   %        49.25
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.89
    L2 Cache Throughput               %        27.85
    SM Active Cycles              cycle      4942.47
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.40
    Achieved Active Warps Per SM           warp        35.23
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.6%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27736
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4942.47
    Total L1 Elapsed Cycles          cycle       411264
    Average L2 Active Cycles         cycle      4477.62
    Total L2 Elapsed Cycles          cycle       177912
    Average SM Active Cycles         cycle      4942.47
    Total SM Elapsed Cycles          cycle       411264
    Average SMSP Active Cycles       cycle      4895.33
    Total SMSP Elapsed Cycles        cycle      1645056
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4440289
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.59
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205269.83
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77624
    Total DRAM Elapsed Cycles        cycle    209270784
    Average L1 Active Cycles         cycle   3205269.83
    Total L1 Elapsed Cycles          cycle    257342768
    Average L2 Active Cycles         cycle   1019216.54
    Total L2 Elapsed Cycles          cycle    110588064
    Average SM Active Cycles         cycle   3205269.83
    Total SM Elapsed Cycles          cycle    257342768
    Average SMSP Active Cycles       cycle   3205383.84
    Total SMSP Elapsed Cycles        cycle   1029371072
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.13%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.86% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.1%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.83% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.13%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.86% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.92
    Elapsed Cycles                cycle         6665
    Memory Throughput                 %        45.35
    DRAM Throughput                   %        45.35
    Duration                         us         8.51
    L1/TEX Cache Throughput           %        25.81
    L2 Cache Throughput               %        25.24
    SM Active Cycles              cycle      4378.12
    Compute (SM) Throughput           %        17.24
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.07
    Achieved Active Warps Per SM           warp        35.07
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.93%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23840
    Total DRAM Elapsed Cycles        cycle       315392
    Average L1 Active Cycles         cycle      4378.12
    Total L1 Elapsed Cycles          cycle       380110
    Average L2 Active Cycles         cycle      3981.04
    Total L2 Elapsed Cycles          cycle       166008
    Average SM Active Cycles         cycle      4378.12
    Total SM Elapsed Cycles          cycle       380110
    Average SMSP Active Cycles       cycle      4331.57
    Total SMSP Elapsed Cycles        cycle      1520440
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       785.31
    Elapsed Cycles                cycle         7163
    Memory Throughput                 %        49.43
    DRAM Throughput                   %        49.43
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        24.00
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      4992.28
    Compute (SM) Throughput           %        16.01
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.64
    Achieved Active Warps Per SM           warp        34.87
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27840
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4992.28
    Total L1 Elapsed Cycles          cycle       409248
    Average L2 Active Cycles         cycle      4368.33
    Total L2 Elapsed Cycles          cycle       178320
    Average SM Active Cycles         cycle      4992.28
    Total SM Elapsed Cycles          cycle       409248
    Average SMSP Active Cycles       cycle      4820.73
    Total SMSP Elapsed Cycles        cycle      1636992
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.76
    Elapsed Cycles                cycle         7141
    Memory Throughput                 %        49.43
    DRAM Throughput                   %        49.43
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.89
    SM Active Cycles              cycle      4936.64
    Compute (SM) Throughput           %        15.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.04
    Achieved Active Warps Per SM           warp        34.58
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.96%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       336896
    Average L1 Active Cycles         cycle      4936.64
    Total L1 Elapsed Cycles          cycle       410792
    Average L2 Active Cycles         cycle      4511.67
    Total L2 Elapsed Cycles          cycle       177744
    Average SM Active Cycles         cycle      4936.64
    Total SM Elapsed Cycles          cycle       410792
    Average SMSP Active Cycles       cycle      5023.54
    Total SMSP Elapsed Cycles        cycle      1643168
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       783.78
    Elapsed Cycles                cycle         7250
    Memory Throughput                 %        48.53
    DRAM Throughput                   %        48.53
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.71
    L2 Cache Throughput               %        27.50
    SM Active Cycles              cycle         4866
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 24.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.99
    Achieved Active Warps Per SM           warp        36.48
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.01%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27826.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle         4866
    Total L1 Elapsed Cycles          cycle       414412
    Average L2 Active Cycles         cycle      4527.33
    Total L2 Elapsed Cycles          cycle       180456
    Average SM Active Cycles         cycle         4866
    Total SM Elapsed Cycles          cycle       414412
    Average SMSP Active Cycles       cycle      4987.02
    Total SMSP Elapsed Cycles        cycle      1657648
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449318
    Memory Throughput                 %        51.06
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205250.10
    Compute (SM) Throughput           %        51.06
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77245.33
    Total DRAM Elapsed Cycles        cycle    209697792
    Average L1 Active Cycles         cycle   3205250.10
    Total L1 Elapsed Cycles          cycle    257014298
    Average L2 Active Cycles         cycle   1034523.21
    Total L2 Elapsed Cycles          cycle    110818704
    Average SM Active Cycles         cycle   3205250.10
    Total SM Elapsed Cycles          cycle    257014298
    Average SMSP Active Cycles       cycle   3204629.46
    Total SMSP Elapsed Cycles        cycle   1028057192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.11%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.22%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.96% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.11%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.21
    SM Frequency                    Mhz       784.91
    Elapsed Cycles                cycle         6607
    Memory Throughput                 %        45.97
    DRAM Throughput                   %        45.97
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.13
    L2 Cache Throughput               %        25.50
    SM Active Cycles              cycle      4496.69
    Compute (SM) Throughput           %        17.35
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.68
    Achieved Active Warps Per SM           warp        34.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.32%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24005.33
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4496.69
    Total L1 Elapsed Cycles          cycle       377626
    Average L2 Active Cycles         cycle      4098.54
    Total L2 Elapsed Cycles          cycle       164568
    Average SM Active Cycles         cycle      4496.69
    Total SM Elapsed Cycles          cycle       377626
    Average SMSP Active Cycles       cycle      4484.30
    Total SMSP Elapsed Cycles        cycle      1510504
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.07
    Elapsed Cycles                cycle         7218
    Memory Throughput                 %        48.86
    DRAM Throughput                   %        48.86
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.72
    L2 Cache Throughput               %        27.62
    SM Active Cycles              cycle      4968.72
    Compute (SM) Throughput           %        15.82
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.00
    Achieved Active Warps Per SM           warp        35.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27765.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4968.72
    Total L1 Elapsed Cycles          cycle       414256
    Average L2 Active Cycles         cycle      4458.08
    Total L2 Elapsed Cycles          cycle       179664
    Average SM Active Cycles         cycle      4968.72
    Total SM Elapsed Cycles          cycle       414256
    Average SMSP Active Cycles       cycle      4884.88
    Total SMSP Elapsed Cycles        cycle      1657024
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.38
    Elapsed Cycles                cycle         7312
    Memory Throughput                 %        48.51
    DRAM Throughput                   %        48.51
    Duration                         us         9.34
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        27.25
    SM Active Cycles              cycle      4956.67
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.84
    Achieved Active Warps Per SM           warp        35.92
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4956.67
    Total L1 Elapsed Cycles          cycle       414514
    Average L2 Active Cycles         cycle      4564.79
    Total L2 Elapsed Cycles          cycle       182016
    Average SM Active Cycles         cycle      4956.67
    Total SM Elapsed Cycles          cycle       414514
    Average SMSP Active Cycles       cycle      4909.39
    Total SMSP Elapsed Cycles        cycle      1658056
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       784.50
    Elapsed Cycles                cycle         7156
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.12
    L1/TEX Cache Throughput           %        23.77
    L2 Cache Throughput               %        27.83
    SM Active Cycles              cycle      5129.21
    Compute (SM) Throughput           %        15.86
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 30.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.57
    Achieved Active Warps Per SM           warp        33.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.43%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5129.21
    Total L1 Elapsed Cycles          cycle       413284
    Average L2 Active Cycles         cycle      4547.46
    Total L2 Elapsed Cycles          cycle       178272
    Average SM Active Cycles         cycle      5129.21
    Total SM Elapsed Cycles          cycle       413284
    Average SMSP Active Cycles       cycle      4931.82
    Total SMSP Elapsed Cycles        cycle      1653136
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4436194
    Memory Throughput                 %        50.97
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3205365.64
    Compute (SM) Throughput           %        50.97
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78141.33
    Total DRAM Elapsed Cycles        cycle    209077248
    Average L1 Active Cycles         cycle   3205365.64
    Total L1 Elapsed Cycles          cycle    257446754
    Average L2 Active Cycles         cycle   1028154.67
    Total L2 Elapsed Cycles          cycle    110486232
    Average SM Active Cycles         cycle   3205365.64
    Total SM Elapsed Cycles          cycle    257446754
    Average SMSP Active Cycles       cycle   3205453.52
    Total SMSP Elapsed Cycles        cycle   1029787016
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.2%                                                                                           
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.97% above the average, while the minimum instance value is 10.44% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.77% above the average, while the minimum instance value is 10.48% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.2%                                                                                           
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.97% above the average, while the minimum instance value is 10.44% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       782.84
    Elapsed Cycles                cycle         6690
    Memory Throughput                 %        45.49
    DRAM Throughput                   %        45.49
    Duration                         us         8.54
    L1/TEX Cache Throughput           %        25.57
    L2 Cache Throughput               %        25.17
    SM Active Cycles              cycle      4419.28
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.11
    Achieved Active Warps Per SM           warp        34.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.89%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     23834.67
    Total DRAM Elapsed Cycles        cycle       314368
    Average L1 Active Cycles         cycle      4419.28
    Total L1 Elapsed Cycles          cycle       385968
    Average L2 Active Cycles         cycle      3934.58
    Total L2 Elapsed Cycles          cycle       166608
    Average SM Active Cycles         cycle      4419.28
    Total SM Elapsed Cycles          cycle       385968
    Average SMSP Active Cycles       cycle      4275.02
    Total SMSP Elapsed Cycles        cycle      1543872
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.67
    Elapsed Cycles                cycle         7193
    Memory Throughput                 %        48.94
    DRAM Throughput                   %        48.94
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.91
    L2 Cache Throughput               %        27.72
    SM Active Cycles              cycle      4977.48
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.18
    Achieved Active Warps Per SM           warp        34.65
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27728
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4977.48
    Total L1 Elapsed Cycles          cycle       410722
    Average L2 Active Cycles         cycle      4520.08
    Total L2 Elapsed Cycles          cycle       179112
    Average SM Active Cycles         cycle      4977.48
    Total SM Elapsed Cycles          cycle       410722
    Average SMSP Active Cycles       cycle      4927.19
    Total SMSP Elapsed Cycles        cycle      1642888
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       782.88
    Elapsed Cycles                cycle         7195
    Memory Throughput                 %        49.11
    DRAM Throughput                   %        49.11
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.69
    SM Active Cycles              cycle      4968.76
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.19
    Achieved Active Warps Per SM           warp        35.13
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.81%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27824
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4968.76
    Total L1 Elapsed Cycles          cycle       412626
    Average L2 Active Cycles         cycle      4411.75
    Total L2 Elapsed Cycles          cycle       179112
    Average SM Active Cycles         cycle      4968.76
    Total SM Elapsed Cycles          cycle       412626
    Average SMSP Active Cycles       cycle      4828.90
    Total SMSP Elapsed Cycles        cycle      1650504
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.18
    SM Frequency                    Mhz       782.35
    Elapsed Cycles                cycle         7266
    Memory Throughput                 %        48.53
    DRAM Throughput                   %        48.53
    Duration                         us         9.28
    L1/TEX Cache Throughput           %        23.97
    L2 Cache Throughput               %        27.42
    SM Active Cycles              cycle      4897.59
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.08
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27829.33
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4897.59
    Total L1 Elapsed Cycles          cycle       409680
    Average L2 Active Cycles         cycle      4450.46
    Total L2 Elapsed Cycles          cycle       180888
    Average SM Active Cycles         cycle      4897.59
    Total SM Elapsed Cycles          cycle       409680
    Average SMSP Active Cycles       cycle      4891.20
    Total SMSP Elapsed Cycles        cycle      1638720
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4451511
    Memory Throughput                 %        51.02
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205056.45
    Compute (SM) Throughput           %        51.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77378.67
    Total DRAM Elapsed Cycles        cycle    209801216
    Average L1 Active Cycles         cycle   3205056.45
    Total L1 Elapsed Cycles          cycle    257194784
    Average L2 Active Cycles         cycle   1025580.92
    Total L2 Elapsed Cycles          cycle    110867760
    Average SM Active Cycles         cycle   3205056.45
    Total SM Elapsed Cycles          cycle    257194784
    Average SMSP Active Cycles       cycle   3205399.36
    Total SMSP Elapsed Cycles        cycle   1028779136
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.19%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.93% above the average, while the minimum instance value is 10.37% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 19.99%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.65% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.19%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.93% above the average, while the minimum instance value is 10.37% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       781.25
    Elapsed Cycles                cycle         6703
    Memory Throughput                 %        45.54
    DRAM Throughput                   %        45.54
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.26
    L2 Cache Throughput               %        25.14
    SM Active Cycles              cycle      4473.21
    Compute (SM) Throughput           %        17.30
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.96
    Achieved Active Warps Per SM           warp        35.02
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.04%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24016
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4473.21
    Total L1 Elapsed Cycles          cycle       378846
    Average L2 Active Cycles         cycle      3965.42
    Total L2 Elapsed Cycles          cycle       166944
    Average SM Active Cycles         cycle      4473.21
    Total SM Elapsed Cycles          cycle       378846
    Average SMSP Active Cycles       cycle      4407.98
    Total SMSP Elapsed Cycles        cycle      1515384
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.07
    Elapsed Cycles                cycle         7179
    Memory Throughput                 %        48.99
    DRAM Throughput                   %        48.99
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.70
    L2 Cache Throughput               %        27.73
    SM Active Cycles              cycle      4911.84
    Compute (SM) Throughput           %        15.81
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.08
    Achieved Active Warps Per SM           warp        35.08
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.92%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4911.84
    Total L1 Elapsed Cycles          cycle       414436
    Average L2 Active Cycles         cycle      4472.12
    Total L2 Elapsed Cycles          cycle       178920
    Average SM Active Cycles         cycle      4911.84
    Total SM Elapsed Cycles          cycle       414436
    Average SMSP Active Cycles       cycle      4893.81
    Total SMSP Elapsed Cycles        cycle      1657744
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.42
    Elapsed Cycles                cycle         7112
    Memory Throughput                 %        49.74
    DRAM Throughput                   %        49.74
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.86
    L2 Cache Throughput               %        28.01
    SM Active Cycles              cycle      4978.72
    Compute (SM) Throughput           %        15.92
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.49
    Achieved Active Warps Per SM           warp        34.32
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27845.33
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4978.72
    Total L1 Elapsed Cycles          cycle       411650
    Average L2 Active Cycles         cycle      4443.04
    Total L2 Elapsed Cycles          cycle       177072
    Average SM Active Cycles         cycle      4978.72
    Total SM Elapsed Cycles          cycle       411650
    Average SMSP Active Cycles       cycle      4849.64
    Total SMSP Elapsed Cycles        cycle      1646600
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.13
    SM Frequency                    Mhz       783.10
    Elapsed Cycles                cycle         7193
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.79
    L2 Cache Throughput               %        27.66
    SM Active Cycles              cycle      4961.33
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.22
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27808
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4961.33
    Total L1 Elapsed Cycles          cycle       412728
    Average L2 Active Cycles         cycle      4499.54
    Total L2 Elapsed Cycles          cycle       179112
    Average SM Active Cycles         cycle      4961.33
    Total SM Elapsed Cycles          cycle       412728
    Average SMSP Active Cycles       cycle      4922.36
    Total SMSP Elapsed Cycles        cycle      1650912
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4451150
    Memory Throughput                 %        50.99
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204863.28
    Compute (SM) Throughput           %        50.99
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        77944
    Total DRAM Elapsed Cycles        cycle    209783808
    Average L1 Active Cycles         cycle   3204863.28
    Total L1 Elapsed Cycles          cycle    257355446
    Average L2 Active Cycles         cycle   1000304.67
    Total L2 Elapsed Cycles          cycle    110858712
    Average SM Active Cycles         cycle   3204863.28
    Total SM Elapsed Cycles          cycle    257355446
    Average SMSP Active Cycles       cycle   3205724.44
    Total SMSP Elapsed Cycles        cycle   1029421784
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.79% above the average, while the minimum instance value is 10.49% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.31% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.79% above the average, while the minimum instance value is 10.49% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.09
    Elapsed Cycles                cycle         6709
    Memory Throughput                 %        45.15
    DRAM Throughput                   %        45.15
    Duration                         us         8.58
    L1/TEX Cache Throughput           %        25.98
    L2 Cache Throughput               %        25.10
    SM Active Cycles              cycle      4348.43
    Compute (SM) Throughput           %        17.02
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23808
    Total DRAM Elapsed Cycles        cycle       316416
    Average L1 Active Cycles         cycle      4348.43
    Total L1 Elapsed Cycles          cycle       385074
    Average L2 Active Cycles         cycle      4024.12
    Total L2 Elapsed Cycles          cycle       166992
    Average SM Active Cycles         cycle      4348.43
    Total SM Elapsed Cycles          cycle       385074
    Average SMSP Active Cycles       cycle      4386.58
    Total SMSP Elapsed Cycles        cycle      1540296
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       784.33
    Elapsed Cycles                cycle         7180
    Memory Throughput                 %        49.09
    DRAM Throughput                   %        49.09
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.73
    L2 Cache Throughput               %        27.75
    SM Active Cycles              cycle      5028.29
    Compute (SM) Throughput           %        15.84
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.1%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.88
    Achieved Active Warps Per SM           warp        34.50
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.12%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.9%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27733.33
    Total DRAM Elapsed Cycles        cycle       338944
    Average L1 Active Cycles         cycle      5028.29
    Total L1 Elapsed Cycles          cycle       413738
    Average L2 Active Cycles         cycle      4475.12
    Total L2 Elapsed Cycles          cycle       178728
    Average SM Active Cycles         cycle      5028.29
    Total SM Elapsed Cycles          cycle       413738
    Average SMSP Active Cycles       cycle      4911.40
    Total SMSP Elapsed Cycles        cycle      1654952
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       783.58
    Elapsed Cycles                cycle         7199
    Memory Throughput                 %        48.82
    DRAM Throughput                   %        48.82
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.96
    L2 Cache Throughput               %        27.70
    SM Active Cycles              cycle      5008.57
    Compute (SM) Throughput           %        15.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27744
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5008.57
    Total L1 Elapsed Cycles          cycle       410012
    Average L2 Active Cycles         cycle      4469.33
    Total L2 Elapsed Cycles          cycle       179136
    Average SM Active Cycles         cycle      5008.57
    Total SM Elapsed Cycles          cycle       410012
    Average SMSP Active Cycles       cycle      4894.61
    Total SMSP Elapsed Cycles        cycle      1640048
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       782.69
    Elapsed Cycles                cycle         7164
    Memory Throughput                 %        49.38
    DRAM Throughput                   %        49.38
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.87
    L2 Cache Throughput               %        27.79
    SM Active Cycles              cycle      5027.05
    Compute (SM) Throughput           %        15.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.41
    Achieved Active Warps Per SM           warp        35.24
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.59%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      5027.05
    Total L1 Elapsed Cycles          cycle       411382
    Average L2 Active Cycles         cycle      4483.46
    Total L2 Elapsed Cycles          cycle       178416
    Average SM Active Cycles         cycle      5027.05
    Total SM Elapsed Cycles          cycle       411382
    Average SMSP Active Cycles       cycle      4903.19
    Total SMSP Elapsed Cycles        cycle      1645528
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4451681
    Memory Throughput                 %        50.91
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.59
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3205080.72
    Compute (SM) Throughput           %        50.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.04
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78245.33
    Total DRAM Elapsed Cycles        cycle    209808384
    Average L1 Active Cycles         cycle   3205080.72
    Total L1 Elapsed Cycles          cycle    257751500
    Average L2 Active Cycles         cycle   1039188.62
    Total L2 Elapsed Cycles          cycle    110871984
    Average SM Active Cycles         cycle   3205080.72
    Total SM Elapsed Cycles          cycle    257751500
    Average SMSP Active Cycles       cycle   3205286.53
    Total SMSP Elapsed Cycles        cycle   1031006000
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.85% above the average, while the minimum instance value is 10.42% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.02%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.76% above the average, while the minimum instance value is 10.53% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.85% above the average, while the minimum instance value is 10.42% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       782.87
    Elapsed Cycles                cycle         6590
    Memory Throughput                 %        46.29
    DRAM Throughput                   %        46.29
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.58
    L2 Cache Throughput               %        25.54
    SM Active Cycles              cycle      4417.57
    Compute (SM) Throughput           %        17.41
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.49
    Achieved Active Warps Per SM           warp        34.80
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.51%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24016
    Total DRAM Elapsed Cycles        cycle       311296
    Average L1 Active Cycles         cycle      4417.57
    Total L1 Elapsed Cycles          cycle       376348
    Average L2 Active Cycles         cycle      4049.75
    Total L2 Elapsed Cycles          cycle       164184
    Average SM Active Cycles         cycle      4417.57
    Total SM Elapsed Cycles          cycle       376348
    Average SMSP Active Cycles       cycle      4411.07
    Total SMSP Elapsed Cycles        cycle      1505392
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.20
    SM Frequency                    Mhz       784.13
    Elapsed Cycles                cycle         7253
    Memory Throughput                 %        48.41
    DRAM Throughput                   %        48.41
    Duration                         us         9.25
    L1/TEX Cache Throughput           %        23.66
    L2 Cache Throughput               %        27.46
    SM Active Cycles              cycle      4987.93
    Compute (SM) Throughput           %        15.78
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.69
    Achieved Active Warps Per SM           warp        34.89
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.31%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27760
    Total DRAM Elapsed Cycles        cycle       344064
    Average L1 Active Cycles         cycle      4987.93
    Total L1 Elapsed Cycles          cycle       415432
    Average L2 Active Cycles         cycle      4489.04
    Total L2 Elapsed Cycles          cycle       180648
    Average SM Active Cycles         cycle      4987.93
    Total SM Elapsed Cycles          cycle       415432
    Average SMSP Active Cycles       cycle      4891.06
    Total SMSP Elapsed Cycles        cycle      1661728
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.85
    Elapsed Cycles                cycle         7082
    Memory Throughput                 %        49.73
    DRAM Throughput                   %        49.73
    Duration                         us         9.06
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        28.14
    SM Active Cycles              cycle      4913.03
    Compute (SM) Throughput           %        15.79
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.6%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.36
    Achieved Active Warps Per SM           warp        34.73
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.64%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27754.67
    Total DRAM Elapsed Cycles        cycle       334848
    Average L1 Active Cycles         cycle      4913.03
    Total L1 Elapsed Cycles          cycle       415130
    Average L2 Active Cycles         cycle      4471.17
    Total L2 Elapsed Cycles          cycle       176304
    Average SM Active Cycles         cycle      4913.03
    Total SM Elapsed Cycles          cycle       415130
    Average SMSP Active Cycles       cycle      4888.46
    Total SMSP Elapsed Cycles        cycle      1660520
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.91
    Elapsed Cycles                cycle         7200
    Memory Throughput                 %        48.92
    DRAM Throughput                   %        48.92
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.46
    L2 Cache Throughput               %        27.67
    SM Active Cycles              cycle      4993.93
    Compute (SM) Throughput           %        15.66
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.22
    Achieved Active Warps Per SM           warp        34.67
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.78%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27720
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4993.93
    Total L1 Elapsed Cycles          cycle       418586
    Average L2 Active Cycles         cycle      4545.83
    Total L2 Elapsed Cycles          cycle       179328
    Average SM Active Cycles         cycle      4993.93
    Total SM Elapsed Cycles          cycle       418586
    Average SMSP Active Cycles       cycle      5005.06
    Total SMSP Elapsed Cycles        cycle      1674344
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4437781
    Memory Throughput                 %        50.95
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.57
    L2 Cache Throughput               %         3.80
    SM Active Cycles              cycle   3206099.97
    Compute (SM) Throughput           %        50.95
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.22
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77493.33
    Total DRAM Elapsed Cycles        cycle    209154048
    Average L1 Active Cycles         cycle   3206099.97
    Total L1 Elapsed Cycles          cycle    257545778
    Average L2 Active Cycles         cycle   1031645.33
    Total L2 Elapsed Cycles          cycle    110525880
    Average SM Active Cycles         cycle   3206099.97
    Total SM Elapsed Cycles          cycle    257545778
    Average SMSP Active Cycles       cycle   3204673.78
    Total SMSP Elapsed Cycles        cycle   1030183112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.09%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.82% above the average, while the minimum instance value is 10.47% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.11%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.86% above the average, while the minimum instance value is 10.59% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.09%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.82% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       781.39
    Elapsed Cycles                cycle         6577
    Memory Throughput                 %        46.02
    DRAM Throughput                   %        46.02
    Duration                         us         8.42
    L1/TEX Cache Throughput           %        25.62
    L2 Cache Throughput               %        25.58
    SM Active Cycles              cycle      4410.22
    Compute (SM) Throughput           %        17.16
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.52
    Achieved Active Warps Per SM           warp        34.33
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.48%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23800
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4410.22
    Total L1 Elapsed Cycles          cycle       381984
    Average L2 Active Cycles         cycle      4031.88
    Total L2 Elapsed Cycles          cycle       163776
    Average SM Active Cycles         cycle      4410.22
    Total SM Elapsed Cycles          cycle       381984
    Average SMSP Active Cycles       cycle      4489.81
    Total SMSP Elapsed Cycles        cycle      1527936
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       784.42
    Elapsed Cycles                cycle         7180
    Memory Throughput                 %        49.33
    DRAM Throughput                   %        49.33
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        24.02
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      4951.41
    Compute (SM) Throughput           %        16.03
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 27.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.83
    Achieved Active Warps Per SM           warp        34.96
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.17%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27784
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4951.41
    Total L1 Elapsed Cycles          cycle       408946
    Average L2 Active Cycles         cycle      4513.12
    Total L2 Elapsed Cycles          cycle       178752
    Average SM Active Cycles         cycle      4951.41
    Total SM Elapsed Cycles          cycle       408946
    Average SMSP Active Cycles       cycle      4934.96
    Total SMSP Elapsed Cycles        cycle      1635784
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       780.64
    Elapsed Cycles                cycle         7147
    Memory Throughput                 %        49.18
    DRAM Throughput                   %        49.18
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.90
    L2 Cache Throughput               %        27.89
    SM Active Cycles              cycle      4918.24
    Compute (SM) Throughput           %        15.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 25.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.20
    Achieved Active Warps Per SM           warp        35.62
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.8%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27696
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4918.24
    Total L1 Elapsed Cycles          cycle       411024
    Average L2 Active Cycles         cycle      4453.79
    Total L2 Elapsed Cycles          cycle       177840
    Average SM Active Cycles         cycle      4918.24
    Total SM Elapsed Cycles          cycle       411024
    Average SMSP Active Cycles       cycle      4866.96
    Total SMSP Elapsed Cycles        cycle      1644096
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.09
    Elapsed Cycles                cycle         7219
    Memory Throughput                 %        48.84
    DRAM Throughput                   %        48.84
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        24.19
    L2 Cache Throughput               %        27.57
    SM Active Cycles              cycle      5011.21
    Compute (SM) Throughput           %        16.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.15
    Achieved Active Warps Per SM           warp        35.11
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27757.33
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5011.21
    Total L1 Elapsed Cycles          cycle       406070
    Average L2 Active Cycles         cycle      4448.79
    Total L2 Elapsed Cycles          cycle       179712
    Average SM Active Cycles         cycle      5011.21
    Total SM Elapsed Cycles          cycle       406070
    Average SMSP Active Cycles       cycle      4866.60
    Total SMSP Elapsed Cycles        cycle      1624280
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4449271
    Memory Throughput                 %        50.91
    DRAM Throughput                   %         0.22
    Duration                         ms         5.60
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.79
    SM Active Cycles              cycle   3204814.12
    Compute (SM) Throughput           %        50.91
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77701.33
    Total DRAM Elapsed Cycles        cycle    209694720
    Average L1 Active Cycles         cycle   3204814.12
    Total L1 Elapsed Cycles          cycle    257752926
    Average L2 Active Cycles         cycle   1027541.62
    Total L2 Elapsed Cycles          cycle    110812176
    Average SM Active Cycles         cycle   3204814.12
    Total SM Elapsed Cycles          cycle    257752926
    Average SMSP Active Cycles       cycle   3204953.29
    Total SMSP Elapsed Cycles        cycle   1031011704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.05%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.81% above the average, while the minimum instance value is 10.31% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.47% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.05%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.81% above the average, while the minimum instance value is 10.31% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.12
    SM Frequency                    Mhz       783.17
    Elapsed Cycles                cycle         6617
    Memory Throughput                 %        46.47
    DRAM Throughput                   %        46.47
    Duration                         us         8.45
    L1/TEX Cache Throughput           %        25.11
    L2 Cache Throughput               %        25.43
    SM Active Cycles              cycle      4499.55
    Compute (SM) Throughput           %        17.50
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.8%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.18
    Achieved Active Warps Per SM           warp        34.17
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.82%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     24029.33
    Total DRAM Elapsed Cycles        cycle       310272
    Average L1 Active Cycles         cycle      4499.55
    Total L1 Elapsed Cycles          cycle       374424
    Average L2 Active Cycles         cycle      4059.42
    Total L2 Elapsed Cycles          cycle       164760
    Average SM Active Cycles         cycle      4499.55
    Total SM Elapsed Cycles          cycle       374424
    Average SMSP Active Cycles       cycle      4426.72
    Total SMSP Elapsed Cycles        cycle      1497696
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       785.14
    Elapsed Cycles                cycle         7187
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.74
    SM Active Cycles              cycle      4952.48
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        71.84
    Achieved Active Warps Per SM           warp        34.49
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 28.16%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (71.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        27800
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4952.48
    Total L1 Elapsed Cycles          cycle       412648
    Average L2 Active Cycles         cycle      4495.29
    Total L2 Elapsed Cycles          cycle       178848
    Average SM Active Cycles         cycle      4952.48
    Total SM Elapsed Cycles          cycle       412648
    Average SMSP Active Cycles       cycle      4914.28
    Total SMSP Elapsed Cycles        cycle      1650592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       783.56
    Elapsed Cycles                cycle         7125
    Memory Throughput                 %        49.62
    DRAM Throughput                   %        49.62
    Duration                         us         9.09
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.96
    SM Active Cycles              cycle      4864.69
    Compute (SM) Throughput           %        15.88
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.2%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.77
    Achieved Active Warps Per SM           warp        35.41
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.23%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27778.67
    Total DRAM Elapsed Cycles        cycle       335872
    Average L1 Active Cycles         cycle      4864.69
    Total L1 Elapsed Cycles          cycle       412696
    Average L2 Active Cycles         cycle      4510.25
    Total L2 Elapsed Cycles          cycle       177408
    Average SM Active Cycles         cycle      4864.69
    Total SM Elapsed Cycles          cycle       412696
    Average SMSP Active Cycles       cycle      4910.80
    Total SMSP Elapsed Cycles        cycle      1650784
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.17
    SM Frequency                    Mhz       783.66
    Elapsed Cycles                cycle         7225
    Memory Throughput                 %        48.93
    DRAM Throughput                   %        48.93
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.49
    L2 Cache Throughput               %        27.55
    SM Active Cycles              cycle      5022.19
    Compute (SM) Throughput           %        15.67
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 28.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        72.02
    Achieved Active Warps Per SM           warp        34.57
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 27.98%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (72.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27810.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      5022.19
    Total L1 Elapsed Cycles          cycle       418302
    Average L2 Active Cycles         cycle      4511.67
    Total L2 Elapsed Cycles          cycle       180048
    Average SM Active Cycles         cycle      5022.19
    Total SM Elapsed Cycles          cycle       418302
    Average SMSP Active Cycles       cycle      4903.35
    Total SMSP Elapsed Cycles        cycle      1673208
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.98
    Elapsed Cycles                cycle      4433992
    Memory Throughput                 %        50.93
    DRAM Throughput                   %         0.23
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.58
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205685.03
    Compute (SM) Throughput           %        50.93
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.23
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     78754.67
    Total DRAM Elapsed Cycles        cycle    208975872
    Average L1 Active Cycles         cycle   3205685.03
    Total L1 Elapsed Cycles          cycle    257684172
    Average L2 Active Cycles         cycle   1030732.38
    Total L2 Elapsed Cycles          cycle    110431416
    Average SM Active Cycles         cycle   3205685.03
    Total SM Elapsed Cycles          cycle    257684172
    Average SMSP Active Cycles       cycle   3205300.48
    Total SMSP Elapsed Cycles        cycle   1030736688
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.06%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.80% above the average, while the minimum instance value is 10.51% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.08%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.83% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.06%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.80% above the average, while the minimum instance value is 10.51% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 15, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.14
    SM Frequency                    Mhz       782.70
    Elapsed Cycles                cycle         6819
    Memory Throughput                 %        44.66
    DRAM Throughput                   %        44.66
    Duration                         us         8.70
    L1/TEX Cache Throughput           %        25.96
    L2 Cache Throughput               %        24.68
    SM Active Cycles              cycle      4352.21
    Compute (SM) Throughput           %        16.98
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 23.7%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        76.28
    Achieved Active Warps Per SM           warp        36.61
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.72%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (76.3%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        23856
    Total DRAM Elapsed Cycles        cycle       320512
    Average L1 Active Cycles         cycle      4352.21
    Total L1 Elapsed Cycles          cycle       385858
    Average L2 Active Cycles         cycle      3955.50
    Total L2 Elapsed Cycles          cycle       169776
    Average SM Active Cycles         cycle      4352.21
    Total SM Elapsed Cycles          cycle       385858
    Average SMSP Active Cycles       cycle      4323.10
    Total SMSP Elapsed Cycles        cycle      1543432
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.19
    SM Frequency                    Mhz       784.23
    Elapsed Cycles                cycle         7207
    Memory Throughput                 %        49.05
    DRAM Throughput                   %        49.05
    Duration                         us         9.18
    L1/TEX Cache Throughput           %        23.98
    L2 Cache Throughput               %        27.65
    SM Active Cycles              cycle      4962.41
    Compute (SM) Throughput           %        16.00
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.9%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.10
    Achieved Active Warps Per SM           warp        35.09
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.9%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27874.67
    Total DRAM Elapsed Cycles        cycle       340992
    Average L1 Active Cycles         cycle      4962.41
    Total L1 Elapsed Cycles          cycle       409628
    Average L2 Active Cycles         cycle      4508.75
    Total L2 Elapsed Cycles          cycle       179424
    Average SM Active Cycles         cycle      4962.41
    Total SM Elapsed Cycles          cycle       409628
    Average SMSP Active Cycles       cycle      4940.88
    Total SMSP Elapsed Cycles        cycle      1638512
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.48
    Elapsed Cycles                cycle         7174
    Memory Throughput                 %        49.36
    DRAM Throughput                   %        49.36
    Duration                         us         9.15
    L1/TEX Cache Throughput           %        23.80
    L2 Cache Throughput               %        27.76
    SM Active Cycles              cycle      4953.26
    Compute (SM) Throughput           %        15.87
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.4%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        73.64
    Achieved Active Warps Per SM           warp        35.35
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26.36%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (73.6%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27797.33
    Total DRAM Elapsed Cycles        cycle       337920
    Average L1 Active Cycles         cycle      4953.26
    Total L1 Elapsed Cycles          cycle       412898
    Average L2 Active Cycles         cycle      4492.50
    Total L2 Elapsed Cycles          cycle       178656
    Average SM Active Cycles         cycle      4953.26
    Total SM Elapsed Cycles          cycle       412898
    Average SMSP Active Cycles       cycle      4921.95
    Total SMSP Elapsed Cycles        cycle      1651592
    -------------------------- ----------- ------------

  void extract_mat<16>(const float *, float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.15
    SM Frequency                    Mhz       783.25
    Elapsed Cycles                cycle         7219
    Memory Throughput                 %        49.10
    DRAM Throughput                   %        49.10
    Duration                         us         9.22
    L1/TEX Cache Throughput           %        23.93
    L2 Cache Throughput               %        27.61
    SM Active Cycles              cycle      4919.17
    Compute (SM) Throughput           %        15.96
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 26.0%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        74.00
    Achieved Active Warps Per SM           warp        35.52
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 26%                                                                                       
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (74.0%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     27818.67
    Total DRAM Elapsed Cycles        cycle       339968
    Average L1 Active Cycles         cycle      4919.17
    Total L1 Elapsed Cycles          cycle       410524
    Average L2 Active Cycles         cycle      4491.17
    Total L2 Elapsed Cycles          cycle       179736
    Average SM Active Cycles         cycle      4919.17
    Total SM Elapsed Cycles          cycle       410524
    Average SMSP Active Cycles       cycle      4900.31
    Total SMSP Elapsed Cycles        cycle      1642096
    -------------------------- ----------- ------------

  void fa_kernel<64, 32, 256, 32, 1>(const float *, const float *, const float *, float *, int, float) (128, 1, 1)x(256, 1, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.24
    SM Frequency                    Mhz       794.99
    Elapsed Cycles                cycle      4437751
    Memory Throughput                 %        50.94
    DRAM Throughput                   %         0.22
    Duration                         ms         5.58
    L1/TEX Cache Throughput           %        70.60
    L2 Cache Throughput               %         3.81
    SM Active Cycles              cycle   3205010.67
    Compute (SM) Throughput           %        50.94
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    128
    Registers Per Thread             register/thread              58
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block           43.78
    # SMs                                         SM              58
    Threads                                   thread           32768
    Uses Green Context                                             0
    Waves Per SM                                                1.10
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %        33.33
    Achieved Occupancy                        %        29.24
    Achieved Active Warps Per SM           warp        14.03
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 66.67%                                                                                    
          The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (33.3%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     77437.33
    Total DRAM Elapsed Cycles        cycle    209153536
    Average L1 Active Cycles         cycle   3205010.67
    Total L1 Elapsed Cycles          cycle    257612336
    Average L2 Active Cycles         cycle   1031547.08
    Total L2 Elapsed Cycles          cycle    110528424
    Average SM Active Cycles         cycle   3205010.67
    Total SM Elapsed Cycles          cycle    257612336
    Average SMSP Active Cycles       cycle   3205101.88
    Total SMSP Elapsed Cycles        cycle   1030449344
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 20.01%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 27.73% above the average, while the minimum instance value is 10.52% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.04%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 27.78% above the average, while the minimum instance value is 10.54% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 20.01%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 27.73% above the average, while the minimum instance value is 10.52% below the      
          average.                                                                                                      

  void concat_mat<16>(float *, const float *, int, int, int, int, int, int) (2, 512, 1)x(16, 16, 1), Context 1, Stream 16, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz         6.16
    SM Frequency                    Mhz       781.63
    Elapsed Cycles                cycle         6629
    Memory Throughput                 %        46.06
    DRAM Throughput                   %        46.06
    Duration                         us         8.48
    L1/TEX Cache Throughput           %        24.79
    L2 Cache Throughput               %        25.39
    SM Active Cycles              cycle      4558.57
    Compute (SM) Throughput           %        17.14
    ----------------------- ----------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1024
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              58
    Threads                                   thread          262144
    Uses Green Context                                             0
    Waves Per SM                                                2.94
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 328 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 33.3% of the total kernel runtime with a lower occupancy of 29.5%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        70.50
    Achieved Active Warps Per SM           warp        33.84
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (70.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        24056
    Total DRAM Elapsed Cycles        cycle       313344
    Average L1 Active Cycles         cycle      4558.57
    Total L1 Elapsed Cycles          cycle       382382
    Average L2 Active Cycles         cycle      4096.71
    Total L2 Elapsed Cycles          cycle       165096
    Average SM Active Cycles         cycle      4558.57
    Total SM Elapsed Cycles          cycle       382382
    Average SMSP Active Cycles       cycle      4395.32
    Total SMSP Elapsed Cycles        cycle      1529528
    -------------------------- ----------- ------------

